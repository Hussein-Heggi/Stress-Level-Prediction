{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41799f29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:11.146036Z",
     "iopub.status.busy": "2025-12-10T21:54:11.145833Z",
     "iopub.status.idle": "2025-12-10T21:54:13.477012Z",
     "shell.execute_reply": "2025-12-10T21:54:13.476248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedGroupKFold\n",
    "except ImportError:\n",
    "    StratifiedGroupKFold = None\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f46d09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:13.480680Z",
     "iopub.status.busy": "2025-12-10T21:54:13.479972Z",
     "iopub.status.idle": "2025-12-10T21:54:13.483972Z",
     "shell.execute_reply": "2025-12-10T21:54:13.483224Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "DEFAULT_DATASET_ROOT = Path(\"./Datasets\")\n",
    "DATASET_ROOT = Path(os.getenv(\"DATASET_ROOT\", DEFAULT_DATASET_ROOT))\n",
    "STATES = [\"STRESS\", \"AEROBIC\", \"ANAEROBIC\"]\n",
    "TARGET_FS = 4.0\n",
    "WINDOW_SECONDS = 60\n",
    "WINDOW_STEP_SECONDS = 30\n",
    "MIN_LABEL_COVERAGE = 0.6\n",
    "SEED = 42\n",
    "MAX_SUBJECTS = None  \n",
    "APPLY_CHANNEL_NORMALIZATION = True\n",
    "APPLY_DIFF_CHANNELS = True\n",
    "APPLY_TEMPORAL_AUG = True\n",
    "\n",
    "TEMPORAL_AUG_COUNTS = {\"low_stress\": 2, \"high_stress\": 1, \"moderate_stress\": 1}\n",
    "LABEL_SMOOTHING = 0.05\n",
    "EMA_DECAY = 0.995\n",
    "TWO_STAGE_THRESHOLD = 0.4\n",
    "\n",
    "\n",
    "GROUP_SPLIT = True\n",
    "NUM_FOLDS = 5\n",
    "FOLD_INDEX = 0\n",
    "USE_STRATIFIED_GROUP_SPLIT = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44a274b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:13.486581Z",
     "iopub.status.busy": "2025-12-10T21:54:13.486255Z",
     "iopub.status.idle": "2025-12-10T21:54:13.508081Z",
     "shell.execute_reply": "2025-12-10T21:54:13.506792Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "STRESS_STAGE_ORDER_S = [\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_STAGE_ORDER_F = [\"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_TAG_PAIRS_S = [(3, 4), (5, 6), (7, 8), (9, 10), (11, 12)]\n",
    "STRESS_TAG_PAIRS_F = [(2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "STRESS_PHASES = {\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"}\n",
    "STRESS_LEVEL_BOUNDS = {\"low\": 3.0, \"moderate\": 6.0}\n",
    "STRESS_LEVEL_PHASE_BOUNDS = {\n",
    "    \"Stroop\": {\"low\": 2.5, \"moderate\": 5.0},\n",
    "    \"Opposite Opinion\": {\"low\": 2.5, \"moderate\": 5.5},\n",
    "    \"Real Opinion\": {\"low\": 2.8, \"moderate\": 5.5},\n",
    "    \"TMCT\": {\"low\": 2.8, \"moderate\": 5.8},\n",
    "    \"Subtract\": {\"low\": 2.8, \"moderate\": 5.8},\n",
    "}\n",
    "STRESS_LEVEL_FILES = [\"Stress_Level_v1.csv\", \"Stress_Level_v2.csv\"]\n",
    "MIN_LABEL_COVERAGE = 0.6\n",
    "\n",
    "\n",
    "def load_stress_levels():\n",
    "    levels = {}\n",
    "    for fname in STRESS_LEVEL_FILES:\n",
    "        path = Path(fname)\n",
    "        if not path.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        for subject, row in df.iterrows():\n",
    "            subj = str(subject).strip()\n",
    "            levels[subj] = {\n",
    "                col: (float(row[col]) if not pd.isna(row[col]) else np.nan)\n",
    "                for col in df.columns\n",
    "            }\n",
    "    return levels\n",
    "\n",
    "\n",
    "STRESS_LEVELS = load_stress_levels()\n",
    "\n",
    "\n",
    "def base_subject_id(subject: str) -> str:\n",
    "    return subject.split(\"_\")[0]\n",
    "\n",
    "\n",
    "def read_signal(path: Path):\n",
    "    with open(path, \"r\") as f:\n",
    "        start_line = f.readline().strip()\n",
    "        if not start_line:\n",
    "            raise ValueError(f\"Missing start timestamp in {path}\")\n",
    "        start_ts = pd.to_datetime(start_line.split(\",\")[0])\n",
    "        fs_line = f.readline().strip()\n",
    "        if not fs_line:\n",
    "            raise ValueError(f\"Missing sample rate in {path}\")\n",
    "        fs = float(fs_line.split(\",\")[0])\n",
    "        data = np.genfromtxt(f, delimiter=\",\")\n",
    "    data = np.asarray(data, dtype=float)\n",
    "    data = np.nan_to_num(data, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if data.ndim == 0:\n",
    "        data = data.reshape(1, 1)\n",
    "    return fs, data.squeeze(), start_ts\n",
    "\n",
    "\n",
    "def read_tags(path: Path, start_ts: pd.Timestamp):\n",
    "    if not path.exists():\n",
    "        return []\n",
    "    df = pd.read_csv(path, header=None)\n",
    "    tags = []\n",
    "    for ts_str in df[0].astype(str):\n",
    "        ts = pd.to_datetime(ts_str)\n",
    "        tags.append((ts - start_ts).total_seconds())\n",
    "    return [(t, t) for t in tags]\n",
    "\n",
    "\n",
    "def stress_intervals_from_tags(tags, subject):\n",
    "    if not tags:\n",
    "        return []\n",
    "    times = [t for t, _ in tags]\n",
    "    if subject.startswith(\"S\"):\n",
    "        idx_pairs = STRESS_TAG_PAIRS_S\n",
    "        stage_order = STRESS_STAGE_ORDER_S\n",
    "    else:\n",
    "        idx_pairs = STRESS_TAG_PAIRS_F\n",
    "        stage_order = STRESS_STAGE_ORDER_F\n",
    "    base_id = base_subject_id(subject)\n",
    "    spans = []\n",
    "    for stage, (i, j) in zip(stage_order, idx_pairs):\n",
    "        if i < len(times) and j < len(times) and times[j] > times[i]:\n",
    "            level = STRESS_LEVELS.get(base_id, {}).get(stage)\n",
    "            spans.append({\"start\": times[i], \"end\": times[j], \"stage\": stage, \"stress_level\": level})\n",
    "    return spans\n",
    "\n",
    "\n",
    "def active_intervals_from_tags(tags):\n",
    "    if len(tags) < 2:\n",
    "        return []\n",
    "    spans = []\n",
    "    for (a, _), (b, _) in zip(tags[:-1], tags[1:]):\n",
    "        if b > a:\n",
    "            spans.append({\"start\": a, \"end\": b, \"stage\": \"active\", \"stress_level\": 0.0})\n",
    "    return spans\n",
    "\n",
    "\n",
    "def stress_bucket(level: float = None, phase: str = None) -> str:\n",
    "    if phase in {\"aerobic\", \"anaerobic\", \"rest\", \"active\"}:\n",
    "        return \"no_stress\"\n",
    "    if level is None or pd.isna(level) or level <= 0:\n",
    "        return \"no_stress\"\n",
    "    bounds = STRESS_LEVEL_PHASE_BOUNDS.get(phase, STRESS_LEVEL_BOUNDS)\n",
    "    if level <= bounds[\"low\"]:\n",
    "        return \"low_stress\"\n",
    "    if level <= bounds[\"moderate\"]:\n",
    "        return \"moderate_stress\"\n",
    "    return \"high_stress\"\n",
    "\n",
    "\n",
    "def resample_to_rate(signal: np.ndarray, src_fs: float, tgt_fs: float) -> np.ndarray:\n",
    "    if signal.ndim == 1:\n",
    "        signal = signal[:, None]\n",
    "    signal = np.nan_to_num(signal, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    src_len = signal.shape[0]\n",
    "    duration = src_len / src_fs\n",
    "    tgt_len = int(duration * tgt_fs)\n",
    "    if tgt_len <= 0:\n",
    "        return np.zeros((0, signal.shape[1]), dtype=np.float32)\n",
    "    src_t = np.linspace(0, duration, src_len, endpoint=False)\n",
    "    tgt_t = np.linspace(0, duration, tgt_len, endpoint=False)\n",
    "    resampled = np.vstack([\n",
    "        np.interp(tgt_t, src_t, signal[:, i])\n",
    "        for i in range(signal.shape[1])\n",
    "    ]).T.astype(np.float32)\n",
    "    resampled = np.nan_to_num(resampled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    if resampled.shape[1] == 1:\n",
    "        return resampled[:, 0]\n",
    "    return resampled\n",
    "\n",
    "\n",
    "def window_intervals(duration: float, win_s: int, step_s: int):\n",
    "    windows = []\n",
    "    t = 0.0\n",
    "    while t + win_s <= duration:\n",
    "        windows.append((t, t + win_s))\n",
    "        t += step_s\n",
    "    return windows\n",
    "\n",
    "\n",
    "def assign_label(win, intervals):\n",
    "    start, end = win\n",
    "    length = end - start\n",
    "    best_label = None\n",
    "    best_cov = 0.0\n",
    "    best_span = None\n",
    "    for label, spans in intervals.items():\n",
    "        overlap = 0.0\n",
    "        span_choice = None\n",
    "        span_overlap = 0.0\n",
    "        for span in spans:\n",
    "            a = span[\"start\"] if isinstance(span, dict) else span[0]\n",
    "            b = span[\"end\"] if isinstance(span, dict) else span[1]\n",
    "            inter = max(0.0, min(end, b) - max(start, a))\n",
    "            if inter > 0:\n",
    "                overlap += inter\n",
    "                if inter > span_overlap:\n",
    "                    span_overlap = inter\n",
    "                    span_choice = span\n",
    "        coverage = overlap / length\n",
    "        if coverage > best_cov:\n",
    "            best_cov = coverage\n",
    "            best_label = label\n",
    "            best_span = span_choice\n",
    "    if best_cov >= MIN_LABEL_COVERAGE and best_label is not None:\n",
    "        return best_label, best_span\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def make_label_intervals(state: str, subject: str, tags, duration: float):\n",
    "    rest_span = [{\"start\": 0.0, \"end\": duration, \"stage\": \"rest\", \"stress_level\": 0.0}]\n",
    "    if state == \"STRESS\":\n",
    "        stress_spans = stress_intervals_from_tags(tags, subject)\n",
    "        if not stress_spans:\n",
    "            return {\"rest\": rest_span}\n",
    "        return {\"stress\": stress_spans, \"rest\": rest_span}\n",
    "    active = active_intervals_from_tags(tags)\n",
    "    label = \"aerobic\" if state == \"AEROBIC\" else \"anaerobic\"\n",
    "    if not active:\n",
    "        return {label: rest_span, \"rest\": rest_span}\n",
    "    return {label: active, \"rest\": rest_span}\n",
    "\n",
    "\n",
    "def load_subject_state(state: str, subject: str):\n",
    "    folder = DATASET_ROOT / state / subject\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(folder)\n",
    "    fs_eda, eda_raw, start_ts = read_signal(folder / \"EDA.csv\")\n",
    "    temp_path = folder / \"TEMP.csv\"\n",
    "    if temp_path.exists():\n",
    "        fs_temp, temp_raw, _ = read_signal(temp_path)\n",
    "    else:\n",
    "        fs_temp, temp_raw = fs_eda, np.zeros_like(eda_raw)\n",
    "    fs_acc, acc_raw, _ = read_signal(folder / \"ACC.csv\")\n",
    "    acc_raw = np.atleast_2d(acc_raw)\n",
    "    acc_mag = np.linalg.norm(acc_raw, axis=1)\n",
    "    bvp_path = folder / \"BVP.csv\"\n",
    "    if bvp_path.exists():\n",
    "        fs_bvp, bvp_raw, _ = read_signal(bvp_path)\n",
    "    else:\n",
    "        fs_bvp, bvp_raw = None, None\n",
    "    tags = read_tags(folder / \"tags.csv\", start_ts)\n",
    "    sensors = {\n",
    "        \"EDA\": np.nan_to_num(np.asarray(eda_raw, dtype=float), nan=0.0, posinf=0.0, neginf=0.0),\n",
    "        \"TEMP\": np.nan_to_num(np.asarray(temp_raw, dtype=float), nan=0.0, posinf=0.0, neginf=0.0),\n",
    "        \"ACC_MAG\": np.nan_to_num(acc_mag, nan=0.0, posinf=0.0, neginf=0.0),\n",
    "    }\n",
    "    if bvp_raw is not None:\n",
    "        sensors[\"BVP\"] = np.nan_to_num(np.asarray(bvp_raw, dtype=float), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    fs_map = {\"EDA\": fs_eda, \"TEMP\": fs_temp, \"ACC_MAG\": fs_acc}\n",
    "    if fs_bvp:\n",
    "        fs_map[\"BVP\"] = fs_bvp\n",
    "    duration = len(sensors[\"EDA\"]) / fs_eda\n",
    "    return {\"sensors\": sensors, \"fs\": fs_map, \"tags\": tags, \"duration\": duration}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83737a60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:13.511599Z",
     "iopub.status.busy": "2025-12-10T21:54:13.511207Z",
     "iopub.status.idle": "2025-12-10T21:54:13.520710Z",
     "shell.execute_reply": "2025-12-10T21:54:13.520078Z"
    }
   },
   "outputs": [],
   "source": [
    "EXPECTED_LEN = int(WINDOW_SECONDS * TARGET_FS)\n",
    "BASE_CHANNELS = [\"EDA\", \"TEMP\", \"ACC\", \"BVP\"]\n",
    "PHASE_ENCODING = {\n",
    "    \"baseline\": 0,\n",
    "    \"rest\": 0,\n",
    "    \"stress\": 1,\n",
    "    \"stroop\": 2,\n",
    "    \"tmct\": 3,\n",
    "    \"real opinion\": 4,\n",
    "    \"opposite opinion\": 5,\n",
    "    \"subtract\": 6,\n",
    "    \"aerobic\": 7,\n",
    "    \"anaerobic\": 8,\n",
    "    \"active\": 9,\n",
    "}\n",
    "NUMERIC_STABILITY_EPS = 1e-6\n",
    "\n",
    "\n",
    "def sanitize_array(array, dtype=np.float32):\n",
    "    if array is None:\n",
    "        return None\n",
    "    return np.nan_to_num(np.asarray(array, dtype=dtype), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "\n",
    "def safe_corrcoef(a, b, eps=NUMERIC_STABILITY_EPS):\n",
    "    if a is None or b is None or not len(a) or not len(b):\n",
    "        return 0.0\n",
    "    a_std = float(np.std(a))\n",
    "    b_std = float(np.std(b))\n",
    "    if a_std < eps or b_std < eps:\n",
    "        return 0.0\n",
    "    corr = np.corrcoef(a, b)[0, 1]\n",
    "    if np.isnan(corr) or np.isinf(corr):\n",
    "        return 0.0\n",
    "    return float(corr)\n",
    "\n",
    "\n",
    "def _slice_or_pad(signal: np.ndarray, start: int, end: int) -> np.ndarray:\n",
    "    length = end - start\n",
    "    if signal is None or len(signal) == 0:\n",
    "        return np.zeros(length, dtype=np.float32)\n",
    "    if end > len(signal):\n",
    "        pad = end - len(signal)\n",
    "        segment = signal[start: len(signal)]\n",
    "        if pad > 0:\n",
    "            segment = np.concatenate([segment, np.zeros(pad, dtype=segment.dtype)])\n",
    "    else:\n",
    "        segment = signal[start:end]\n",
    "    segment = np.asarray(segment, dtype=np.float32)\n",
    "    return sanitize_array(segment)\n",
    "\n",
    "\n",
    "def extract_respiratory_signal(bvp, fs=4.0):\n",
    "    from scipy.signal import butter, filtfilt, welch\n",
    "    from scipy.integrate import trapezoid\n",
    "    if bvp is None:\n",
    "        return np.zeros(EXPECTED_LEN, dtype=np.float32), 0.0\n",
    "    if not len(bvp):\n",
    "        return np.zeros_like(bvp), 0.0\n",
    "    ny = fs / 2\n",
    "    low, high = 0.15 / ny, 0.4 / ny\n",
    "    b, a = butter(4, [max(low, 0.001), min(high, 0.99)], btype='band')\n",
    "    filtered = filtfilt(b, a, bvp)\n",
    "    freqs, psd = welch(bvp, fs=fs, nperseg=min(128, len(bvp)))\n",
    "    mask = (freqs >= 0.15) & (freqs <= 0.4)\n",
    "    resp_power = trapezoid(psd[mask], freqs[mask]) if mask.any() else 0.0\n",
    "    return sanitize_array(filtered), float(resp_power)\n",
    "\n",
    "\n",
    "def sample_entropy_signal(bvp, fs=4.0):\n",
    "    from scipy.signal import find_peaks\n",
    "    bvp = sanitize_array(bvp)\n",
    "    peaks, _ = find_peaks(bvp, distance=max(int(0.5 * fs), 1), prominence=0.5 * np.std(bvp))\n",
    "    if len(peaks) < 5:\n",
    "        return np.zeros_like(bvp)\n",
    "    ibi = np.diff(peaks) / fs\n",
    "    if len(ibi) < 5:\n",
    "        return np.zeros_like(bvp)\n",
    "    window = max(int(10 * fs), 1)\n",
    "    entropy_signal = np.zeros_like(bvp)\n",
    "    for i in range(len(bvp)):\n",
    "        start = max(0, i - window)\n",
    "        segment_peaks = peaks[(peaks >= start) & (peaks < i)]\n",
    "        if len(segment_peaks) < 5:\n",
    "            continue\n",
    "        seg_ibi = np.diff(segment_peaks) / fs\n",
    "        if len(seg_ibi) < 5:\n",
    "            continue\n",
    "        m = 2\n",
    "        r = max(0.2 * np.std(seg_ibi), NUMERIC_STABILITY_EPS)\n",
    "        count_m = 0\n",
    "        count_m1 = 0\n",
    "        for j in range(len(seg_ibi) - m):\n",
    "            template = seg_ibi[j:j + m]\n",
    "            for k in range(j + 1, len(seg_ibi) - m):\n",
    "                if np.max(np.abs(template - seg_ibi[k:k + m])) <= r:\n",
    "                    count_m += 1\n",
    "                    if np.abs(seg_ibi[j + m] - seg_ibi[k + m]) <= r:\n",
    "                        count_m1 += 1\n",
    "        if count_m > 0 and count_m1 > 0:\n",
    "            entropy_signal[i] = -np.log(count_m1 / count_m)\n",
    "    return sanitize_array(entropy_signal)\n",
    "\n",
    "\n",
    "def add_wavelet_channels(raw_channels, wavelet='db4', level=3):\n",
    "    import pywt\n",
    "    raw_channels = sanitize_array(raw_channels)\n",
    "    wavelet_channels = []\n",
    "    for ch in raw_channels:\n",
    "        coeffs = pywt.wavedec(ch, wavelet, level=level)\n",
    "        for coeff in coeffs[1:]:\n",
    "            coeff = sanitize_array(coeff)\n",
    "            if len(coeff) < EXPECTED_LEN:\n",
    "                coeff = np.pad(coeff, (0, EXPECTED_LEN - len(coeff)), mode='edge')\n",
    "            elif len(coeff) > EXPECTED_LEN:\n",
    "                coeff = coeff[:EXPECTED_LEN]\n",
    "            wavelet_channels.append(coeff.astype(np.float32))\n",
    "    if not wavelet_channels:\n",
    "        return np.zeros((0, EXPECTED_LEN), dtype=np.float32)\n",
    "    stacked = np.vstack(wavelet_channels)\n",
    "    return sanitize_array(stacked)\n",
    "\n",
    "\n",
    "def extract_window_features(eda, temp, acc, bvp):\n",
    "    eda = sanitize_array(eda)\n",
    "    temp = sanitize_array(temp)\n",
    "    acc = sanitize_array(acc)\n",
    "    bvp = sanitize_array(bvp)\n",
    "    signals = [(eda, 'eda'), (temp, 'temp'), (acc, 'acc'), (bvp, 'bvp')]\n",
    "    feats = []\n",
    "    for signal, _ in signals:\n",
    "        feats.extend([\n",
    "            float(np.mean(signal)),\n",
    "            float(np.std(signal)),\n",
    "            float(np.min(signal)),\n",
    "            float(np.max(signal)),\n",
    "            float(np.percentile(signal, 25)),\n",
    "            float(np.percentile(signal, 75)),\n",
    "        ])\n",
    "        diff = np.diff(signal) if len(signal) > 1 else np.zeros(1, dtype=np.float32)\n",
    "        feats.extend([\n",
    "            float(np.mean(diff)) if len(diff) else 0.0,\n",
    "            float(np.std(diff)) if len(diff) else 0.0,\n",
    "            float(np.max(np.abs(diff))) if len(diff) else 0.0,\n",
    "        ])\n",
    "    feats.append(safe_corrcoef(eda, bvp))\n",
    "    feats.append(safe_corrcoef(eda, acc))\n",
    "    return sanitize_array(np.array(feats, dtype=np.float32))\n",
    "\n",
    "\n",
    "def phase_id_from_label(label: str) -> int:\n",
    "    if label is None:\n",
    "        return 0\n",
    "    key = label.lower()\n",
    "    return PHASE_ENCODING.get(key, 0)\n",
    "\n",
    "\n",
    "def extract_enhanced_channels(eda, temp, acc, bvp, fs=4.0):\n",
    "    from scipy.signal import hilbert\n",
    "    eda = sanitize_array(eda)\n",
    "    temp = sanitize_array(temp)\n",
    "    acc = sanitize_array(acc)\n",
    "    bvp = sanitize_array(bvp)\n",
    "    base_channels = [eda, temp, acc, bvp]\n",
    "    channels = base_channels.copy()\n",
    "    channels.extend([\n",
    "        np.diff(eda, prepend=eda[0]),\n",
    "        np.diff(temp, prepend=temp[0]),\n",
    "        np.diff(acc, prepend=acc[0]),\n",
    "        np.diff(bvp, prepend=bvp[0]),\n",
    "    ])\n",
    "    window_tonic = int(10 * fs)\n",
    "    eda_tonic = np.convolve(eda, np.ones(window_tonic) / window_tonic, mode='same')\n",
    "    eda_phasic = eda - eda_tonic\n",
    "    channels.extend([eda_tonic, eda_phasic])\n",
    "    eda_diff = np.diff(eda, prepend=eda[0])\n",
    "    eda_accel = np.diff(eda_diff, prepend=eda_diff[0])\n",
    "    channels.append(eda_accel)\n",
    "    window_short = int(5 * fs)\n",
    "    window_long = int(15 * fs)\n",
    "    eda_ma_short = np.convolve(eda, np.ones(window_short) / window_short, mode='same')\n",
    "    eda_ma_long = np.convolve(eda, np.ones(window_long) / window_long, mode='same')\n",
    "    channels.extend([eda_ma_short, eda_ma_long])\n",
    "    bvp_envelope = np.abs(hilbert(bvp))\n",
    "    channels.append(bvp_envelope)\n",
    "    channels.append(eda * bvp)\n",
    "    acc_smoothed = np.convolve(acc, np.ones(int(3 * fs)) / (3 * fs), mode='same')\n",
    "    channels.append(acc_smoothed)\n",
    "    resp_signal, _ = extract_respiratory_signal(bvp, fs)\n",
    "    channels.append(resp_signal)\n",
    "    entropy_signal = sample_entropy_signal(bvp, fs)\n",
    "    channels.append(entropy_signal)\n",
    "    wavelet_extra = add_wavelet_channels(np.array(base_channels))\n",
    "    channels.extend(list(wavelet_extra))\n",
    "    channels = [sanitize_array(ch) for ch in channels]\n",
    "    stacked = np.stack(channels, axis=0).astype(np.float32)\n",
    "    return sanitize_array(stacked)\n",
    "\n",
    "\n",
    "def build_sequence_dataset(states: List[str] = STATES, max_subjects: int = 0):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    subjects = []\n",
    "    feature_vectors = []\n",
    "    phase_ids = []\n",
    "    for state in states:\n",
    "        state_dir = DATASET_ROOT / state\n",
    "        if not state_dir.exists():\n",
    "            continue\n",
    "        subject_ids = sorted([p.name for p in state_dir.iterdir() if p.is_dir()])\n",
    "        if max_subjects and max_subjects > 0:\n",
    "            subject_ids = subject_ids[:max_subjects]\n",
    "        for subj in tqdm(subject_ids, desc=f\"{state}\"):\n",
    "            try:\n",
    "                info = load_subject_state(state, subj)\n",
    "            except Exception as exc:\n",
    "                print(f\"Skip {state}/{subj}: {exc}\")\n",
    "                continue\n",
    "            sensors = info[\"sensors\"]\n",
    "            fs_map = info[\"fs\"]\n",
    "            tags = info[\"tags\"]\n",
    "            duration = info[\"duration\"]\n",
    "\n",
    "            eda = resample_to_rate(sensors[\"EDA\"], fs_map[\"EDA\"], TARGET_FS)\n",
    "            temp = resample_to_rate(\n",
    "                sensors.get(\"TEMP\", np.zeros_like(eda)),\n",
    "                fs_map.get(\"TEMP\", TARGET_FS),\n",
    "                TARGET_FS,\n",
    "            ) if \"TEMP\" in sensors else np.zeros_like(eda)\n",
    "            acc = resample_to_rate(sensors[\"ACC_MAG\"], fs_map[\"ACC_MAG\"], TARGET_FS)\n",
    "            if \"BVP\" in sensors and \"BVP\" in fs_map:\n",
    "                bvp = resample_to_rate(sensors[\"BVP\"], fs_map[\"BVP\"], TARGET_FS)\n",
    "            else:\n",
    "                bvp = np.zeros_like(eda)\n",
    "\n",
    "            eda = sanitize_array(eda)\n",
    "            temp = sanitize_array(temp)\n",
    "            acc = sanitize_array(acc)\n",
    "            bvp = sanitize_array(bvp)\n",
    "\n",
    "            intervals = make_label_intervals(state, subj, tags, duration)\n",
    "            windows = window_intervals(duration, WINDOW_SECONDS, WINDOW_STEP_SECONDS)\n",
    "\n",
    "            for win in windows:\n",
    "                label_name, span_meta = assign_label(win, intervals)\n",
    "                if label_name is None or span_meta is None:\n",
    "                    continue\n",
    "                start_idx = int(round(win[0] * TARGET_FS))\n",
    "                end_idx = start_idx + EXPECTED_LEN\n",
    "                eda_win = sanitize_array(_slice_or_pad(eda, start_idx, end_idx))\n",
    "                temp_win = sanitize_array(_slice_or_pad(temp, start_idx, end_idx))\n",
    "                acc_win = sanitize_array(_slice_or_pad(acc, start_idx, end_idx))\n",
    "                bvp_win = sanitize_array(_slice_or_pad(bvp, start_idx, end_idx))\n",
    "\n",
    "                stress_stage = span_meta.get(\"stage\") if isinstance(span_meta, dict) else None\n",
    "                stress_level = span_meta.get(\"stress_level\") if isinstance(span_meta, dict) else None\n",
    "                if label_name == \"stress\":\n",
    "                    if stress_level is None or np.isnan(stress_level):\n",
    "                        continue\n",
    "                else:\n",
    "                    stress_level = 0.0\n",
    "                phase_label = stress_stage if stress_stage else label_name\n",
    "                stress_class = stress_bucket(stress_level, phase_label)\n",
    "\n",
    "                tensor = sanitize_array(extract_enhanced_channels(eda_win, temp_win, acc_win, bvp_win, TARGET_FS))\n",
    "                stats = sanitize_array(extract_window_features(eda_win, temp_win, acc_win, bvp_win))\n",
    "                sequences.append(tensor)\n",
    "                labels.append(stress_class)\n",
    "                subjects.append(base_subject_id(subj))\n",
    "                feature_vectors.append(stats)\n",
    "                phase_ids.append(phase_id_from_label(phase_label))\n",
    "\n",
    "    sequences = sanitize_array(np.stack(sequences))\n",
    "    labels = np.array(labels)\n",
    "    subjects = np.array(subjects)\n",
    "    feature_vectors = sanitize_array(np.stack(feature_vectors))\n",
    "    phase_ids = np.array(phase_ids, dtype=np.int64)\n",
    "    return sequences, labels, subjects, feature_vectors, phase_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fa9274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:13.522809Z",
     "iopub.status.busy": "2025-12-10T21:54:13.522295Z",
     "iopub.status.idle": "2025-12-10T21:54:31.503452Z",
     "shell.execute_reply": "2025-12-10T21:54:31.502680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52dbf86c496142b0911ca1f591512512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "STRESS:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip STRESS/f14_a: No columns to parse from file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd63afeed414be2be2d82d4f5eaeab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AEROBIC:   0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "572a4d3dee6247418880bce9071d2788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ANAEROBIC:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET STATISTICS\n",
      "================================================================================\n",
      "Raw sequences: (6788, 30, 240)\n",
      "Aux features: (6788, 38)\n",
      "Phase IDs: [0 2 3 5 9]\n",
      "Label distribution:\n",
      "  high_stress         :   462 (  6.8%)\n",
      "  low_stress          :    91 (  1.3%)\n",
      "  moderate_stress     :   664 (  9.8%)\n",
      "  no_stress           :  5571 ( 82.1%)\n",
      "================================================================================\n",
      "APPLYING SUBJECT-SPECIFIC BASELINE NORMALIZATION\n",
      "================================================================================\n",
      "  S01     :  155 rest windows → baseline\n",
      "  S02     :  188 rest windows → baseline\n",
      "  S03     :  122 rest windows → baseline\n",
      "  S04     :  140 rest windows → baseline\n",
      "  S05     :  136 rest windows → baseline\n",
      "  S06     :  124 rest windows → baseline\n",
      "  S07     :  124 rest windows → baseline\n",
      "  S08     :  135 rest windows → baseline\n",
      "  S09     :  137 rest windows → baseline\n",
      "  S10     :  137 rest windows → baseline\n",
      "  S11     :  130 rest windows → baseline\n",
      "  S12     :   69 rest windows → baseline\n",
      "  S13     :  140 rest windows → baseline\n",
      "  S14     :  143 rest windows → baseline\n",
      "  S15     :  138 rest windows → baseline\n",
      "  S16     :  146 rest windows → baseline\n",
      "  S17     :  140 rest windows → baseline\n",
      "  S18     :  140 rest windows → baseline\n",
      "  f01     :  217 rest windows → baseline\n",
      "  f02     :  195 rest windows → baseline\n",
      "  f03     :  241 rest windows → baseline\n",
      "  f04     :  197 rest windows → baseline\n",
      "  f05     :  277 rest windows → baseline\n",
      "  f06     :  198 rest windows → baseline\n",
      "  f07     :  214 rest windows → baseline\n",
      "  f08     :  204 rest windows → baseline\n",
      "  f09     :  198 rest windows → baseline\n",
      "  f10     :  171 rest windows → baseline\n",
      "  f11     :  204 rest windows → baseline\n",
      "  f12     :  263 rest windows → baseline\n",
      "  f13     :  246 rest windows → baseline\n",
      "  f14     :   29 rest windows → baseline\n",
      "  f15     :   95 rest windows → baseline\n",
      "  f16     :   57 rest windows → baseline\n",
      "  f17     :   61 rest windows → baseline\n",
      "  f18     :   60 rest windows → baseline\n",
      "✓ Subject-specific normalization complete\n",
      "================================================================================\n",
      "FINAL DATASET: (6788, 30, 240)\n",
      "Feature matrix: (6788, 38)\n",
      "Subjects: 36\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "sequences, labels, subjects, feature_vectors, phase_ids = build_sequence_dataset(max_subjects=MAX_SUBJECTS if MAX_SUBJECTS else 0)\n",
    "\n",
    "num_channels = sequences.shape[1]\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Raw sequences: {sequences.shape}\")\n",
    "print(f\"Aux features: {feature_vectors.shape}\")\n",
    "print(f\"Phase IDs: {np.unique(phase_ids)}\")\n",
    "print(f\"Label distribution:\")\n",
    "label_dist = pd.Series(labels).value_counts().sort_index()\n",
    "for label, count in label_dist.items():\n",
    "    pct = 100 * count / len(labels)\n",
    "    print(f\"  {label:20s}: {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "if APPLY_CHANNEL_NORMALIZATION and sequences.size:\n",
    "    print(\"\" + \"=\" * 80)\n",
    "    print(\"APPLYING SUBJECT-SPECIFIC BASELINE NORMALIZATION\")\n",
    "    print(\"=\" * 80)\n",
    "    normalized = sequences.copy()\n",
    "    for subject in np.unique(subjects):\n",
    "        subject_mask = subjects == subject\n",
    "        rest_mask = subject_mask & (labels == 'no_stress')\n",
    "        if rest_mask.sum() > 0:\n",
    "            baseline_mean = sequences[rest_mask].mean(axis=(0, 2), keepdims=True)\n",
    "            baseline_std = sequences[rest_mask].std(axis=(0, 2), keepdims=True) + 1e-6\n",
    "            print(f\"  {subject:8s}: {rest_mask.sum():4d} rest windows → baseline\")\n",
    "        else:\n",
    "            baseline_mean = sequences[subject_mask].mean(axis=(0, 2), keepdims=True)\n",
    "            baseline_std = sequences[subject_mask].std(axis=(0, 2), keepdims=True) + 1e-6\n",
    "            print(f\"  {subject:8s}: {subject_mask.sum():4d} total windows (NO REST DATA)\")\n",
    "        normalized[subject_mask] = (sequences[subject_mask] - baseline_mean) / baseline_std\n",
    "    sequences = normalized\n",
    "    print(\"✓ Subject-specific normalization complete\")\n",
    "\n",
    "print(\"\" + \"=\" * 80)\n",
    "print(f\"FINAL DATASET: {sequences.shape}\")\n",
    "print(f\"Feature matrix: {feature_vectors.shape}\")\n",
    "print(f\"Subjects: {len(np.unique(subjects))}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9eb44aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:31.506411Z",
     "iopub.status.busy": "2025-12-10T21:54:31.506021Z",
     "iopub.status.idle": "2025-12-10T21:54:31.510342Z",
     "shell.execute_reply": "2025-12-10T21:54:31.509235Z"
    }
   },
   "outputs": [],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences: np.ndarray, features: np.ndarray, phase_ids: np.ndarray, labels: np.ndarray):\n",
    "        self.sequences = torch.from_numpy(sequences)\n",
    "        self.features = torch.from_numpy(features)\n",
    "        self.phase_ids = torch.from_numpy(phase_ids).long()\n",
    "        self.labels = torch.from_numpy(labels).long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.features[idx], self.phase_ids[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4a1b2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:31.512696Z",
     "iopub.status.busy": "2025-12-10T21:54:31.512457Z",
     "iopub.status.idle": "2025-12-10T21:54:31.517570Z",
     "shell.execute_reply": "2025-12-10T21:54:31.516923Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, channels: int, kernel_size: int = 5, dilation: int = 1):\n",
    "        super().__init__()\n",
    "        padding = ((kernel_size - 1) // 2) * dilation\n",
    "        self.conv1 = nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        self.conv2 = nn.Conv1d(channels, channels, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.bn2 = nn.BatchNorm1d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class MultiScaleSequenceEncoder(nn.Module):\n",
    "    def __init__(self, input_channels: int, cnn_channels: int = 64):\n",
    "        super().__init__()\n",
    "        self.conv_short = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, cnn_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.ReLU(),\n",
    "            ResNetBlock(cnn_channels, kernel_size=3, dilation=1),\n",
    "            ResNetBlock(cnn_channels, kernel_size=3, dilation=1),\n",
    "        )\n",
    "        self.conv_medium = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, cnn_channels, kernel_size=7, padding=3),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.ReLU(),\n",
    "            ResNetBlock(cnn_channels, kernel_size=7, dilation=2),\n",
    "            ResNetBlock(cnn_channels, kernel_size=7, dilation=2),\n",
    "        )\n",
    "        self.conv_long = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, cnn_channels, kernel_size=15, padding=7),\n",
    "            nn.BatchNorm1d(cnn_channels),\n",
    "            nn.ReLU(),\n",
    "            ResNetBlock(cnn_channels, kernel_size=15, dilation=4),\n",
    "            ResNetBlock(cnn_channels, kernel_size=15, dilation=4),\n",
    "        )\n",
    "        merged_channels = cnn_channels * 3\n",
    "        self.merge = nn.Sequential(\n",
    "            nn.Conv1d(merged_channels, 128, kernel_size=1),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=128,\n",
    "            hidden_size=128,\n",
    "            num_layers=3,\n",
    "            dropout=0.3,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=256, num_heads=8, dropout=0.2, batch_first=True)\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat_short = self.conv_short(x)\n",
    "        feat_medium = self.conv_medium(x)\n",
    "        feat_long = self.conv_long(x)\n",
    "        merged = torch.cat([feat_short, feat_medium, feat_long], dim=1)\n",
    "        merged = self.merge(merged)\n",
    "        lstm_in = merged.transpose(1, 2)\n",
    "        lstm_out, _ = self.lstm(lstm_in)\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        temporal_features = attn_out.mean(dim=1)\n",
    "        cnn_features = self.global_pool(merged).squeeze(-1)\n",
    "        return torch.cat([temporal_features, cnn_features], dim=1)\n",
    "\n",
    "\n",
    "class PhaseAwareHybridNet(nn.Module):\n",
    "    def __init__(self, input_channels: int, num_features: int, num_classes: int, num_phases: int = len(PHASE_ENCODING) + 2):\n",
    "        super().__init__()\n",
    "        self.sequence_encoder = MultiScaleSequenceEncoder(input_channels)\n",
    "        self.feature_branch = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        self.phase_embedding = nn.Embedding(num_phases, 32)\n",
    "        fusion_in = 384 + 64 + 32\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_in, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, sequences, stats, phase_ids):\n",
    "        seq_feat = self.sequence_encoder(sequences)\n",
    "        stat_feat = self.feature_branch(stats)\n",
    "        phase_feat = self.phase_embedding(phase_ids)\n",
    "        combined = torch.cat([seq_feat, stat_feat, phase_feat], dim=1)\n",
    "        return self.classifier(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d812a0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:31.519995Z",
     "iopub.status.busy": "2025-12-10T21:54:31.519634Z",
     "iopub.status.idle": "2025-12-10T21:54:31.524400Z",
     "shell.execute_reply": "2025-12-10T21:54:31.523839Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LABEL ENCODING\n",
      "================================================================================\n",
      "Classes: {np.str_('high_stress'): 0, np.str_('low_stress'): 1, np.str_('moderate_stress'): 2, np.str_('no_stress'): 3}\n",
      "Encoded label distribution:\n",
      "  0: high_stress          →   462 samples\n",
      "  1: low_stress           →    91 samples\n",
      "  2: moderate_stress      →   664 samples\n",
      "  3: no_stress            →  5571 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "LR = 5e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_GRAD_NORM = 0.5\n",
    "USE_MIXED_PRECISION = torch.cuda.is_available()\n",
    "\n",
    "le = LabelEncoder()\n",
    "encoded_labels = le.fit_transform(labels)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LABEL ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Classes:\", dict(zip(le.classes_, range(num_classes))))\n",
    "print(f\"Encoded label distribution:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    count = (encoded_labels == i).sum()\n",
    "    print(f\"  {i}: {class_name:20s} → {count:5d} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869da7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T21:54:31.527050Z",
     "iopub.status.busy": "2025-12-10T21:54:31.526744Z",
     "iopub.status.idle": "2025-12-10T21:55:06.772904Z",
     "shell.execute_reply": "2025-12-10T21:55:06.772238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PHASE 2 PREPARATION: DATA BALANCING + TRAINING CONFIG\n",
      "================================================================================\n",
      "Split strategy: StratifiedGroupKFold fold 1/5\n",
      "  Train windows: 5511 from 30 subjects\n",
      "  Test windows:  1277 from 6 subjects\n",
      "Train subjects: S01, S03, S04, S05, S06, S08, S09, S10, S11, S12, S13, S15, S16, S17, S18, f01, f02, f03, f04, f05, f06, f08, f10, f12, f13, f14, f15, f16, f17, f18\n",
      "Test subjects: S02, S07, S14, f07, f09, f11\n",
      "Dataset split overview:\n",
      "  Train: (5511, 30, 240)\n",
      "  Test:  (1277, 30, 240)\n",
      "Train class distribution (BEFORE balancing):\n",
      "  high_stress         :   351 (  6.4%)\n",
      "  low_stress          :    53 (  1.0%)\n",
      "  moderate_stress     :   607 ( 11.0%)\n",
      "  no_stress           :  4500 ( 81.7%)\n",
      "Test class distribution:\n",
      "  high_stress         :   111 (  8.7%)\n",
      "  low_stress          :    38 (  3.0%)\n",
      "  moderate_stress     :    57 (  4.5%)\n",
      "  no_stress           :  1071 ( 83.9%)\n",
      "================================================================================\n",
      "TEMPORAL AUGMENTATION\n",
      "================================================================================\n",
      "  high_stress: 351 samples → augmenting 1x\n",
      "  low_stress: 53 samples → augmenting 2x\n",
      "  moderate_stress: 607 samples → augmenting 1x\n",
      "================================================================================\n",
      "FINAL CLASS DISTRIBUTION\n",
      "================================================================================\n",
      "Train: (6575, 30, 240)\n",
      "  high_stress         :   702 ( 10.7%)\n",
      "  low_stress          :   159 (  2.4%)\n",
      "  moderate_stress     :  1214 ( 18.5%)\n",
      "  no_stress           :  4500 ( 68.4%)\n",
      "Class weights / alpha for focal loss: {np.str_('high_stress'): np.float32(2.3415241), np.str_('low_stress'): np.float32(10.33805), np.str_('moderate_stress'): np.float32(1.3539951), np.str_('no_stress'): np.float32(0.36527777)}\n",
      "================================================================================\n",
      "TRAINING: 40 epochs (mixed precision: True)\n",
      "================================================================================\n",
      "Epoch 01 | Train Loss: 0.6496 | Val Loss: 0.5546 | Val Acc: 0.625 | Val F1 (macro): 0.388 | Val F1 (weighted): 0.703\n",
      "Epoch 02 | Train Loss: 0.2471 | Val Loss: 0.5282 | Val Acc: 0.914 | Val F1 (macro): 0.591 | Val F1 (weighted): 0.915\n",
      "Epoch 03 | Train Loss: 0.1620 | Val Loss: 0.9298 | Val Acc: 0.898 | Val F1 (macro): 0.490 | Val F1 (weighted): 0.888\n",
      "Epoch 04 | Train Loss: 0.2024 | Val Loss: 0.9870 | Val Acc: 0.895 | Val F1 (macro): 0.464 | Val F1 (weighted): 0.882\n",
      "Epoch 05 | Train Loss: 0.1307 | Val Loss: 1.2323 | Val Acc: 0.896 | Val F1 (macro): 0.477 | Val F1 (weighted): 0.886\n",
      "Epoch 06 | Train Loss: 0.0651 | Val Loss: 1.2287 | Val Acc: 0.898 | Val F1 (macro): 0.516 | Val F1 (weighted): 0.893\n",
      "Epoch 07 | Train Loss: 0.1125 | Val Loss: 1.3669 | Val Acc: 0.889 | Val F1 (macro): 0.441 | Val F1 (weighted): 0.886\n",
      "Epoch 08 | Train Loss: 0.0950 | Val Loss: 1.9524 | Val Acc: 0.885 | Val F1 (macro): 0.399 | Val F1 (weighted): 0.874\n",
      "Epoch 09 | Train Loss: 0.0688 | Val Loss: 1.3007 | Val Acc: 0.892 | Val F1 (macro): 0.485 | Val F1 (weighted): 0.890\n",
      "Epoch 10 | Train Loss: 0.0618 | Val Loss: 1.4741 | Val Acc: 0.890 | Val F1 (macro): 0.469 | Val F1 (weighted): 0.889\n",
      "Epoch 11 | Train Loss: 0.0389 | Val Loss: 1.2969 | Val Acc: 0.894 | Val F1 (macro): 0.520 | Val F1 (weighted): 0.895\n",
      "Epoch 12 | Train Loss: 0.0422 | Val Loss: 1.4696 | Val Acc: 0.884 | Val F1 (macro): 0.434 | Val F1 (weighted): 0.883\n",
      "Epoch 13 | Train Loss: 0.0687 | Val Loss: 1.4775 | Val Acc: 0.885 | Val F1 (macro): 0.428 | Val F1 (weighted): 0.885\n",
      "Epoch 14 | Train Loss: 0.0891 | Val Loss: 1.2863 | Val Acc: 0.900 | Val F1 (macro): 0.535 | Val F1 (weighted): 0.895\n",
      "Epoch 15 | Train Loss: 0.0804 | Val Loss: 1.2650 | Val Acc: 0.900 | Val F1 (macro): 0.539 | Val F1 (weighted): 0.897\n",
      "Epoch 16 | Train Loss: 0.0661 | Val Loss: 1.1613 | Val Acc: 0.898 | Val F1 (macro): 0.545 | Val F1 (weighted): 0.897\n",
      "Epoch 17 | Train Loss: 0.0450 | Val Loss: 1.1873 | Val Acc: 0.899 | Val F1 (macro): 0.550 | Val F1 (weighted): 0.896\n",
      "Epoch 18 | Train Loss: 0.0427 | Val Loss: 1.5346 | Val Acc: 0.885 | Val F1 (macro): 0.451 | Val F1 (weighted): 0.882\n",
      "Epoch 19 | Train Loss: 0.0379 | Val Loss: 1.3279 | Val Acc: 0.886 | Val F1 (macro): 0.485 | Val F1 (weighted): 0.885\n",
      "Epoch 20 | Train Loss: 0.0424 | Val Loss: 1.2119 | Val Acc: 0.894 | Val F1 (macro): 0.541 | Val F1 (weighted): 0.895\n",
      "Epoch 21 | Train Loss: 0.0309 | Val Loss: 1.3260 | Val Acc: 0.893 | Val F1 (macro): 0.522 | Val F1 (weighted): 0.893\n",
      "Epoch 22 | Train Loss: 0.0281 | Val Loss: 1.3713 | Val Acc: 0.893 | Val F1 (macro): 0.511 | Val F1 (weighted): 0.893\n",
      "Epoch 23 | Train Loss: 0.0316 | Val Loss: 1.5284 | Val Acc: 0.885 | Val F1 (macro): 0.444 | Val F1 (weighted): 0.883\n",
      "Epoch 24 | Train Loss: 0.0302 | Val Loss: 1.3277 | Val Acc: 0.894 | Val F1 (macro): 0.532 | Val F1 (weighted): 0.896\n",
      "Epoch 25 | Train Loss: 0.0315 | Val Loss: 1.2423 | Val Acc: 0.897 | Val F1 (macro): 0.545 | Val F1 (weighted): 0.898\n",
      "Epoch 26 | Train Loss: 0.0829 | Val Loss: 1.7731 | Val Acc: 0.886 | Val F1 (macro): 0.413 | Val F1 (weighted): 0.882\n",
      "Epoch 27 | Train Loss: 0.0991 | Val Loss: 1.1367 | Val Acc: 0.905 | Val F1 (macro): 0.579 | Val F1 (weighted): 0.902\n",
      "Epoch 28 | Train Loss: 0.0521 | Val Loss: 1.3162 | Val Acc: 0.901 | Val F1 (macro): 0.558 | Val F1 (weighted): 0.898\n",
      "Epoch 29 | Train Loss: 0.0500 | Val Loss: 1.2242 | Val Acc: 0.898 | Val F1 (macro): 0.554 | Val F1 (weighted): 0.900\n",
      "Epoch 30 | Train Loss: 0.0520 | Val Loss: 1.1927 | Val Acc: 0.900 | Val F1 (macro): 0.561 | Val F1 (weighted): 0.900\n",
      "Epoch 31 | Train Loss: 0.0486 | Val Loss: 1.0942 | Val Acc: 0.906 | Val F1 (macro): 0.600 | Val F1 (weighted): 0.905\n",
      "Epoch 32 | Train Loss: 0.0334 | Val Loss: 1.1408 | Val Acc: 0.907 | Val F1 (macro): 0.592 | Val F1 (weighted): 0.901\n",
      "Epoch 33 | Train Loss: 0.0469 | Val Loss: 1.2658 | Val Acc: 0.895 | Val F1 (macro): 0.531 | Val F1 (weighted): 0.894\n",
      "Epoch 34 | Train Loss: 0.0338 | Val Loss: 1.2253 | Val Acc: 0.907 | Val F1 (macro): 0.595 | Val F1 (weighted): 0.904\n",
      "Epoch 35 | Train Loss: 0.0348 | Val Loss: 1.1801 | Val Acc: 0.906 | Val F1 (macro): 0.597 | Val F1 (weighted): 0.904\n",
      "Epoch 36 | Train Loss: 0.0392 | Val Loss: 1.3701 | Val Acc: 0.897 | Val F1 (macro): 0.535 | Val F1 (weighted): 0.894\n",
      "Epoch 37 | Train Loss: 0.0380 | Val Loss: 1.4838 | Val Acc: 0.891 | Val F1 (macro): 0.478 | Val F1 (weighted): 0.889\n",
      "Epoch 38 | Train Loss: 0.0418 | Val Loss: 1.2728 | Val Acc: 0.896 | Val F1 (macro): 0.545 | Val F1 (weighted): 0.895\n",
      "Epoch 39 | Train Loss: 0.0331 | Val Loss: 1.3005 | Val Acc: 0.891 | Val F1 (macro): 0.527 | Val F1 (weighted): 0.890\n",
      "Epoch 40 | Train Loss: 0.0272 | Val Loss: 1.2364 | Val Acc: 0.894 | Val F1 (macro): 0.542 | Val F1 (weighted): 0.894\n",
      "================================================================================\n",
      "FINAL EVALUATION ON TEST SET\n",
      "================================================================================\n",
      "OVERALL METRICS (direct predictions):\n",
      "  Accuracy:         0.8943 (89.4%)\n",
      "  Macro F1:         0.5425 (54.2%)\n",
      "  Weighted F1:      0.8937 (89.4%)\n",
      "  Macro Precision:  0.6272\n",
      "  Macro Recall:     0.5560\n",
      "Two-stage metrics:\n",
      "  Accuracy:         0.8943 (89.4%)\n",
      "  Macro F1:         0.5425 (54.2%)\n",
      "  Weighted F1:      0.8937 (89.4%)\n",
      "================================================================================\n",
      "PER-CLASS METRICS\n",
      "================================================================================\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    high_stress     0.5000    0.1802    0.2649       111\n",
      "     low_stress     0.7727    0.4474    0.5667        38\n",
      "moderate_stress     0.2361    0.5965    0.3383        57\n",
      "      no_stress     1.0000    1.0000    1.0000      1071\n",
      "\n",
      "       accuracy                         0.8943      1277\n",
      "      macro avg     0.6272    0.5560    0.5425      1277\n",
      "   weighted avg     0.9157    0.8943    0.8937      1277\n",
      "\n",
      "================================================================================\n",
      "CONFUSION MATRIX\n",
      "================================================================================\n",
      "                 high_stress  low_stress  moderate_stress  no_stress\n",
      "high_stress               20           1               90          0\n",
      "low_stress                 1          17               20          0\n",
      "moderate_stress           19           4               34          0\n",
      "no_stress                  0           0                0       1071\n",
      "================================================================================\n",
      "SUBJECT-LEVEL PERFORMANCE (TEST SET)\n",
      "================================================================================\n",
      "subject  samples accuracy macro_f1\n",
      "    S02      204    0.951    0.393\n",
      "    S07      139    0.892    0.333\n",
      "    S14      162    0.889    0.367\n",
      "    f07      268    0.896    0.537\n",
      "    f09      249    0.880    0.423\n",
      "    f11      255    0.867    0.375\n",
      "================================================================================\n",
      "COMPARISON TO BASELINE\n",
      "================================================================================\n",
      "BASELINE (before Phase 1):\n",
      "  Accuracy:    75.9%\n",
      "  Macro F1:    36.0%\n",
      "PHASE 2+ HYBRID (multi-scale CNN + hybrid features + focal loss):\n",
      "  Accuracy:    89.4%  (+13.5pp)\n",
      "  Macro F1:    54.2%  (+18.2pp)\n",
      "✓ TARGET ACHIEVED! Macro F1 improved by +18.2pp (target: +8-12pp)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GroupKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from scipy.interpolate import interp1d\n",
    "from contextlib import nullcontext\n",
    "import math\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2 PREPARATION: DATA BALANCING + TRAINING CONFIG\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "stress_class_names = {\"high_stress\", \"low_stress\", \"moderate_stress\"}\n",
    "stress_indices = [i for i, name in enumerate(le.classes_) if name in stress_class_names]\n",
    "no_stress_idx = int(np.where(le.classes_ == 'no_stress')[0][0]) if 'no_stress' in le.classes_ else None\n",
    "stress_idx_tensor = torch.tensor(stress_indices, device=device) if stress_indices else None\n",
    "\n",
    "\n",
    "def time_warp_augment(sequence, warp_factor):\n",
    "    channels, length = sequence.shape\n",
    "    new_length = int(length * warp_factor)\n",
    "    warped = []\n",
    "    for ch in range(channels):\n",
    "        if new_length < 4:\n",
    "            warped.append(sequence[ch])\n",
    "            continue\n",
    "        f = interp1d(np.arange(length), sequence[ch], kind='cubic', fill_value='extrapolate')\n",
    "        new_indices = np.linspace(0, length - 1, new_length)\n",
    "        warped_ch = f(new_indices)\n",
    "        warped_ch_resampled = np.interp(\n",
    "            np.arange(length),\n",
    "            np.linspace(0, length - 1, new_length),\n",
    "            warped_ch,\n",
    "        )\n",
    "        warped.append(warped_ch_resampled)\n",
    "    return np.array(warped, dtype=np.float32)\n",
    "\n",
    "\n",
    "def stats_from_sequence(seq):\n",
    "    eda, temp, acc, bvp = seq[0], seq[1], seq[2], seq[3]\n",
    "    return extract_window_features(eda, temp, acc, bvp)\n",
    "\n",
    "\n",
    "def temporal_augmentation(X, feats, phases, y, augment_counts):\n",
    "    if not len(X):\n",
    "        return X, feats, phases, y\n",
    "    X_aug = list(X)\n",
    "    F_aug = list(feats)\n",
    "    P_aug = list(phases)\n",
    "    y_aug = list(y)\n",
    "    for class_idx, class_name in enumerate(le.classes_):\n",
    "        if class_name not in augment_counts:\n",
    "            continue\n",
    "        class_indices = np.where(y == class_idx)[0]\n",
    "        if not len(class_indices):\n",
    "            continue\n",
    "        aug_factor = augment_counts[class_name]\n",
    "        print(f\"  {class_name}: {len(class_indices)} samples → augmenting {aug_factor}x\")\n",
    "        for idx in class_indices:\n",
    "            sample = X[idx]\n",
    "            phase = phases[idx]\n",
    "            for _ in range(aug_factor):\n",
    "                warp_factor = np.random.uniform(0.95, 1.05)\n",
    "                aug_sample = time_warp_augment(sample, warp_factor)\n",
    "                noise_std = 0.03 * np.std(sample, axis=1, keepdims=True)\n",
    "                aug_sample = aug_sample + np.random.normal(0, noise_std, aug_sample.shape).astype(np.float32)\n",
    "                shift_range = max(1, int(0.1 * sample.shape[1]))\n",
    "                shift = np.random.randint(-shift_range, shift_range)\n",
    "                aug_sample = np.roll(aug_sample, shift, axis=1)\n",
    "                aug_sample = np.nan_to_num(aug_sample, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "                X_aug.append(aug_sample)\n",
    "                F_aug.append(stats_from_sequence(aug_sample))\n",
    "                P_aug.append(phase)\n",
    "                y_aug.append(class_idx)\n",
    "    return (\n",
    "        np.array(X_aug, dtype=np.float32),\n",
    "        np.array(F_aug, dtype=np.float32),\n",
    "        np.array(P_aug, dtype=np.int64),\n",
    "        np.array(y_aug),\n",
    "    )\n",
    "\n",
    "\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.995):\n",
    "        self.decay = decay\n",
    "        self.shadow = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in model.named_parameters()\n",
    "            if param.requires_grad\n",
    "        }\n",
    "        self.backup = None\n",
    "\n",
    "    def update(self, model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            self.shadow[name].mul_(self.decay).add_(param.detach(), alpha=1.0 - self.decay)\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {\n",
    "            name: param.detach().clone()\n",
    "            for name, param in model.named_parameters()\n",
    "            if param.requires_grad\n",
    "        }\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.shadow:\n",
    "                param.data.copy_(self.shadow[name])\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.backup is None:\n",
    "            return\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data.copy_(self.backup[name])\n",
    "        self.backup = None\n",
    "\n",
    "\n",
    "if GROUP_SPLIT:\n",
    "    splitter_desc = \"GroupKFold\"\n",
    "    if USE_STRATIFIED_GROUP_SPLIT and 'StratifiedGroupKFold' in globals() and StratifiedGroupKFold is not None:\n",
    "        splitter = StratifiedGroupKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=SEED)\n",
    "        splitter_desc = \"StratifiedGroupKFold\"\n",
    "    else:\n",
    "        splitter = GroupKFold(n_splits=NUM_FOLDS)\n",
    "    splits = list(splitter.split(sequences, encoded_labels, groups=subjects))\n",
    "    if FOLD_INDEX >= len(splits):\n",
    "        raise ValueError(f\"FOLD_INDEX {FOLD_INDEX} out of range for {NUM_FOLDS} folds.\")\n",
    "    train_idx, test_idx = splits[FOLD_INDEX]\n",
    "    split_desc = f\"{splitter_desc} fold {FOLD_INDEX + 1}/{NUM_FOLDS}\"\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        np.arange(len(sequences)),\n",
    "        test_size=0.2,\n",
    "        stratify=encoded_labels,\n",
    "        random_state=SEED,\n",
    "    )\n",
    "    split_desc = \"Stratified random 80/20 split\"\n",
    "\n",
    "X_train, X_test = sequences[train_idx], sequences[test_idx]\n",
    "feat_train, feat_test = feature_vectors[train_idx], feature_vectors[test_idx]\n",
    "phase_train, phase_test = phase_ids[train_idx], phase_ids[test_idx]\n",
    "y_train, y_test = encoded_labels[train_idx], encoded_labels[test_idx]\n",
    "train_subjects = subjects[train_idx]\n",
    "test_subjects = subjects[test_idx]\n",
    "held_out_subjects = test_subjects.copy()\n",
    "\n",
    "print(f\"Split strategy: {split_desc}\")\n",
    "print(f\"  Train windows: {X_train.shape[0]} from {len(np.unique(train_subjects))} subjects\")\n",
    "print(f\"  Test windows:  {X_test.shape[0]} from {len(np.unique(test_subjects))} subjects\")\n",
    "print(\"Train subjects:\", ', '.join(sorted(np.unique(train_subjects))))\n",
    "print(\"Test subjects:\", ', '.join(sorted(np.unique(test_subjects))))\n",
    "\n",
    "print(f\"Dataset split overview:\")\n",
    "print(f\"  Train: {X_train.shape}\")\n",
    "print(f\"  Test:  {X_test.shape}\")\n",
    "\n",
    "print(f\"Train class distribution (BEFORE balancing):\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    count = (y_train == i).sum()\n",
    "    pct = 100 * count / len(y_train)\n",
    "    print(f\"  {class_name:20s}: {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"Test class distribution:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    count = (y_test == i).sum()\n",
    "    pct = 100 * count / len(y_test)\n",
    "    print(f\"  {class_name:20s}: {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "if APPLY_TEMPORAL_AUG:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TEMPORAL AUGMENTATION\")\n",
    "    print(\"=\" * 80)\n",
    "    X_train, feat_train, phase_train, y_train = temporal_augmentation(\n",
    "        X_train,\n",
    "        feat_train,\n",
    "        phase_train,\n",
    "        y_train,\n",
    "        TEMPORAL_AUG_COUNTS,\n",
    "    )\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    count = (y_train == i).sum()\n",
    "    pct = 100 * count / len(y_train)\n",
    "    print(f\"  {class_name:20s}: {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "feat_train = scaler.fit_transform(feat_train).astype(np.float32)\n",
    "feat_test = scaler.transform(feat_test).astype(np.float32)\n",
    "feat_train = np.nan_to_num(feat_train, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "feat_test = np.nan_to_num(feat_test, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_train = np.nan_to_num(X_train.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_test = np.nan_to_num(X_test.astype(np.float32), nan=0.0, posinf=0.0, neginf=0.0)\n",
    "phase_train = phase_train.astype(np.int64)\n",
    "phase_test = phase_test.astype(np.int64)\n",
    "\n",
    "def ensure_long_enough(arr, target_len):\n",
    "    if arr.ndim != 3:\n",
    "        return arr\n",
    "    if arr.shape[2] == target_len:\n",
    "        return arr\n",
    "    if arr.shape[2] > target_len:\n",
    "        return arr[:, :, :target_len]\n",
    "    pad = target_len - arr.shape[2]\n",
    "    return np.pad(arr, ((0, 0), (0, 0), (0, pad)), mode='edge')\n",
    "\n",
    "X_train = ensure_long_enough(X_train, EXPECTED_LEN)\n",
    "X_test = ensure_long_enough(X_test, EXPECTED_LEN)\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, feat_train, phase_train, y_train)\n",
    "test_dataset = SequenceDataset(X_test, feat_test, phase_test, y_test)\n",
    "\n",
    "class_counts = np.bincount(y_train, minlength=num_classes).clip(min=1).astype(np.float32)\n",
    "class_weights = len(y_train) / (num_classes * class_counts)\n",
    "class_weights = np.nan_to_num(class_weights, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "alpha_tensor = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "\n",
    "sample_weights = 1.0 / class_counts[y_train]\n",
    "sample_weights = np.nan_to_num(sample_weights, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "sample_weights = torch.tensor(sample_weights, dtype=torch.double)\n",
    "num_samples = BATCH_SIZE * math.ceil(len(y_train) / BATCH_SIZE)\n",
    "train_sampler = WeightedRandomSampler(sample_weights, num_samples=num_samples, replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Class weights / alpha for focal loss: {dict(zip(le.classes_, class_weights))}\")\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0, label_smoothing=0.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(\n",
    "            logits,\n",
    "            targets,\n",
    "            reduction='none',\n",
    "            label_smoothing=self.label_smoothing,\n",
    "        )\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        if self.alpha is not None:\n",
    "            loss = self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss\n",
    "        else:\n",
    "            loss = (1 - pt) ** self.gamma * ce_loss\n",
    "        return loss.mean()\n",
    "\n",
    "num_channels = sequences.shape[1]\n",
    "num_features = feature_vectors.shape[1]\n",
    "model = PhaseAwareHybridNet(\n",
    "    input_channels=num_channels,\n",
    "    num_features=num_features,\n",
    "    num_classes=num_classes,\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY, betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "criterion = FocalLoss(alpha=alpha_tensor, gamma=2.0, label_smoothing=LABEL_SMOOTHING)\n",
    "def _null_autocast():\n",
    "    return nullcontext()\n",
    "\n",
    "scaler = None\n",
    "autocast_cm = _null_autocast\n",
    "if USE_MIXED_PRECISION and device.type == 'cuda':\n",
    "    try:\n",
    "        scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "        def autocast_cm():\n",
    "            return torch.amp.autocast('cuda', enabled=True)\n",
    "    except TypeError:\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "        def autocast_cm():\n",
    "            return torch.cuda.amp.autocast(enabled=True)\n",
    "ema = ModelEMA(model, decay=EMA_DECAY) if EMA_DECAY else None\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"TRAINING: {EPOCHS} epochs (mixed precision: {USE_MIXED_PRECISION})\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "best_model_state = None\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for xb, fb, pb, yb in train_loader:\n",
    "        xb, fb, pb, yb = xb.to(device), fb.to(device), pb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast_cm():\n",
    "            logits = model(xb, fb, pb)\n",
    "            loss = criterion(logits, yb)\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "            optimizer.step()\n",
    "        if ema:\n",
    "            ema.update(model)\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "        global_step += 1\n",
    "        scheduler.step(global_step)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    if ema:\n",
    "        ema.apply_shadow(model)\n",
    "    val_loss = 0.0\n",
    "    raw_preds = []\n",
    "    raw_targets = []\n",
    "    stage_preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb, fb, pb, yb in test_loader:\n",
    "            xb, fb, pb, yb = xb.to(device), fb.to(device), pb.to(device), yb.to(device)\n",
    "            with autocast_cm():\n",
    "                logits = model(xb, fb, pb)\n",
    "                loss = criterion(logits, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            stage = preds.clone()\n",
    "            if stress_indices:\n",
    "                stress_prob = probs[:, stress_indices].sum(dim=1)\n",
    "                stress_mask = stress_prob >= TWO_STAGE_THRESHOLD\n",
    "                if stress_mask.any() and stress_idx_tensor is not None:\n",
    "                    stress_probs = probs[stress_mask][:, stress_indices]\n",
    "                    best_local = torch.argmax(stress_probs, dim=1)\n",
    "                    stage[stress_mask] = stress_idx_tensor[best_local]\n",
    "                if no_stress_idx is not None:\n",
    "                    stage[~stress_mask] = no_stress_idx\n",
    "            raw_preds.append(preds.cpu().numpy())\n",
    "            stage_preds.append(stage.cpu().numpy())\n",
    "            raw_targets.append(yb.cpu().numpy())\n",
    "    if ema:\n",
    "        ema.restore(model)\n",
    "    val_loss /= len(test_loader.dataset)\n",
    "    raw_preds = np.concatenate(raw_preds)\n",
    "    stage_preds = np.concatenate(stage_preds)\n",
    "    raw_targets = np.concatenate(raw_targets)\n",
    "    val_acc = accuracy_score(raw_targets, raw_preds)\n",
    "    val_f1_macro = f1_score(raw_targets, raw_preds, average='macro', zero_division=0)\n",
    "    val_f1_weighted = f1_score(raw_targets, raw_preds, average='weighted', zero_division=0)\n",
    "\n",
    "    if val_f1_macro > best_val_f1:\n",
    "        best_val_f1 = val_f1_macro\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.3f} | Val F1 (macro): {val_f1_macro:.3f} \"\n",
    "        f\"| Val F1 (weighted): {val_f1_weighted:.3f}\"\n",
    "    )\n",
    "\n",
    "if best_model_state is not None:\n",
    "    model.load_state_dict(best_model_state)\n",
    "if ema:\n",
    "    ema.apply_shadow(model)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "model.eval()\n",
    "raw_preds = []\n",
    "stage_preds = []\n",
    "raw_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, fb, pb, yb in test_loader:\n",
    "        xb, fb, pb, yb = xb.to(device), fb.to(device), pb.to(device), yb.to(device)\n",
    "        with autocast_cm():\n",
    "            logits = model(xb, fb, pb)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "        stage = preds.clone()\n",
    "        if stress_indices:\n",
    "            stress_prob = probs[:, stress_indices].sum(dim=1)\n",
    "            stress_mask = stress_prob >= TWO_STAGE_THRESHOLD\n",
    "            if stress_mask.any() and stress_idx_tensor is not None:\n",
    "                stress_probs = probs[stress_mask][:, stress_indices]\n",
    "                best_local = torch.argmax(stress_probs, dim=1)\n",
    "                stage[stress_mask] = stress_idx_tensor[best_local]\n",
    "            if no_stress_idx is not None:\n",
    "                stage[~stress_mask] = no_stress_idx\n",
    "        raw_preds.append(preds.cpu().numpy())\n",
    "        stage_preds.append(stage.cpu().numpy())\n",
    "        raw_targets.append(yb.cpu().numpy())\n",
    "\n",
    "if ema:\n",
    "    ema.restore(model)\n",
    "\n",
    "raw_preds = np.concatenate(raw_preds)\n",
    "stage_preds = np.concatenate(stage_preds)\n",
    "raw_targets = np.concatenate(raw_targets)\n",
    "\n",
    "test_acc = accuracy_score(raw_targets, raw_preds)\n",
    "test_f1_macro = f1_score(raw_targets, raw_preds, average='macro', zero_division=0)\n",
    "test_f1_weighted = f1_score(raw_targets, raw_preds, average='weighted', zero_division=0)\n",
    "test_precision_macro = precision_score(raw_targets, raw_preds, average='macro', zero_division=0)\n",
    "test_recall_macro = recall_score(raw_targets, raw_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(\"OVERALL METRICS (direct predictions):\")\n",
    "print(f\"  Accuracy:         {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
    "print(f\"  Macro F1:         {test_f1_macro:.4f} ({test_f1_macro*100:.1f}%)\")\n",
    "print(f\"  Weighted F1:      {test_f1_weighted:.4f} ({test_f1_weighted*100:.1f}%)\")\n",
    "print(f\"  Macro Precision:  {test_precision_macro:.4f}\")\n",
    "print(f\"  Macro Recall:     {test_recall_macro:.4f}\")\n",
    "\n",
    "stage_acc = accuracy_score(raw_targets, stage_preds)\n",
    "stage_f1_macro = f1_score(raw_targets, stage_preds, average='macro', zero_division=0)\n",
    "stage_f1_weighted = f1_score(raw_targets, stage_preds, average='weighted', zero_division=0)\n",
    "print(\"Two-stage metrics:\")\n",
    "print(f\"  Accuracy:         {stage_acc:.4f} ({stage_acc*100:.1f}%)\")\n",
    "print(f\"  Macro F1:         {stage_f1_macro:.4f} ({stage_f1_macro*100:.1f}%)\")\n",
    "print(f\"  Weighted F1:      {stage_f1_weighted:.4f} ({stage_f1_weighted*100:.1f}%)\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PER-CLASS METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\" + classification_report(raw_targets, raw_preds, target_names=le.classes_, digits=4, zero_division=0))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(raw_targets, raw_preds)\n",
    "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\" + str(cm_df))\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUBJECT-LEVEL PERFORMANCE (TEST SET)\")\n",
    "print(\"=\" * 80)\n",
    "subject_records = []\n",
    "for subj in sorted(np.unique(held_out_subjects)):\n",
    "    subj_mask = held_out_subjects == subj\n",
    "    subj_true = raw_targets[subj_mask]\n",
    "    subj_pred = stage_preds[subj_mask]\n",
    "    subject_records.append({\n",
    "        \"subject\": subj,\n",
    "        \"samples\": int(subj_mask.sum()),\n",
    "        \"accuracy\": accuracy_score(subj_true, subj_pred) if subj_mask.sum() else 0.0,\n",
    "        \"macro_f1\": f1_score(subj_true, subj_pred, average='macro', zero_division=0),\n",
    "    })\n",
    "if subject_records:\n",
    "    subject_df = pd.DataFrame(subject_records)\n",
    "    print(subject_df.to_string(index=False, formatters={\n",
    "        \"accuracy\": \"{:.3f}\".format,\n",
    "        \"macro_f1\": \"{:.3f}\".format,\n",
    "    }))\n",
    "else:\n",
    "    print(\"No held-out subjects to report (check split configuration).\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON TO BASELINE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"BASELINE (before Phase 1):\")\n",
    "print(\"  Accuracy:    75.9%\")\n",
    "print(\"  Macro F1:    36.0%\")\n",
    "print(f\"PHASE 2+ HYBRID (multi-scale CNN + hybrid features + focal loss):\")\n",
    "print(f\"  Accuracy:    {stage_acc*100:.1f}%  ({(stage_acc-0.759)*100:+.1f}pp)\")\n",
    "print(f\"  Macro F1:    {stage_f1_macro*100:.1f}%  ({(stage_f1_macro-0.360)*100:+.1f}pp)\")\n",
    "\n",
    "improvement = (stage_f1_macro - 0.360) * 100\n",
    "if improvement >= 8:\n",
    "    print(f\"✓ TARGET ACHIEVED! Macro F1 improved by {improvement:+.1f}pp (target: +8-12pp)\")\n",
    "elif improvement >= 5:\n",
    "    print(f\"✓ Good progress! Macro F1 improved by {improvement:+.1f}pp\")\n",
    "else:\n",
    "    print(f\"⚠ Improvement: {improvement:+.1f}pp. Continue with next phases if needed.\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a06208ad69c4d4cb51bc8eebda276aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ba0702def68443a9a49a025c657c0d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5a3694cf2e9445f1bf5a52bda7899e02",
        "IPY_MODEL_71d57578c1c647d29d1b8071b240a4d6",
        "IPY_MODEL_65a6f23625d04e6d80b824cebd1958ae"
       ],
       "layout": "IPY_MODEL_f531ca6957d64026a8a94014c391c9a1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "3e57612d337444b4bdc18331dce1e1b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4026a25d59cf4b31957fd9ce50ed8dab": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d6d675daf2d444c83479d916217a54b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4026a25d59cf4b31957fd9ce50ed8dab",
       "max": 32,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6dba705ea0664383be7e97b158b28635",
       "tabbable": null,
       "tooltip": null,
       "value": 32
      }
     },
     "501340d6ffe34b8ea24bdfce1789db50": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "51ce5ef228c348c0971f89b65d476042": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56c5cd6f12dd4d9c91b62ba94ae44692": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a3694cf2e9445f1bf5a52bda7899e02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_694899dc97064979ba116bb1fee488d3",
       "placeholder": "​",
       "style": "IPY_MODEL_6fe3dd568d584fae87bd671de1fac4cd",
       "tabbable": null,
       "tooltip": null,
       "value": "AEROBIC: 100%"
      }
     },
     "5ccd35c4f50a46d4bc4ad88c507a1f6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "65a6f23625d04e6d80b824cebd1958ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bd98d548ec72487db390252d7e8094f0",
       "placeholder": "​",
       "style": "IPY_MODEL_51ce5ef228c348c0971f89b65d476042",
       "tabbable": null,
       "tooltip": null,
       "value": " 31/31 [00:05&lt;00:00,  5.00it/s]"
      }
     },
     "694899dc97064979ba116bb1fee488d3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6d50418385a041b9af98b7cfeed1e33a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d6743247a80d4d56a9e7cd3489448f06",
        "IPY_MODEL_e15d66cb46264f7daa65c0fd68ea8e3f",
        "IPY_MODEL_c143b5c5b522418f8fac25c26ffff8db"
       ],
       "layout": "IPY_MODEL_a7cc8694aba2495e858533109de756c7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "6dba705ea0664383be7e97b158b28635": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6fe3dd568d584fae87bd671de1fac4cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "715a8526c838455e87345760f7c28ca2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "71d57578c1c647d29d1b8071b240a4d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b9c9213c10104315ba797d6be070cf5a",
       "max": 31,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a5c4d5d14e3b446492e220fa0566f168",
       "tabbable": null,
       "tooltip": null,
       "value": 31
      }
     },
     "8c74fef78b3c492ab703d122f0bd14ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_cb64fdd6be1c449b8d3d7c41ab6ee8b2",
        "IPY_MODEL_4d6d675daf2d444c83479d916217a54b",
        "IPY_MODEL_d492fb4d3ae34c2591209bd8755e20f7"
       ],
       "layout": "IPY_MODEL_f737e8174d5f4a7ea76bdc16c1efefad",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8e14ab79b2d747bba8b988b989b7c449": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5c4d5d14e3b446492e220fa0566f168": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a7cc8694aba2495e858533109de756c7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b9c9213c10104315ba797d6be070cf5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bc4d6bece2aa4195a037c9446aec4e25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bd98d548ec72487db390252d7e8094f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c143b5c5b522418f8fac25c26ffff8db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_501340d6ffe34b8ea24bdfce1789db50",
       "placeholder": "​",
       "style": "IPY_MODEL_3e57612d337444b4bdc18331dce1e1b4",
       "tabbable": null,
       "tooltip": null,
       "value": " 37/37 [00:07&lt;00:00,  3.77it/s]"
      }
     },
     "cb64fdd6be1c449b8d3d7c41ab6ee8b2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_56c5cd6f12dd4d9c91b62ba94ae44692",
       "placeholder": "​",
       "style": "IPY_MODEL_bc4d6bece2aa4195a037c9446aec4e25",
       "tabbable": null,
       "tooltip": null,
       "value": "ANAEROBIC: 100%"
      }
     },
     "d0d9d02129c14bf0acb25f5484fbe88a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d492fb4d3ae34c2591209bd8755e20f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_0a06208ad69c4d4cb51bc8eebda276aa",
       "placeholder": "​",
       "style": "IPY_MODEL_5ccd35c4f50a46d4bc4ad88c507a1f6a",
       "tabbable": null,
       "tooltip": null,
       "value": " 32/32 [00:04&lt;00:00,  4.78it/s]"
      }
     },
     "d6743247a80d4d56a9e7cd3489448f06": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0d9d02129c14bf0acb25f5484fbe88a",
       "placeholder": "​",
       "style": "IPY_MODEL_fd1013cb952a46e29bb979433a9a8759",
       "tabbable": null,
       "tooltip": null,
       "value": "STRESS: 100%"
      }
     },
     "e15d66cb46264f7daa65c0fd68ea8e3f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e14ab79b2d747bba8b988b989b7c449",
       "max": 37,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_715a8526c838455e87345760f7c28ca2",
       "tabbable": null,
       "tooltip": null,
       "value": 37
      }
     },
     "f531ca6957d64026a8a94014c391c9a1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f737e8174d5f4a7ea76bdc16c1efefad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fd1013cb952a46e29bb979433a9a8759": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
