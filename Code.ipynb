{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Level Prediction Pipeline\n",
    "\n",
    "End-to-end preprocessing and modeling for stress vs non-stress, including aerobic/anaerobic as active negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.signal import welch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "- Window length 60s, step 30s\n",
    "- Minimum label coverage 0.7\n",
    "- Protocol tag mapping per provided notebook\n",
    "- Data constraints applied during load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DEFAULT_DATASET_ROOT = Path(\"./Datasets\")\n",
    "DATASET_ROOT = Path(os.getenv(\"DATASET_ROOT\", DEFAULT_DATASET_ROOT))\n",
    "\n",
    "# Windowing\n",
    "WINDOW_SECONDS = 60\n",
    "WINDOW_STEP_SECONDS = 30\n",
    "MIN_LABEL_COVERAGE = 0.7\n",
    "\n",
    "# Sampling / weighting\n",
    "PHASE_BALANCE_NO_STRESS_RATIO = 2.0\n",
    "PHASE_WEIGHT_MAP = {\n",
    "    \"Stroop\": 1.6,\n",
    "    \"Opposite Opinion\": 1.4,\n",
    "    \"Real Opinion\": 1.3,\n",
    "    \"Subtract\": 1.2,\n",
    "    \"TMCT\": 1.4,\n",
    "}\n",
    "PHASE_LABEL_OVERSAMPLE = {\n",
    "    (\"Stroop\", \"high_stress\"): 2.5,\n",
    "    (\"Opposite Opinion\", \"low_stress\"): 2.0,\n",
    "    (\"TMCT\", \"high_stress\"): 1.5,\n",
    "}\n",
    "EMA_SPAN = 5\n",
    "PHASE_Z_MIN_STD = 1e-3\n",
    "STRESS_PROB_THRESHOLD = 0.4\n",
    "\n",
    "# Data constraints\n",
    "DUPLICATE_CUTS = {\"S02\": {\"ACC\": 49545, \"BVP\": 99091, \"EDA\": 6195, \"TEMP\": 6195}}\n",
    "MISSING_SENSORS = {\"f07\": {\"BVP\", \"TEMP\", \"HR\", \"IBI\"}}\n",
    "\n",
    "STATES = [\"STRESS\", \"AEROBIC\", \"ANAEROBIC\"]\n",
    "\n",
    "# Stress protocol metadata\n",
    "STRESS_STAGE_ORDER_S = [\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_STAGE_ORDER_F = [\"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_TAG_PAIRS_S = [(3, 4), (5, 6), (7, 8), (9, 10), (11, 12)]\n",
    "STRESS_TAG_PAIRS_F = [(2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "STRESS_LEVEL_FILES = [\"Stress_Level_v1.csv\", \"Stress_Level_v2.csv\"]\n",
    "\n",
    "def load_stress_levels() -> Dict[str, Dict[str, float]]:\n",
    "    tables = []\n",
    "    for fname in STRESS_LEVEL_FILES:\n",
    "        path = Path(fname)\n",
    "        if not path.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        tables.append(df)\n",
    "    levels: Dict[str, Dict[str, float]] = {}\n",
    "    for table in tables:\n",
    "        for subject, row in table.iterrows():\n",
    "            subj = str(subject).strip()\n",
    "            levels[subj] = {col: (float(row[col]) if not pd.isna(row[col]) else np.nan) for col in table.columns}\n",
    "    return levels\n",
    "\n",
    "\n",
    "STRESS_LEVELS = load_stress_levels()\n",
    "\n",
    "\n",
    "STRESS_PHASES = {\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"}\n",
    "STRESS_LEVEL_BOUNDS = {\"low\": 3.0, \"moderate\": 6.0}\n",
    "STRESS_LEVEL_PHASE_BOUNDS = {\n",
    "    \"Stroop\": {\"low\": 2.5, \"moderate\": 5.0},\n",
    "    \"Opposite Opinion\": {\"low\": 2.5, \"moderate\": 5.5},\n",
    "    \"Real Opinion\": {\"low\": 2.8, \"moderate\": 5.5},\n",
    "    \"TMCT\": {\"low\": 2.8, \"moderate\": 5.8},\n",
    "    \"Subtract\": {\"low\": 2.8, \"moderate\": 5.8},\n",
    "}\n",
    "TEMPORAL_FEATURES = [\"eda_mean\", \"eda_std\", \"acc_energy\", \"temp_mean\", \"hr_mean\"]\n",
    "FEATURE_EXCLUDE_COLS = {\n",
    "    \"subject\",\n",
    "    \"state\",\n",
    "    \"phase\",\n",
    "    \"stress_stage\",\n",
    "    \"stress_level\",\n",
    "    \"label\",\n",
    "    \"is_stress\",\n",
    "    \"win_start\",\n",
    "    \"win_end\",\n",
    "    \"phase_start\",\n",
    "    \"phase_end\",\n",
    "}\n",
    "\n",
    "ACC_ACTIVITY_WINDOW_SEC = 2.0\n",
    "ACC_ACTIVITY_STEP_SEC = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Empatica format and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base subject ID without session suffix.\n",
    "def base_subject_id(subject: str) -> str:\n",
    "    return subject.split(\"_\")[0]\n",
    "\n",
    "\n",
    "# Read signal CSV and return (fs, data, start_timestamp).\n",
    "def read_signal(path: Path) -> Tuple[float, np.ndarray, pd.Timestamp]:\n",
    "    with open(path, \"r\") as f:\n",
    "        start_line = f.readline()\n",
    "        if not start_line:\n",
    "            raise ValueError(f\"Missing start timestamp in {path}\")\n",
    "        start = pd.to_datetime(start_line.split(\",\")[0])\n",
    "        fs_line = f.readline()\n",
    "        if not fs_line:\n",
    "            raise ValueError(f\"Missing sample rate in {path}\")\n",
    "        fs = float(fs_line.split(\",\")[0])\n",
    "        data = np.genfromtxt(f, delimiter=\",\")\n",
    "    if np.isscalar(data):\n",
    "        if math.isnan(float(data)):\n",
    "            data = np.empty((0, 1))\n",
    "        else:\n",
    "            data = np.array([[float(data)]])\n",
    "    elif data.size == 0:\n",
    "        data = np.empty((0, 1))\n",
    "    else:\n",
    "        data = np.asarray(data, dtype=float)\n",
    "        if np.isnan(data).all():\n",
    "            data = np.empty((0, 1))\n",
    "        elif data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "    return fs, data, start\n",
    "\n",
    "\n",
    "# Read IBI CSV and return inter-beat intervals in seconds.\n",
    "def read_ibi(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        return np.array([])\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None, skiprows=1)\n",
    "    except (pd.errors.EmptyDataError, FileNotFoundError):\n",
    "        return np.array([])\n",
    "    if df.empty:\n",
    "        return np.array([])\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "    if df.empty:\n",
    "        return np.array([])\n",
    "    col = 1 if df.shape[1] > 1 else 0\n",
    "    return df.iloc[:, col].to_numpy(dtype=float)\n",
    "\n",
    "\n",
    "# Read tags.csv and return list of tag timestamps (seconds since start).\n",
    "def read_tags(path: Path, start_ts: pd.Timestamp) -> List[Tuple[float, float]]:\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return []\n",
    "    tags = []\n",
    "    for ts_str in df[0].astype(str):\n",
    "        ts = pd.to_datetime(ts_str)\n",
    "        tags.append((ts - start_ts).total_seconds())\n",
    "    return [(t, t) for t in tags]\n",
    "\n",
    "\n",
    "# Extract stress intervals from tags based on subject version.\n",
    "def stress_intervals_from_tags(tags: List[Tuple[float, float]], subject: str) -> List[dict]:\n",
    "    if not tags:\n",
    "        return []\n",
    "    t = [x[0] for x in tags]\n",
    "    if subject.startswith(\"S\"):\n",
    "        idx_pairs = STRESS_TAG_PAIRS_S\n",
    "        stage_order = STRESS_STAGE_ORDER_S\n",
    "    else:\n",
    "        idx_pairs = STRESS_TAG_PAIRS_F\n",
    "        stage_order = STRESS_STAGE_ORDER_F\n",
    "    base_id = base_subject_id(subject)\n",
    "    spans = []\n",
    "    for stage, (i, j) in zip(stage_order, idx_pairs):\n",
    "        if i < len(t) and j < len(t) and t[j] > t[i]:\n",
    "            level = STRESS_LEVELS.get(base_id, {}).get(stage)\n",
    "            spans.append({\n",
    "                \"start\": t[i],\n",
    "                \"end\": t[j],\n",
    "                \"stage\": stage,\n",
    "                \"stress_level\": level,\n",
    "            })\n",
    "    return spans\n",
    "\n",
    "\n",
    "# Extract active intervals from tags.\n",
    "def active_intervals_from_tags(tags: List[Tuple[float, float]], label: str) -> List[dict]:\n",
    "    if len(tags) < 2:\n",
    "        return []\n",
    "    t = [x[0] for x in tags]\n",
    "    spans = []\n",
    "    for a, b in zip(t[:-1], t[1:]):\n",
    "        if b > a:\n",
    "            spans.append({\n",
    "                \"start\": a,\n",
    "                \"end\": b,\n",
    "                \"stage\": label,\n",
    "                \"stress_level\": 0.0,\n",
    "            })\n",
    "    return spans\n",
    "\n",
    "\n",
    "# Map numeric stress score to four-level category.\n",
    "def stress_bucket(level: float | None, phase: str | None) -> str:\n",
    "    if phase in {\"aerobic\", \"anaerobic\", \"rest\"}:\n",
    "        return \"no_stress\"\n",
    "    if level is None or pd.isna(level) or level <= 0:\n",
    "        return \"no_stress\"\n",
    "    bounds = STRESS_LEVEL_PHASE_BOUNDS.get(phase, STRESS_LEVEL_BOUNDS)\n",
    "    if level <= bounds[\"low\"]:\n",
    "        return \"low_stress\"\n",
    "    if level <= bounds[\"moderate\"]:\n",
    "        return \"moderate_stress\"\n",
    "    return \"high_stress\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample signal to target frequency.\n",
    "def resample_to_rate(signal: np.ndarray, src_fs: float, tgt_fs: float) -> np.ndarray:\n",
    "    if signal.ndim == 1:\n",
    "        signal = signal[:, None]\n",
    "    src_len = signal.shape[0]\n",
    "    duration = src_len / src_fs\n",
    "    tgt_len = int(duration * tgt_fs)\n",
    "    src_t = np.linspace(0, duration, src_len)\n",
    "    tgt_t = np.linspace(0, duration, tgt_len)\n",
    "    resampled = np.stack([np.interp(tgt_t, src_t, signal[:, i]) for i in range(signal.shape[1])], axis=1)\n",
    "    if resampled.shape[1] == 1:\n",
    "        return resampled[:, 0]\n",
    "    return resampled\n",
    "\n",
    "# Simple Hampel filter for outlier suppression.\n",
    "def hampel_filter(x: np.ndarray, k: int = 5, t0: float = 3.0) -> np.ndarray:\n",
    "    x = x.copy()\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        window = x[max(i - k, 0): min(i + k, n)]\n",
    "        med = np.median(window)\n",
    "        mad = np.median(np.abs(window - med)) or 1e-6\n",
    "        if abs(x[i] - med) > t0 * mad:\n",
    "            x[i] = med\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate ACC activity envelope using sliding RMS to capture movement intensity.\n",
    "def acc_activity_signal(\n",
    "    acc_raw: np.ndarray,\n",
    "    fs: float,\n",
    "    win_sec: float = ACC_ACTIVITY_WINDOW_SEC,\n",
    "    step_sec: float = ACC_ACTIVITY_STEP_SEC,\n",
    ") -> np.ndarray:\n",
    "    if acc_raw.size == 0 or fs <= 0:\n",
    "        return np.array([])\n",
    "    if acc_raw.ndim == 1:\n",
    "        acc_raw = acc_raw[:, None]\n",
    "    magnitude = np.linalg.norm(acc_raw, axis=1)\n",
    "    win = max(1, int(round(win_sec * fs)))\n",
    "    step = max(1, int(round(step_sec * fs)))\n",
    "    if len(magnitude) < win:\n",
    "        rms = float(np.sqrt(np.mean(magnitude ** 2))) if len(magnitude) else 0.0\n",
    "        return np.array([rms])\n",
    "    activity = []\n",
    "    start = 0\n",
    "    while start + win <= len(magnitude):\n",
    "        segment = magnitude[start:start + win]\n",
    "        rms = float(np.sqrt(np.mean(segment ** 2)))\n",
    "        activity.append(rms)\n",
    "        start += step\n",
    "    if start < len(magnitude):\n",
    "        segment = magnitude[-win:]\n",
    "        rms = float(np.sqrt(np.mean(segment ** 2)))\n",
    "        if not activity or abs(activity[-1] - rms) > 1e-9:\n",
    "            activity.append(rms)\n",
    "    return np.asarray(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_centroid(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size == 0 or fs <= 0:\n",
    "        return np.nan\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(256, len(signal)))\n",
    "    total = psd.sum()\n",
    "    if total <= 0:\n",
    "        return np.nan\n",
    "    return float(np.sum(freqs * psd) / total)\n",
    "\n",
    "\n",
    "def bandpower(signal: np.ndarray, fs: float, fmin: float, fmax: float) -> float:\n",
    "    if signal.size == 0 or fs <= 0 or fmax <= fmin:\n",
    "        return np.nan\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(256, len(signal)))\n",
    "    mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    trap = getattr(np, \"trapezoid\", np.trapz)\n",
    "    return float(trap(psd[mask], freqs[mask]))\n",
    "\n",
    "\n",
    "def linear_slope(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size < 2 or fs <= 0:\n",
    "        return 0.0\n",
    "    t = np.arange(signal.size) / fs\n",
    "    slope, _ = np.polyfit(t, signal, 1)\n",
    "    return float(slope)\n",
    "\n",
    "\n",
    "def peak_rate(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size < 3 or fs <= 0:\n",
    "        return 0.0\n",
    "    duration = signal.size / fs\n",
    "    if duration <= 0:\n",
    "        return 0.0\n",
    "    peaks = np.sum((signal[1:-1] > signal[:-2]) & (signal[1:-1] > signal[2:]))\n",
    "    return float(peaks / duration)\n",
    "\n",
    "\n",
    "def eda_features(eda: np.ndarray, fs: float) -> Dict[str, float]:\n",
    "    if eda.size == 0 or fs <= 0:\n",
    "        return {\n",
    "            \"eda_mean\": np.nan,\n",
    "            \"eda_std\": np.nan,\n",
    "            \"eda_slope\": np.nan,\n",
    "            \"eda_peak_rate\": np.nan,\n",
    "            \"eda_range\": np.nan,\n",
    "            \"eda_power_slow\": np.nan,\n",
    "            \"eda_power_mid\": np.nan,\n",
    "            \"eda_power_fast\": np.nan,\n",
    "            \"eda_centroid\": np.nan,\n",
    "        }\n",
    "    clean = hampel_filter(eda)\n",
    "    return {\n",
    "        \"eda_mean\": float(np.mean(clean)),\n",
    "        \"eda_std\": float(np.std(clean)),\n",
    "        \"eda_slope\": linear_slope(clean, fs),\n",
    "        \"eda_peak_rate\": peak_rate(clean, fs),\n",
    "        \"eda_range\": float(np.max(clean) - np.min(clean)),\n",
    "        \"eda_power_slow\": bandpower(clean, fs, 0.01, 0.05),\n",
    "        \"eda_power_mid\": bandpower(clean, fs, 0.045, 0.25),\n",
    "        \"eda_power_fast\": bandpower(clean, fs, 0.25, 1.5),\n",
    "        \"eda_centroid\": spectral_centroid(clean, fs),\n",
    "    }\n",
    "\n",
    "\n",
    "def temp_features(temp: np.ndarray, fs: float) -> Dict[str, float]:\n",
    "    if temp.size == 0 or fs <= 0:\n",
    "        return {\n",
    "            \"temp_mean\": np.nan,\n",
    "            \"temp_std\": np.nan,\n",
    "            \"temp_slope\": np.nan,\n",
    "            \"temp_min\": np.nan,\n",
    "            \"temp_max\": np.nan,\n",
    "        }\n",
    "    return {\n",
    "        \"temp_mean\": float(np.mean(temp)),\n",
    "        \"temp_std\": float(np.std(temp)),\n",
    "        \"temp_slope\": linear_slope(temp, fs),\n",
    "        \"temp_min\": float(np.min(temp)),\n",
    "        \"temp_max\": float(np.max(temp)),\n",
    "    }\n",
    "\n",
    "\n",
    "def hrv_features(ibi: np.ndarray) -> Dict[str, float]:\n",
    "    if ibi.size < 2:\n",
    "        return {\n",
    "            \"hr_mean\": np.nan,\n",
    "            \"rmssd\": np.nan,\n",
    "            \"sdnn\": np.nan,\n",
    "            \"pnn50\": np.nan,\n",
    "            \"lf_hf\": np.nan,\n",
    "            \"sd1\": np.nan,\n",
    "            \"sd2\": np.nan,\n",
    "        }\n",
    "    rr = ibi.astype(float)\n",
    "    diff = np.diff(rr)\n",
    "    rmssd = float(np.sqrt(np.mean(diff ** 2))) if diff.size else np.nan\n",
    "    sdnn = float(np.std(rr))\n",
    "    pnn50 = float(np.mean(np.abs(diff) > 0.05)) if diff.size else np.nan\n",
    "    hr_mean = float(60.0 / np.mean(rr)) if np.mean(rr) > 0 else np.nan\n",
    "\n",
    "    if diff.size:\n",
    "        sd1 = float(np.sqrt(0.5) * np.std(diff))\n",
    "    else:\n",
    "        sd1 = np.nan\n",
    "    if not np.isnan(sdnn) and not np.isnan(sd1):\n",
    "        sd2_sq = max(0.0, 2 * (sdnn ** 2) - 0.5 * (sd1 ** 2))\n",
    "        sd2 = float(np.sqrt(sd2_sq))\n",
    "    else:\n",
    "        sd2 = np.nan\n",
    "\n",
    "    lf_hf = np.nan\n",
    "    try:\n",
    "        t = np.cumsum(rr)\n",
    "        t = t - t[0]\n",
    "        if t[-1] > 0 and rr.size >= 4:\n",
    "            fs_interp = 4.0\n",
    "            grid = np.arange(0, t[-1], 1 / fs_interp)\n",
    "            if grid.size >= 8:\n",
    "                interp_rr = np.interp(grid, t[: len(grid)], rr[: len(grid)])\n",
    "                lf = bandpower(interp_rr, fs_interp, 0.04, 0.15)\n",
    "                hf = bandpower(interp_rr, fs_interp, 0.15, 0.4)\n",
    "                if hf and hf > 0:\n",
    "                    lf_hf = float(lf / hf)\n",
    "    except Exception:\n",
    "        lf_hf = np.nan\n",
    "\n",
    "    return {\n",
    "        \"hr_mean\": hr_mean,\n",
    "        \"rmssd\": rmssd,\n",
    "        \"sdnn\": sdnn,\n",
    "        \"pnn50\": pnn50,\n",
    "        \"lf_hf\": lf_hf,\n",
    "        \"sd1\": sd1,\n",
    "        \"sd2\": sd2,\n",
    "    }\n",
    "\n",
    "\n",
    "def acc_features(acc_mag: np.ndarray, fs: float, acc_activity: np.ndarray | None = None) -> Dict[str, float]:\n",
    "    if len(acc_mag) == 0:\n",
    "        base = {\n",
    "            \"acc_mean\": np.nan,\n",
    "            \"acc_std\": np.nan,\n",
    "            \"acc_energy\": np.nan,\n",
    "            \"acc_peak_freq\": np.nan,\n",
    "            \"acc_bandpower_low\": np.nan,\n",
    "            \"acc_bandpower_mid\": np.nan,\n",
    "            \"acc_bandpower_high\": np.nan,\n",
    "            \"acc_mad\": np.nan,\n",
    "        }\n",
    "    else:\n",
    "        energy = np.mean(acc_mag ** 2)\n",
    "        base = {\n",
    "            \"acc_mean\": float(np.mean(acc_mag)),\n",
    "            \"acc_std\": float(np.std(acc_mag)),\n",
    "            \"acc_energy\": float(energy),\n",
    "            \"acc_peak_freq\": spectral_centroid(acc_mag, fs),\n",
    "            \"acc_bandpower_low\": bandpower(acc_mag, fs, 0.1, 0.5),\n",
    "            \"acc_bandpower_mid\": bandpower(acc_mag, fs, 0.5, 2.0),\n",
    "            \"acc_bandpower_high\": bandpower(acc_mag, fs, 2.0, 8.0),\n",
    "            \"acc_mad\": float(np.median(np.abs(acc_mag - np.median(acc_mag)))),\n",
    "        }\n",
    "    if acc_activity is not None and len(acc_activity):\n",
    "        base.update(\n",
    "            {\n",
    "                \"acc_activity_mean\": float(np.mean(acc_activity)),\n",
    "                \"acc_activity_std\": float(np.std(acc_activity)),\n",
    "                \"acc_activity_max\": float(np.max(acc_activity)),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        base.update(\n",
    "            {\n",
    "                \"acc_activity_mean\": np.nan,\n",
    "                \"acc_activity_std\": np.nan,\n",
    "                \"acc_activity_max\": np.nan,\n",
    "            }\n",
    "        )\n",
    "    return base\n",
    "\n",
    "\n",
    "def combine_features(eda, eda_fs, acc_mag, acc_fs, temp, temp_fs, ibi, acc_activity=None) -> Dict[str, float]:\n",
    "    feats = {}\n",
    "    feats.update(eda_features(eda, eda_fs))\n",
    "    feats.update(acc_features(acc_mag, acc_fs, acc_activity))\n",
    "    feats.update(temp_features(temp, temp_fs))\n",
    "    feats.update(hrv_features(ibi))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing and label assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Window:\n",
    "    start: float\n",
    "    end: float\n",
    "    label: str\n",
    "    subject: str\n",
    "    state: str\n",
    "\n",
    "\n",
    "def window_intervals(duration: float, win_s: int = WINDOW_SECONDS, step_s: int = WINDOW_STEP_SECONDS) -> List[Tuple[float, float]]:\n",
    "    windows = []\n",
    "    t = 0.0\n",
    "    while t + win_s <= duration:\n",
    "        windows.append((t, t + win_s))\n",
    "        t += step_s\n",
    "    return windows\n",
    "\n",
    "\n",
    "def _span_bounds(span) -> Tuple[float, float]:\n",
    "    if isinstance(span, dict):\n",
    "        return span[\"start\"], span[\"end\"]\n",
    "    return span\n",
    "\n",
    "\n",
    "def assign_label(win: Tuple[float, float], intervals: Dict[str, List[dict]]) -> Tuple[str | None, dict | None]:\n",
    "    start, end = win\n",
    "    length = end - start\n",
    "    best_label = None\n",
    "    best_cov = 0.0\n",
    "    best_span = None\n",
    "    for lbl, spans in intervals.items():\n",
    "        label_overlap = 0.0\n",
    "        label_best_span = None\n",
    "        label_best_overlap = 0.0\n",
    "        for span in spans:\n",
    "            a, b = _span_bounds(span)\n",
    "            inter = max(0.0, min(end, b) - max(start, a))\n",
    "            if inter > 0:\n",
    "                label_overlap += inter\n",
    "                if inter > label_best_overlap:\n",
    "                    label_best_overlap = inter\n",
    "                    label_best_span = span\n",
    "        coverage = label_overlap / length\n",
    "        if coverage > best_cov:\n",
    "            best_cov = coverage\n",
    "            best_label = lbl\n",
    "            best_span = label_best_span\n",
    "    if best_cov >= MIN_LABEL_COVERAGE and best_label is not None:\n",
    "        return best_label, best_span\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def make_label_intervals(state: str, subject: str, tags: List[Tuple[float, float]], duration: float) -> Dict[str, List[dict]]:\n",
    "    rest_span = {\"start\": 0.0, \"end\": duration, \"stage\": \"rest\", \"stress_level\": 0.0}\n",
    "    if state == \"STRESS\":\n",
    "        stress_spans = stress_intervals_from_tags(tags, subject)\n",
    "        if not stress_spans:\n",
    "            return {\"rest\": [rest_span]}\n",
    "        return {\n",
    "            \"stress\": stress_spans,\n",
    "            \"rest\": [rest_span],\n",
    "        }\n",
    "    else:\n",
    "        lbl = \"aerobic\" if state == \"AEROBIC\" else \"anaerobic\"\n",
    "        active = active_intervals_from_tags(tags, lbl)\n",
    "        return {\n",
    "            lbl: active,\n",
    "            \"rest\": [rest_span],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion: read signals per subject/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_state(state: str, subject: str) -> dict:\n",
    "    folder = DATASET_ROOT / state / subject\n",
    "    base_id = base_subject_id(subject)\n",
    "    sensors = {}\n",
    "    fs_map = {}\n",
    "    missing = MISSING_SENSORS.get(base_id, set())\n",
    "\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(folder)\n",
    "    fs_eda, eda_raw, start_ts = read_signal(folder / \"EDA.csv\")\n",
    "    sensors[\"EDA\"] = np.squeeze(eda_raw)\n",
    "    fs_map[\"EDA\"] = fs_eda\n",
    "\n",
    "    temp_path = folder / \"TEMP.csv\"\n",
    "    if \"TEMP\" not in missing and temp_path.exists():\n",
    "        fs_temp, temp_raw, _ = read_signal(temp_path)\n",
    "        sensors[\"TEMP\"] = np.squeeze(temp_raw)\n",
    "        fs_map[\"TEMP\"] = fs_temp\n",
    "\n",
    "    fs_acc, acc_raw, _ = read_signal(folder / \"ACC.csv\")\n",
    "    acc_mag = np.linalg.norm(acc_raw, axis=1)\n",
    "    sensors[\"ACC_MAG\"] = acc_mag\n",
    "    fs_map[\"ACC_MAG\"] = fs_acc\n",
    "    acc_activity = acc_activity_signal(acc_raw, fs_acc)\n",
    "\n",
    "    if acc_activity.size:\n",
    "        sensors[\"ACC_ACTIVITY\"] = acc_activity\n",
    "        fs_map[\"ACC_ACTIVITY\"] = 1.0 / ACC_ACTIVITY_STEP_SEC\n",
    "\n",
    "\n",
    "    if \"IBI\" not in missing:\n",
    "        sensors[\"IBI\"] = read_ibi(folder / \"IBI.csv\")\n",
    "    else:\n",
    "        sensors[\"IBI\"] = np.array([])\n",
    "    tags = read_tags(folder / \"tags.csv\", start_ts)\n",
    "\n",
    "    if base_id in DUPLICATE_CUTS:\n",
    "        cuts = DUPLICATE_CUTS[base_id]\n",
    "        if \"EDA\" in cuts and \"EDA\" in sensors:\n",
    "            sensors[\"EDA\"] = sensors[\"EDA\"][:cuts[\"EDA\"]]\n",
    "        if \"TEMP\" in cuts and \"TEMP\" in sensors:\n",
    "            sensors[\"TEMP\"] = sensors[\"TEMP\"][:cuts[\"TEMP\"]]\n",
    "        if \"ACC\" in cuts and \"ACC_MAG\" in sensors:\n",
    "            sensors[\"ACC_MAG\"] = sensors[\"ACC_MAG\"][:cuts[\"ACC\"]]\n",
    "\n",
    "    duration = len(sensors[\"EDA\"]) / fs_map[\"EDA\"]\n",
    "    return {\n",
    "        \"sensors\": sensors,\n",
    "        \"fs\": fs_map,\n",
    "        \"tags\": tags,\n",
    "        \"duration\": duration,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_baselines(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    baseline = df[df[\"label\"] == \"no_stress\"].groupby(\"subject\").mean(numeric_only=True)\n",
    "    return baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build windowed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_windows_for_subject(state: str, subject: str, tgt_fs: float = 4.0) -> List[dict]:\n",
    "    info = load_subject_state(state, subject)\n",
    "    sensors = info[\"sensors\"]\n",
    "    fs_map = info[\"fs\"]\n",
    "    tags = info[\"tags\"]\n",
    "    duration = info[\"duration\"]\n",
    "\n",
    "    # Resample signals\n",
    "    eda = resample_to_rate(sensors[\"EDA\"], fs_map[\"EDA\"], tgt_fs)\n",
    "    temp = resample_to_rate(sensors[\"TEMP\"], fs_map.get(\"TEMP\", tgt_fs), tgt_fs) if \"TEMP\" in sensors else np.array([])\n",
    "    acc = resample_to_rate(sensors[\"ACC_MAG\"], fs_map[\"ACC_MAG\"], tgt_fs)\n",
    "    acc_activity = resample_to_rate(sensors[\"ACC_ACTIVITY\"], fs_map.get(\"ACC_ACTIVITY\", tgt_fs), tgt_fs) if \"ACC_ACTIVITY\" in sensors else np.array([])\n",
    "\n",
    "    intervals = make_label_intervals(state, subject, tags, duration)\n",
    "    windows = window_intervals(duration, WINDOW_SECONDS, WINDOW_STEP_SECONDS)\n",
    "    rows = []\n",
    "    for w in windows:\n",
    "        lbl, span_meta = assign_label(w, intervals)\n",
    "        if lbl is None or span_meta is None:\n",
    "            continue\n",
    "        start_idx = int(w[0] * tgt_fs)\n",
    "        end_idx = int(w[1] * tgt_fs)\n",
    "        eda_win = eda[start_idx:end_idx]\n",
    "        temp_win = temp[start_idx:end_idx] if len(temp) else np.array([])\n",
    "        acc_win = acc[start_idx:end_idx]\n",
    "        activity_win = acc_activity[start_idx:end_idx] if len(acc_activity) else np.array([])\n",
    "        ibi_win = sensors[\"IBI\"][(sensors[\"IBI\"] > 0)] if len(sensors[\"IBI\"]) else np.array([])\n",
    "\n",
    "        feats = combine_features(eda_win, tgt_fs, acc_win, tgt_fs, temp_win, tgt_fs, ibi_win, activity_win)\n",
    "        stress_stage = span_meta.get(\"stage\") if isinstance(span_meta, dict) else None\n",
    "        stress_level = span_meta.get(\"stress_level\") if isinstance(span_meta, dict) else None\n",
    "        phase_start = span_meta.get(\"start\") if isinstance(span_meta, dict) else None\n",
    "        phase_end = span_meta.get(\"end\") if isinstance(span_meta, dict) else None\n",
    "        if lbl == \"stress\":\n",
    "            if stress_level is None or np.isnan(stress_level):\n",
    "                continue\n",
    "        else:\n",
    "            stress_level = 0.0 if stress_level is None or np.isnan(stress_level) else stress_level\n",
    "        phase_label = stress_stage if stress_stage else lbl\n",
    "        stress_class = stress_bucket(stress_level, phase_label)\n",
    "        phase_duration = None\n",
    "        phase_progress = None\n",
    "        phase_elapsed = None\n",
    "        if phase_start is not None and phase_end is not None and phase_end > phase_start:\n",
    "            phase_duration = float(phase_end - phase_start)\n",
    "            phase_elapsed = float(w[0] - phase_start)\n",
    "            phase_progress = max(0.0, min(1.0, phase_elapsed / phase_duration))\n",
    "        row = {\n",
    "            \"subject\": base_subject_id(subject),\n",
    "            \"state\": state.lower(),\n",
    "            \"phase\": phase_label,\n",
    "            \"stress_stage\": stress_stage if lbl == \"stress\" else None,\n",
    "            \"stress_level\": float(stress_level),\n",
    "            \"label\": stress_class,\n",
    "            \"is_stress\": 1 if phase_label in STRESS_PHASES else 0,\n",
    "            \"win_start\": w[0],\n",
    "            \"win_end\": w[1],\n",
    "            \"phase_start\": phase_start,\n",
    "            \"phase_end\": phase_end,\n",
    "            \"phase_duration\": phase_duration,\n",
    "            \"phase_elapsed\": phase_elapsed,\n",
    "            \"phase_progress\": phase_progress,\n",
    "        }\n",
    "        row.update(feats)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def build_dataset(states: List[str] = STATES, max_subjects: int | None = None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for state in states:\n",
    "        subjects = sorted([p.name for p in (DATASET_ROOT / state).iterdir() if p.is_dir()])\n",
    "        if max_subjects:\n",
    "            subjects = subjects[:max_subjects]\n",
    "        for subj in subjects:\n",
    "            try:\n",
    "                rows.extend(build_windows_for_subject(state, subj))\n",
    "            except Exception as e:\n",
    "                print(f\"Skip {state}/{subj}: {e}\")\n",
    "                continue\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows 6742\n",
      "Phase counts phase\n",
      "rest                2981\n",
      "aerobic             1546\n",
      "anaerobic            998\n",
      "TMCT                 633\n",
      "Opposite Opinion     432\n",
      "Stroop               152\n",
      "Name: count, dtype: int64\n",
      "Label distribution label\n",
      "no_stress          5525\n",
      "moderate_stress     664\n",
      "high_stress         462\n",
      "low_stress           91\n",
      "Name: count, dtype: int64\n",
      "Stress level summary count    6742.000000\n",
      "mean        0.929398\n",
      "std         2.179948\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max        10.000000\n",
      "Name: stress_level, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = build_dataset()\n",
    "df = df.sort_values([\"subject\", \"win_start\"]).reset_index(drop=True)\n",
    "\n",
    "phase_group = df.groupby([\"subject\", \"phase\"], dropna=False)\n",
    "phase_min_start = phase_group[\"win_start\"].transform(\"min\")\n",
    "phase_max_end = phase_group[\"win_end\"].transform(\"max\")\n",
    "phase_duration_fallback = (phase_max_end - phase_min_start).replace(0, np.nan)\n",
    "df[\"phase_elapsed\"] = df[\"phase_elapsed\"].fillna(df[\"win_start\"] - phase_min_start)\n",
    "df[\"phase_duration\"] = df[\"phase_duration\"].fillna(phase_duration_fallback)\n",
    "df[\"phase_progress\"] = df[\"phase_progress\"].fillna(\n",
    "    df[\"phase_elapsed\"] / df[\"phase_duration\"].replace(0, np.nan)\n",
    ")\n",
    "df[\"phase_progress\"] = df[\"phase_progress\"].clip(0.0, 1.0).fillna(0.0)\n",
    "df[\"phase_position\"] = phase_group.cumcount()\n",
    "phase_counts = phase_group.size().rename(\"phase_size\")\n",
    "df = df.join(phase_counts, on=[\"subject\", \"phase\"])\n",
    "df[\"phase_position\"] = np.where(\n",
    "    df[\"phase_size\"] > 1,\n",
    "    df[\"phase_position\"] / (df[\"phase_size\"] - 1),\n",
    "    0.0,\n",
    ")\n",
    "df.drop(columns=[\"phase_size\"], inplace=True)\n",
    "df[\"phase_remaining\"] = (df[\"phase_duration\"] - df[\"phase_elapsed\"]).clip(lower=0).fillna(0)\n",
    "df[\"phase_early\"] = (df[\"phase_progress\"] <= 0.33).astype(int)\n",
    "df[\"phase_mid\"] = ((df[\"phase_progress\"] > 0.33) & (df[\"phase_progress\"] <= 0.66)).astype(int)\n",
    "df[\"phase_late\"] = (df[\"phase_progress\"] > 0.66).astype(int)\n",
    "\n",
    "context_cols = []\n",
    "for feat in TEMPORAL_FEATURES:\n",
    "    prev_col = f\"prev_{feat}\"\n",
    "    delta_col = f\"delta_{feat}\"\n",
    "    roll_mean_col = f\"roll_mean_{feat}\"\n",
    "    roll_std_col = f\"roll_std_{feat}\"\n",
    "    ema_col = f\"ema_{feat}\"\n",
    "    phase_delta_col = f\"phase_delta_{feat}\"\n",
    "    phase_z_col = f\"phase_z_{feat}\"\n",
    "    subj_group = df.groupby(\"subject\")[feat]\n",
    "    df[prev_col] = subj_group.shift(1)\n",
    "    df[delta_col] = df[feat] - df[prev_col]\n",
    "    roll_mean = (\n",
    "        subj_group.rolling(window=3, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    roll_std = (\n",
    "        subj_group.rolling(window=3, min_periods=1)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df[roll_mean_col] = roll_mean.to_numpy()\n",
    "    df[roll_std_col] = roll_std.to_numpy()\n",
    "    df[ema_col] = df.groupby(\"subject\")[feat].transform(lambda s: s.ewm(span=EMA_SPAN, adjust=False).mean())\n",
    "    phase_first = (\n",
    "        df.sort_values(\"win_start\")\n",
    "        .groupby([\"subject\", \"phase\"], dropna=False)[feat]\n",
    "        .transform(\"first\")\n",
    "    )\n",
    "    df[phase_delta_col] = df[feat] - phase_first\n",
    "    phase_running_mean = phase_group[feat].transform(lambda s: s.expanding().mean())\n",
    "    phase_running_std = phase_group[feat].transform(lambda s: s.expanding().std()).replace(0, np.nan)\n",
    "    df[phase_z_col] = (df[feat] - phase_running_mean) / phase_running_std\n",
    "    df[phase_z_col] = df[phase_z_col].replace([np.inf, -np.inf], 0)\n",
    "    context_cols.extend([prev_col, delta_col, roll_mean_col, roll_std_col, ema_col, phase_delta_col, phase_z_col])\n",
    "\n",
    "df[context_cols] = df[context_cols].fillna(0)\n",
    "\n",
    "df.to_csv(\"stress_level_dataset.csv\", index=False)\n",
    "print(\"Rows\", len(df))\n",
    "print(\"Phase counts\", df[\"phase\"].value_counts())\n",
    "print(\"Label distribution\", df[\"label\"].value_counts())\n",
    "print(\"Stress level summary\", df[\"stress_level\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "numeric_cols = [\n",
    "    c for c in df.columns if c not in FEATURE_EXCLUDE_COLS and np.issubdtype(df[c].dtype, np.number)\n",
    "]\n",
    "helper_cols = [\"label\", \"subject\", \"phase\", \"win_start\"]\n",
    "train_df = df.loc[:, numeric_cols + helper_cols].copy()\n",
    "train_df = train_df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def balance_by_phase_label(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    balanced = []\n",
    "    for phase_name, phase_df in data.groupby(\"phase\"):\n",
    "        if phase_df.empty:\n",
    "            continue\n",
    "        stress_df = phase_df[phase_df[\"label\"] != \"no_stress\"]\n",
    "        phase_parts = []\n",
    "        base_target = stress_df[\"label\"].value_counts().max() if not stress_df.empty else 0\n",
    "        phase_factor = PHASE_WEIGHT_MAP.get(phase_name, 1.0)\n",
    "        target = max(1, int(np.ceil(base_target * phase_factor))) if base_target else 0\n",
    "        if not stress_df.empty:\n",
    "            for label_name, label_df in stress_df.groupby(\"label\"):\n",
    "                label_factor = PHASE_LABEL_OVERSAMPLE.get((phase_name, label_name), 1.0)\n",
    "                label_target = max(1, int(np.ceil(target * label_factor))) if target else len(label_df)\n",
    "                if len(label_df) < label_target:\n",
    "                    phase_parts.append(label_df.sample(label_target, replace=True, random_state=42))\n",
    "                else:\n",
    "                    phase_parts.append(label_df)\n",
    "        no_stress_df = phase_df[phase_df[\"label\"] == \"no_stress\"]\n",
    "        if not no_stress_df.empty:\n",
    "            cap = int(np.ceil(PHASE_BALANCE_NO_STRESS_RATIO * max(target, 1))) if target else len(no_stress_df)\n",
    "            if len(no_stress_df) > cap:\n",
    "                phase_parts.append(no_stress_df.sample(cap, random_state=42, replace=False))\n",
    "            else:\n",
    "                phase_parts.append(no_stress_df)\n",
    "        if phase_parts:\n",
    "            balanced.append(pd.concat(phase_parts, ignore_index=True))\n",
    "        else:\n",
    "            balanced.append(phase_df)\n",
    "    return pd.concat(balanced, ignore_index=True)\n",
    "\n",
    "train_df = balance_by_phase_label(train_df)\n",
    "phase_series = train_df[\"phase\"].fillna(\"unknown\")\n",
    "\n",
    "# Phase-specific baselines (relative to first window in each phase)\n",
    "phase_sorted = train_df.sort_values([\"subject\", \"phase\", \"win_start\"])\n",
    "phase_baselines = (\n",
    "    phase_sorted.groupby([\"subject\", \"phase\"], dropna=False)[numeric_cols]\n",
    "    .transform(\"first\")\n",
    ")\n",
    "train_df[numeric_cols] = train_df[numeric_cols] - phase_baselines.fillna(0)\n",
    "\n",
    "# Subject baselines (computed on retained windows)\n",
    "baselines = (\n",
    "    train_df[train_df[\"label\"] == \"no_stress\"]\n",
    "    .groupby(\"subject\")[numeric_cols]\n",
    "    .mean()\n",
    "    .add_prefix(\"baseline_\")\n",
    ")\n",
    "train_df = train_df.join(baselines, on=\"subject\")\n",
    "baseline_cols = [c for c in train_df.columns if c.startswith(\"baseline_\")]\n",
    "for col in numeric_cols:\n",
    "    base_col = f\"baseline_{col}\"\n",
    "    if base_col in train_df:\n",
    "        train_df[col] = train_df[col] - train_df[base_col].fillna(0)\n",
    "train_df = train_df.drop(columns=baseline_cols, errors=\"ignore\")\n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "feature_cols = numeric_cols\n",
    "X = train_df[feature_cols]\n",
    "y = train_df[\"label\"].astype(str)\n",
    "groups = train_df[\"subject\"].to_numpy()\n",
    "phases = phase_series.to_numpy()\n",
    "phase_weight_vector = phase_series.map(lambda p: PHASE_WEIGHT_MAP.get(p, 1.0)).to_numpy()\n",
    "\n",
    "# Drop columns with all NaNs (after normalization)\n",
    "non_nan_cols = X.columns[~X.isna().all()]\n",
    "X = X[non_nan_cols]\n",
    "feature_cols = non_nan_cols.tolist()\n",
    "\n",
    "# Scale & encode\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "label_array = y.to_numpy()\n",
    "stress_mask_full = label_array != \"no_stress\"\n",
    "binary_labels = stress_mask_full.astype(int)\n",
    "binary_class_counts = pd.Series(binary_labels).value_counts()\n",
    "binary_class_weight_map = {\n",
    "    label: len(binary_labels) / (len(binary_class_counts) * count)\n",
    "    for label, count in binary_class_counts.items()\n",
    "}\n",
    "binary_sample_weight = np.array([\n",
    "    binary_class_weight_map[val] * phase_weight_vector[idx]\n",
    "    for idx, val in enumerate(binary_labels)\n",
    "])\n",
    "\n",
    "le_stress = LabelEncoder()\n",
    "le_stress.fit(label_array[stress_mask_full])\n",
    "y_stress_encoded_full = np.full(len(label_array), -1, dtype=int)\n",
    "y_stress_encoded_full[stress_mask_full] = le_stress.transform(label_array[stress_mask_full])\n",
    "stress_class_counts = pd.Series(label_array[stress_mask_full]).value_counts()\n",
    "stress_class_weight_map = {\n",
    "    label: len(stress_class_counts) / count\n",
    "    for label, count in stress_class_counts.items()\n",
    "}\n",
    "stress_class_weight_encoded = {\n",
    "    le_stress.transform([label])[0]: len(label_array[stress_mask_full]) / (len(stress_class_counts) * count)\n",
    "    for label, count in stress_class_counts.items()\n",
    "}\n",
    "\n",
    "X_stress_scaled = X_scaled[stress_mask_full]\n",
    "y_stress_encoded = y_stress_encoded_full[stress_mask_full]\n",
    "groups_stress = groups[stress_mask_full]\n",
    "phases_stress = phases[stress_mask_full]\n",
    "\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "class_weight_map = {\n",
    "    label: len(y) / (len(class_counts) * count)\n",
    "    for label, count in class_counts.items()\n",
    "}\n",
    "class_weight_encoded = {idx: class_weight_map[label] for idx, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/evaluate XGBoost\n",
    "- Grouped by subject to avoid leakage\n",
    "- Binary target `is_stress`\n",
    "- Basic hyperparameters; tune as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SMOTE k_neighbors=1 =====\n",
      "Mean metrics:\n",
      "accuracy       0.356783\n",
      "macro_f1       0.295590\n",
      "weighted_f1    0.282486\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 208.3             24.0                 287.0\n",
      "true_low_stress                  244.7              0.0                 247.0\n",
      "true_moderate_stress              73.7              2.0                 279.0\n",
      "===== SMOTE k_neighbors=2 =====\n",
      "Mean metrics:\n",
      "accuracy       0.348737\n",
      "macro_f1       0.286130\n",
      "weighted_f1    0.272019\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 190.7             25.0                 303.7\n",
      "true_low_stress                  244.7              0.0                 247.0\n",
      "true_moderate_stress              69.0              0.0                 285.7\n",
      "===== SMOTE k_neighbors=3 =====\n",
      "Mean metrics:\n",
      "accuracy       0.342880\n",
      "macro_f1       0.275926\n",
      "weighted_f1    0.256654\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 179.7             32.3                 307.3\n",
      "true_low_stress                  195.7              0.0                 296.0\n",
      "true_moderate_stress              65.0              1.0                 288.7\n",
      "===== SMOTE k_neighbors=4 =====\n",
      "Mean metrics:\n",
      "accuracy       0.349953\n",
      "macro_f1       0.280709\n",
      "weighted_f1    0.261353\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 191.0             17.3                 311.0\n",
      "true_low_stress                  203.0              0.0                 288.7\n",
      "true_moderate_stress              67.7              0.0                 287.0\n",
      "===== SMOTE k_neighbors=5 =====\n",
      "Mean metrics:\n",
      "accuracy       0.332399\n",
      "macro_f1       0.277724\n",
      "weighted_f1    0.265310\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 172.3             48.3                 298.7\n",
      "true_low_stress                  244.7              0.0                 247.0\n",
      "true_moderate_stress              73.0              0.0                 281.7\n",
      "===== SMOTE k_neighbors=6 =====\n",
      "Mean metrics:\n",
      "accuracy       0.336058\n",
      "macro_f1       0.276166\n",
      "weighted_f1    0.262243\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 175.7             41.0                 302.7\n",
      "true_low_stress                  239.3              0.0                 252.3\n",
      "true_moderate_stress              71.3              0.0                 283.3\n",
      "===== SMOTE k_neighbors=7 =====\n",
      "Mean metrics:\n",
      "accuracy       0.349473\n",
      "macro_f1       0.289434\n",
      "weighted_f1    0.275678\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 197.3             29.7                 292.3\n",
      "true_low_stress                  232.7              0.0                 259.0\n",
      "true_moderate_stress              71.0              3.7                 280.0\n",
      "===== SMOTE k_neighbors=8 =====\n",
      "Mean metrics:\n",
      "accuracy       0.326545\n",
      "macro_f1       0.260701\n",
      "weighted_f1    0.241497\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 161.3             28.7                 329.3\n",
      "true_low_stress                  214.7              0.0                 277.0\n",
      "true_moderate_stress              68.0              2.0                 284.7\n",
      "===== SMOTE k_neighbors=9 =====\n",
      "Mean metrics:\n",
      "accuracy       0.348493\n",
      "macro_f1       0.287472\n",
      "weighted_f1    0.273186\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 202.3             46.0                 271.0\n",
      "true_low_stress                  208.3              0.0                 283.3\n",
      "true_moderate_stress              81.0              0.0                 273.7\n",
      "===== SMOTE k_neighbors=10 =====\n",
      "Mean metrics:\n",
      "accuracy       0.345327\n",
      "macro_f1       0.281965\n",
      "weighted_f1    0.265463\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 179.0             32.7                 307.7\n",
      "true_low_stress                  239.3              0.0                 252.3\n",
      "true_moderate_stress              62.0              0.0                 292.7\n",
      "Best SMOTE k_neighbors: 1\n",
      "Mean metrics for best k:\n",
      "accuracy       0.356783\n",
      "macro_f1       0.295590\n",
      "weighted_f1    0.282486\n",
      "dtype: float64\n",
      "Average confusion matrix for best k:\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress\n",
      "true_high_stress                 208.3             24.0                 287.0\n",
      "true_low_stress                  244.7              0.0                 247.0\n",
      "true_moderate_stress              73.7              2.0                 279.0\n",
      "Stress label distribution after balancing:\n",
      "high_stress        1558\n",
      "low_stress         1475\n",
      "moderate_stress    1064\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Grid search over SMOTE k_neighbors for stress-stage classifier\n",
    "if len(np.unique(groups_stress)) < 2:\n",
    "    raise RuntimeError(\"Not enough stress samples for grid search.\")\n",
    "\n",
    "k_values = list(range(1, 11))\n",
    "results = []\n",
    "\n",
    "gkf = GroupKFold(n_splits=3 if len(np.unique(groups_stress)) >= 3 else len(np.unique(groups_stress)))\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"===== SMOTE k_neighbors={k} =====\")\n",
    "    metrics = []\n",
    "    cm_accum = np.zeros((len(le_stress.classes_), len(le_stress.classes_)), dtype=float)\n",
    "    smote = SMOTE(random_state=42, k_neighbors=k)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(gkf.split(X_stress_scaled, y_stress_encoded, groups_stress)):\n",
    "        X_train, X_test = X_stress_scaled[train_idx], X_stress_scaled[test_idx]\n",
    "        y_train, y_test = y_stress_encoded[train_idx], y_stress_encoded[test_idx]\n",
    "        X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "        sample_weight = np.array([stress_class_weight_encoded[val] for val in y_train_bal])\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            n_jobs=4,\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=len(le_stress.classes_),\n",
    "        )\n",
    "        model.fit(X_train_bal, y_train_bal, sample_weight=sample_weight)\n",
    "        preds_encoded = model.predict(X_test)\n",
    "        preds = le_stress.inverse_transform(preds_encoded)\n",
    "        true_labels = le_stress.inverse_transform(y_test)\n",
    "        acc = np.mean(preds == true_labels)\n",
    "        report_dict = classification_report(\n",
    "            true_labels,\n",
    "            preds,\n",
    "            labels=le_stress.classes_,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        cm = confusion_matrix(true_labels, preds, labels=le_stress.classes_)\n",
    "        cm_accum += cm\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"fold\": fold,\n",
    "                \"accuracy\": acc,\n",
    "                \"macro_f1\": report_dict[\"macro avg\"][\"f1-score\"],\n",
    "                \"weighted_f1\": report_dict[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    mean_metrics = metrics_df[[\"accuracy\", \"macro_f1\", \"weighted_f1\"]].mean()\n",
    "    avg_cm = cm_accum / max(1, len(metrics))\n",
    "    results.append(\n",
    "        {\n",
    "            \"k\": k,\n",
    "            \"metrics\": metrics_df,\n",
    "            \"mean_metrics\": mean_metrics,\n",
    "            \"avg_cm\": avg_cm,\n",
    "        }\n",
    "    )\n",
    "    print(\"Mean metrics:\")\n",
    "    print(mean_metrics)\n",
    "    print(\"Average confusion matrix (rows=true, cols=pred):\")\n",
    "    avg_cm_df = pd.DataFrame(\n",
    "        avg_cm,\n",
    "        index=[f\"true_{c}\" for c in le_stress.classes_],\n",
    "        columns=[f\"pred_{c}\" for c in le_stress.classes_],\n",
    "    )\n",
    "    print(avg_cm_df.round(1))\n",
    "\n",
    "best_result = max(results, key=lambda r: r[\"mean_metrics\"][\"macro_f1\"])\n",
    "best_k_value = best_result[\"k\"]\n",
    "best_mean_metrics = best_result[\"mean_metrics\"]\n",
    "best_avg_cm = best_result[\"avg_cm\"]\n",
    "print(\"Best SMOTE k_neighbors:\", best_k_value)\n",
    "print(\"Mean metrics for best k:\")\n",
    "print(best_mean_metrics)\n",
    "print(\"Average confusion matrix for best k:\")\n",
    "best_cm_df = pd.DataFrame(\n",
    "    best_avg_cm,\n",
    "    index=[f\"true_{c}\" for c in le_stress.classes_],\n",
    "    columns=[f\"pred_{c}\" for c in le_stress.classes_],\n",
    ")\n",
    "print(best_cm_df.round(1))\n",
    "print(\"Stress label distribution after balancing:\")\n",
    "print(pd.Series(le_stress.inverse_transform(y_stress_encoded)).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 36 parameter combinations\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3287350879894226, 'macro_f1': 0.26581104170835085, 'weighted_f1': 0.24978108736695512}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3116605482292123, 'macro_f1': 0.2541173313025846, 'weighted_f1': 0.23587066345718868}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.34117233112807804, 'macro_f1': 0.2816362278745374, 'weighted_f1': 0.26559729424371065}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3333657844982602, 'macro_f1': 0.27607586170082726, 'weighted_f1': 0.2582390511136626}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.30772473920637183, 'macro_f1': 0.24810873983945278, 'weighted_f1': 0.2277268945450643}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3231204574814525, 'macro_f1': 0.2621262234665624, 'weighted_f1': 0.242974434576014}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3353083113197745, 'macro_f1': 0.2702786094300929, 'weighted_f1': 0.24950214301012932}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.33457911010531566, 'macro_f1': 0.2757646566568858, 'weighted_f1': 0.2543130419424654}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.2980017751476384, 'macro_f1': 0.25195081632375005, 'weighted_f1': 0.23434526286799548}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.30678726852453786, 'macro_f1': 0.2602176235895008, 'weighted_f1': 0.24237775197193423}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.2972427087895744, 'macro_f1': 0.24794440341101912, 'weighted_f1': 0.22941030300661805}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.30212761590570336, 'macro_f1': 0.2481415727560604, 'weighted_f1': 0.22816113693561868}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.35971100522823046, 'macro_f1': 0.29733706853849845, 'weighted_f1': 0.2839333038157074}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3541004749251191, 'macro_f1': 0.29358553990475933, 'weighted_f1': 0.2800137228199923}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3558089822598655, 'macro_f1': 0.2898386206687131, 'weighted_f1': 0.2747979584028}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3543393117926992, 'macro_f1': 0.28908612175528176, 'weighted_f1': 0.27498249294599425}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3504392602729773, 'macro_f1': 0.28160654744763014, 'weighted_f1': 0.26254942525439035}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.35361564592023526, 'macro_f1': 0.2885400065451283, 'weighted_f1': 0.2735119728742818}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.35288018904691126, 'macro_f1': 0.28910569842287526, 'weighted_f1': 0.2744516066799601}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.36190291271255765, 'macro_f1': 0.29602015910109375, 'weighted_f1': 0.28159367416970743}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3304503943616033, 'macro_f1': 0.2755693152595074, 'weighted_f1': 0.2568609087459719}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3082506762657768, 'macro_f1': 0.2601276844315277, 'weighted_f1': 0.24264975055909255}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3440877275465681, 'macro_f1': 0.28129940321848773, 'weighted_f1': 0.2623393286983567}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3416534067376427, 'macro_f1': 0.27711501079985856, 'weighted_f1': 0.25788200021223506}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.34459113481130904, 'macro_f1': 0.2881104114138095, 'weighted_f1': 0.27436035908305817}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3475185030951058, 'macro_f1': 0.2924320362255756, 'weighted_f1': 0.27887437162083645}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.35361386004741363, 'macro_f1': 0.29388611320878355, 'weighted_f1': 0.27985769178987646}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.35970778892964744, 'macro_f1': 0.2983306353365842, 'weighted_f1': 0.28448907584457334}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3477610933580049, 'macro_f1': 0.29112604002528597, 'weighted_f1': 0.27580949555968415}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.33873872592464793, 'macro_f1': 0.28829431542586226, 'weighted_f1': 0.2747783667974826}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3443405069428689, 'macro_f1': 0.2841240938359042, 'weighted_f1': 0.26419733303099785}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.3526400971214125, 'macro_f1': 0.29755152451280653, 'weighted_f1': 0.2837049406119833}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.326298629001532, 'macro_f1': 0.27506532831464625, 'weighted_f1': 0.2578522565741379}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.314350325280598, 'macro_f1': 0.2660718308280167, 'weighted_f1': 0.249067442127347}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.3438521054071127, 'macro_f1': 0.28364872892050985, 'weighted_f1': 0.26419217344131757}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.339216404371183, 'macro_f1': 0.2815481052495416, 'weighted_f1': 0.26302615602207374}\n",
      "Best XGB params: {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400}\n",
      "Metrics for best params:\n",
      "accuracy       0.359708\n",
      "macro_f1       0.298331\n",
      "weighted_f1    0.284489\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# XGBoost hyperparameter grid search (using best SMOTE k)\n",
    "if \"best_k_value\" not in globals():\n",
    "    raise RuntimeError(\"Run the SMOTE k grid search cell first.\")\n",
    "\n",
    "param_grid = []\n",
    "for max_depth in [3, 4, 5]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for subsample in [0.7, 0.85]:\n",
    "            for colsample in [0.7, 0.9]:\n",
    "                param_grid.append(\n",
    "                    {\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"colsample_bytree\": colsample,\n",
    "                        \"n_estimators\": 400,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "print(f\"Testing {len(param_grid)} parameter combinations\")\n",
    "best_xgb_params = None\n",
    "best_param_metrics = None\n",
    "best_param_score = -np.inf\n",
    "param_results = []\n",
    "param_gkf = GroupKFold(n_splits=3 if len(np.unique(groups_stress)) >= 3 else len(np.unique(groups_stress)))\n",
    "smote_for_params = SMOTE(random_state=42, k_neighbors=best_k_value)\n",
    "\n",
    "for params in param_grid:\n",
    "    metrics = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(param_gkf.split(X_stress_scaled, y_stress_encoded, groups_stress)):\n",
    "        X_train, X_test = X_stress_scaled[train_idx], X_stress_scaled[test_idx]\n",
    "        y_train, y_test = y_stress_encoded[train_idx], y_stress_encoded[test_idx]\n",
    "        X_train_bal, y_train_bal = smote_for_params.fit_resample(X_train, y_train)\n",
    "        sample_weight = np.array([stress_class_weight_encoded[val] for val in y_train_bal])\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            subsample=params[\"subsample\"],\n",
    "            colsample_bytree=params[\"colsample_bytree\"],\n",
    "            n_jobs=4,\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=len(le_stress.classes_),\n",
    "        )\n",
    "        model.fit(X_train_bal, y_train_bal, sample_weight=sample_weight)\n",
    "        preds_encoded = model.predict(X_test)\n",
    "        preds = le_stress.inverse_transform(preds_encoded)\n",
    "        true_labels = le_stress.inverse_transform(y_test)\n",
    "        acc = np.mean(preds == true_labels)\n",
    "        report_dict = classification_report(\n",
    "            true_labels,\n",
    "            preds,\n",
    "            labels=le_stress.classes_,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"fold\": fold,\n",
    "                \"accuracy\": acc,\n",
    "                \"macro_f1\": report_dict[\"macro avg\"][\"f1-score\"],\n",
    "                \"weighted_f1\": report_dict[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    mean_metrics = metrics_df[[\"accuracy\", \"macro_f1\", \"weighted_f1\"]].mean()\n",
    "    param_results.append({\"params\": params, \"metrics\": metrics_df, \"mean\": mean_metrics})\n",
    "    print(f\"Params {params} -> {mean_metrics.to_dict()}\")\n",
    "    if mean_metrics[\"macro_f1\"] > best_param_score:\n",
    "        best_param_score = mean_metrics[\"macro_f1\"]\n",
    "        best_xgb_params = params\n",
    "        best_param_metrics = mean_metrics\n",
    "\n",
    "print(\"Best XGB params:\", best_xgb_params)\n",
    "print(\"Metrics for best params:\")\n",
    "print(best_param_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cross-validated metrics:\n",
      "   fold  accuracy  macro_f1  weighted_f1\n",
      "0     0  0.588800  0.348561     0.535952\n",
      "1     1  0.696484  0.405194     0.670069\n",
      "2     2  0.653113  0.401767     0.596816\n",
      "Mean metrics:\n",
      "accuracy       0.646132\n",
      "macro_f1       0.385174\n",
      "weighted_f1    0.600946\n",
      "dtype: float64\n",
      "Confusion matrix averaged across folds:\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 133.3              1.0                 305.7   \n",
      "true_low_stress                  194.3              0.0                 155.3   \n",
      "true_moderate_stress              73.7              4.0                 187.7   \n",
      "true_no_stress                    10.0              4.0                  76.3   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                79.3  \n",
      "true_low_stress                142.0  \n",
      "true_moderate_stress            89.3  \n",
      "true_no_stress                1751.3  \n",
      "Per-phase diagnostics:\n",
      "              phase  support  accuracy  macro_f1  weighted_f1\n",
      "0  Opposite Opinion     1644  0.253650  0.186835     0.186835\n",
      "1            Stroop      752  0.105053  0.073013     0.064857\n",
      "2              TMCT     1701  0.274544  0.187312     0.268073\n",
      "3           aerobic     1546  0.939198  0.242161     0.968646\n",
      "4         anaerobic      998  0.840681  0.228361     0.913446\n",
      "5              rest     2981  0.993962  0.249243     0.996972\n",
      "Per-phase class metrics:\n",
      "               phase            label  precision    recall        f1  support\n",
      "0   Opposite Opinion      high_stress   0.274603  0.420925  0.332373    411.0\n",
      "1   Opposite Opinion       low_stress   0.000000  0.000000  0.000000    822.0\n",
      "2   Opposite Opinion  moderate_stress   0.318954  0.593674  0.414966    411.0\n",
      "3   Opposite Opinion        no_stress   0.000000  0.000000  0.000000      0.0\n",
      "4             Stroop      high_stress   0.000000  0.000000  0.000000    418.0\n",
      "5             Stroop       low_stress   0.000000  0.000000  0.000000    167.0\n",
      "6             Stroop  moderate_stress   0.211230  0.473054  0.292052    167.0\n",
      "7             Stroop        no_stress   0.000000  0.000000  0.000000      0.0\n",
      "8               TMCT      high_stress   0.480932  0.311385  0.378018    729.0\n",
      "9               TMCT       low_stress   0.000000  0.000000  0.000000    486.0\n",
      "10              TMCT  moderate_stress   0.297398  0.493827  0.371230    486.0\n",
      "11              TMCT        no_stress   0.000000  0.000000  0.000000      0.0\n",
      "12           aerobic      high_stress   0.000000  0.000000  0.000000      0.0\n",
      "13           aerobic       low_stress   0.000000  0.000000  0.000000      0.0\n",
      "14           aerobic  moderate_stress   0.000000  0.000000  0.000000      0.0\n",
      "15           aerobic        no_stress   1.000000  0.939198  0.968646   1546.0\n",
      "16         anaerobic      high_stress   0.000000  0.000000  0.000000      0.0\n",
      "17         anaerobic       low_stress   0.000000  0.000000  0.000000      0.0\n",
      "18         anaerobic  moderate_stress   0.000000  0.000000  0.000000      0.0\n",
      "19         anaerobic        no_stress   1.000000  0.840681  0.913446    998.0\n",
      "20              rest      high_stress   0.000000  0.000000  0.000000      0.0\n",
      "21              rest       low_stress   0.000000  0.000000  0.000000      0.0\n",
      "22              rest  moderate_stress   0.000000  0.000000  0.000000      0.0\n",
      "23              rest        no_stress   1.000000  0.993962  0.996972   2981.0\n"
     ]
    }
   ],
   "source": [
    "# Train final model using best settings\n",
    "if \"best_k_value\" not in globals():\n",
    "    raise RuntimeError(\"Run the SMOTE k grid search cell first.\")\n",
    "\n",
    "final_params_stage2 = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 4,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "}\n",
    "if \"best_xgb_params\" in globals() and best_xgb_params:\n",
    "    final_params_stage2.update(best_xgb_params)\n",
    "\n",
    "final_params_stage1 = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 3,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "}\n",
    "\n",
    "final_metrics = []\n",
    "final_cm = np.zeros((len(le.classes_), len(le.classes_)), dtype=float)\n",
    "phase_records = []\n",
    "final_smote = SMOTE(random_state=42, k_neighbors=best_k_value)\n",
    "final_gkf = GroupKFold(n_splits=3 if len(np.unique(groups)) >= 3 else len(np.unique(groups)))\n",
    "\n",
    "label_strings = y.to_numpy()\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(final_gkf.split(X_scaled, y_encoded, groups)):\n",
    "    # Stage 1: stress vs no-stress\n",
    "    X_train_stage1, X_test_stage1 = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train_stage1, y_test_stage1 = binary_labels[train_idx], binary_labels[test_idx]\n",
    "    sample_weight_stage1 = binary_sample_weight[train_idx]\n",
    "\n",
    "    model_stage1 = XGBClassifier(\n",
    "        n_estimators=final_params_stage1[\"n_estimators\"],\n",
    "        learning_rate=final_params_stage1[\"learning_rate\"],\n",
    "        max_depth=final_params_stage1[\"max_depth\"],\n",
    "        subsample=final_params_stage1[\"subsample\"],\n",
    "        colsample_bytree=final_params_stage1[\"colsample_bytree\"],\n",
    "        n_jobs=4,\n",
    "        objective=\"binary:logistic\",\n",
    "    )\n",
    "    model_stage1.fit(X_train_stage1, y_train_stage1, sample_weight=sample_weight_stage1)\n",
    "\n",
    "    # Stage 2: low/mod/high classifier on stress windows\n",
    "    train_idx_stress = train_idx[stress_mask_full[train_idx]]\n",
    "    if len(train_idx_stress) == 0:\n",
    "        continue\n",
    "    X_train_stage2 = X_scaled[train_idx_stress]\n",
    "    y_train_stage2 = y_stress_encoded_full[train_idx_stress]\n",
    "    if len(np.unique(y_train_stage2)) < len(le_stress.classes_):\n",
    "        continue\n",
    "    X_train_stage2_bal, y_train_stage2_bal = final_smote.fit_resample(X_train_stage2, y_train_stage2)\n",
    "    sample_weight_stage2 = np.array([stress_class_weight_encoded[val] for val in y_train_stage2_bal])\n",
    "\n",
    "    model_stage2 = XGBClassifier(\n",
    "        n_estimators=final_params_stage2[\"n_estimators\"],\n",
    "        learning_rate=final_params_stage2[\"learning_rate\"],\n",
    "        max_depth=final_params_stage2[\"max_depth\"],\n",
    "        subsample=final_params_stage2[\"subsample\"],\n",
    "        colsample_bytree=final_params_stage2[\"colsample_bytree\"],\n",
    "        n_jobs=4,\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=len(le_stress.classes_),\n",
    "    )\n",
    "    model_stage2.fit(X_train_stage2_bal, y_train_stage2_bal, sample_weight=sample_weight_stage2)\n",
    "\n",
    "    # Predictions\n",
    "    stage1_proba = model_stage1.predict_proba(X_test_stage1)[:, 1]\n",
    "    stress_pred_mask = stage1_proba >= STRESS_PROB_THRESHOLD\n",
    "    preds_fold = np.array([\"no_stress\"] * len(test_idx), dtype=object)\n",
    "    if stress_pred_mask.any():\n",
    "        stress_test_indices = test_idx[stress_pred_mask]\n",
    "        X_test_stage2 = X_scaled[stress_test_indices]\n",
    "        stage2_preds_encoded = model_stage2.predict(X_test_stage2)\n",
    "        stage2_preds = le_stress.inverse_transform(stage2_preds_encoded)\n",
    "        preds_fold[stress_pred_mask] = stage2_preds\n",
    "\n",
    "    true_labels = label_strings[test_idx]\n",
    "    acc = np.mean(preds_fold == true_labels)\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        preds_fold,\n",
    "        labels=le.classes_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    cm = confusion_matrix(true_labels, preds_fold, labels=le.classes_)\n",
    "    final_cm += cm\n",
    "    phase_records.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"phase\": phases[test_idx],\n",
    "                \"true\": true_labels,\n",
    "                \"pred\": preds_fold,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    final_metrics.append(\n",
    "        {\n",
    "            \"fold\": fold,\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "if not final_metrics:\n",
    "    raise RuntimeError(\"No folds completed for final training.\")\n",
    "\n",
    "final_metrics_df = pd.DataFrame(final_metrics)\n",
    "print(\"Final cross-validated metrics:\")\n",
    "print(final_metrics_df)\n",
    "print(\"Mean metrics:\")\n",
    "print(final_metrics_df[[\"accuracy\", \"macro_f1\", \"weighted_f1\"]].mean())\n",
    "\n",
    "print(\"Confusion matrix averaged across folds:\")\n",
    "final_cm_df = pd.DataFrame(\n",
    "    final_cm / len(final_metrics),\n",
    "    index=[f\"true_{c}\" for c in le.classes_],\n",
    "    columns=[f\"pred_{c}\" for c in le.classes_],\n",
    ")\n",
    "print(final_cm_df.round(1))\n",
    "\n",
    "if phase_records:\n",
    "    phase_df = pd.concat(phase_records, ignore_index=True)\n",
    "    phase_summaries = []\n",
    "    per_phase_class_rows = []\n",
    "    for phase_name, group_df in phase_df.groupby(\"phase\"):\n",
    "        phase_report = classification_report(\n",
    "            group_df[\"true\"],\n",
    "            group_df[\"pred\"],\n",
    "            labels=le.classes_,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        phase_summaries.append(\n",
    "            {\n",
    "                \"phase\": phase_name,\n",
    "                \"support\": len(group_df),\n",
    "                \"accuracy\": np.mean(group_df[\"true\"] == group_df[\"pred\"]),\n",
    "                \"macro_f1\": phase_report[\"macro avg\"][\"f1-score\"],\n",
    "                \"weighted_f1\": phase_report[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "        for label in le.classes_:\n",
    "            if label not in phase_report:\n",
    "                continue\n",
    "            per_phase_class_rows.append(\n",
    "                {\n",
    "                    \"phase\": phase_name,\n",
    "                    \"label\": label,\n",
    "                    \"precision\": phase_report[label].get(\"precision\", 0.0),\n",
    "                    \"recall\": phase_report[label].get(\"recall\", 0.0),\n",
    "                    \"f1\": phase_report[label].get(\"f1-score\", 0.0),\n",
    "                    \"support\": phase_report[label].get(\"support\", 0),\n",
    "                }\n",
    "            )\n",
    "    phase_summary_df = pd.DataFrame(phase_summaries).sort_values(\"phase\")\n",
    "    print(\"Per-phase diagnostics:\")\n",
    "    print(phase_summary_df)\n",
    "    if per_phase_class_rows:\n",
    "        phase_class_df = pd.DataFrame(per_phase_class_rows)\n",
    "        print(\"Per-phase class metrics:\")\n",
    "        print(phase_class_df.sort_values([\"phase\", \"label\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
