{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Level Prediction Pipeline\n",
    "\n",
    "End-to-end preprocessing and modeling for stress vs non-stress, including aerobic/anaerobic as active negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy.signal import welch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "- Window length 60s, step 30s\n",
    "- Minimum label coverage 0.7\n",
    "- Protocol tag mapping per provided notebook\n",
    "- Data constraints applied during load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DEFAULT_DATASET_ROOT = Path(\"./Datasets\")\n",
    "DATASET_ROOT = Path(os.getenv(\"DATASET_ROOT\", DEFAULT_DATASET_ROOT))\n",
    "\n",
    "# Windowing\n",
    "WINDOW_SECONDS = 60\n",
    "WINDOW_STEP_SECONDS = 30\n",
    "MIN_LABEL_COVERAGE = 0.7\n",
    "\n",
    "# Sampling / weighting\n",
    "PHASE_BALANCE_NO_STRESS_RATIO = 2.0\n",
    "PHASE_WEIGHT_MAP = {\n",
    "    \"Stroop\": 1.5,\n",
    "    \"Opposite Opinion\": 1.4,\n",
    "    \"Real Opinion\": 1.2,\n",
    "    \"Subtract\": 1.2,\n",
    "    \"TMCT\": 1.3,\n",
    "}\n",
    "EMA_SPAN = 5\n",
    "PHASE_Z_MIN_STD = 1e-3\n",
    "\n",
    "# Data constraints\n",
    "DUPLICATE_CUTS = {\"S02\": {\"ACC\": 49545, \"BVP\": 99091, \"EDA\": 6195, \"TEMP\": 6195}}\n",
    "MISSING_SENSORS = {\"f07\": {\"BVP\", \"TEMP\", \"HR\", \"IBI\"}}\n",
    "\n",
    "STATES = [\"STRESS\", \"AEROBIC\", \"ANAEROBIC\"]\n",
    "\n",
    "# Stress protocol metadata\n",
    "STRESS_STAGE_ORDER_S = [\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_STAGE_ORDER_F = [\"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_TAG_PAIRS_S = [(3, 4), (5, 6), (7, 8), (9, 10), (11, 12)]\n",
    "STRESS_TAG_PAIRS_F = [(2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "STRESS_LEVEL_FILES = [\"Stress_Level_v1.csv\", \"Stress_Level_v2.csv\"]\n",
    "\n",
    "def load_stress_levels() -> Dict[str, Dict[str, float]]:\n",
    "    tables = []\n",
    "    for fname in STRESS_LEVEL_FILES:\n",
    "        path = Path(fname)\n",
    "        if not path.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        tables.append(df)\n",
    "    levels: Dict[str, Dict[str, float]] = {}\n",
    "    for table in tables:\n",
    "        for subject, row in table.iterrows():\n",
    "            subj = str(subject).strip()\n",
    "            levels[subj] = {col: (float(row[col]) if not pd.isna(row[col]) else np.nan) for col in table.columns}\n",
    "    return levels\n",
    "\n",
    "\n",
    "STRESS_LEVELS = load_stress_levels()\n",
    "\n",
    "\n",
    "STRESS_PHASES = {\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"}\n",
    "STRESS_LEVEL_BOUNDS = {\"low\": 3.0, \"moderate\": 6.0}\n",
    "TEMPORAL_FEATURES = [\"eda_mean\", \"eda_std\", \"acc_energy\", \"temp_mean\", \"hr_mean\"]\n",
    "FEATURE_EXCLUDE_COLS = {\"subject\", \"state\", \"phase\", \"stress_stage\", \"stress_level\", \"label\", \"is_stress\", \"win_start\", \"win_end\"}\n",
    "\n",
    "ACC_ACTIVITY_WINDOW_SEC = 2.0\n",
    "ACC_ACTIVITY_STEP_SEC = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Empatica format and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base subject ID without session suffix.\n",
    "def base_subject_id(subject: str) -> str:\n",
    "    return subject.split(\"_\")[0]\n",
    "\n",
    "\n",
    "# Read signal CSV and return (fs, data, start_timestamp).\n",
    "def read_signal(path: Path) -> Tuple[float, np.ndarray, pd.Timestamp]:\n",
    "    with open(path, \"r\") as f:\n",
    "        start_line = f.readline()\n",
    "        if not start_line:\n",
    "            raise ValueError(f\"Missing start timestamp in {path}\")\n",
    "        start = pd.to_datetime(start_line.split(\",\")[0])\n",
    "        fs_line = f.readline()\n",
    "        if not fs_line:\n",
    "            raise ValueError(f\"Missing sample rate in {path}\")\n",
    "        fs = float(fs_line.split(\",\")[0])\n",
    "        data = np.genfromtxt(f, delimiter=\",\")\n",
    "    if np.isscalar(data):\n",
    "        if math.isnan(float(data)):\n",
    "            data = np.empty((0, 1))\n",
    "        else:\n",
    "            data = np.array([[float(data)]])\n",
    "    elif data.size == 0:\n",
    "        data = np.empty((0, 1))\n",
    "    else:\n",
    "        data = np.asarray(data, dtype=float)\n",
    "        if np.isnan(data).all():\n",
    "            data = np.empty((0, 1))\n",
    "        elif data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "    return fs, data, start\n",
    "\n",
    "\n",
    "# Read IBI CSV and return inter-beat intervals in seconds.\n",
    "def read_ibi(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        return np.array([])\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None, skiprows=1)\n",
    "    except (pd.errors.EmptyDataError, FileNotFoundError):\n",
    "        return np.array([])\n",
    "    if df.empty:\n",
    "        return np.array([])\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "    if df.empty:\n",
    "        return np.array([])\n",
    "    col = 1 if df.shape[1] > 1 else 0\n",
    "    return df.iloc[:, col].to_numpy(dtype=float)\n",
    "\n",
    "\n",
    "# Read tags.csv and return list of tag timestamps (seconds since start).\n",
    "def read_tags(path: Path, start_ts: pd.Timestamp) -> List[Tuple[float, float]]:\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return []\n",
    "    tags = []\n",
    "    for ts_str in df[0].astype(str):\n",
    "        ts = pd.to_datetime(ts_str)\n",
    "        tags.append((ts - start_ts).total_seconds())\n",
    "    return [(t, t) for t in tags]\n",
    "\n",
    "\n",
    "# Extract stress intervals from tags based on subject version.\n",
    "def stress_intervals_from_tags(tags: List[Tuple[float, float]], subject: str) -> List[dict]:\n",
    "    if not tags:\n",
    "        return []\n",
    "    t = [x[0] for x in tags]\n",
    "    if subject.startswith(\"S\"):\n",
    "        idx_pairs = STRESS_TAG_PAIRS_S\n",
    "        stage_order = STRESS_STAGE_ORDER_S\n",
    "    else:\n",
    "        idx_pairs = STRESS_TAG_PAIRS_F\n",
    "        stage_order = STRESS_STAGE_ORDER_F\n",
    "    base_id = base_subject_id(subject)\n",
    "    spans = []\n",
    "    for stage, (i, j) in zip(stage_order, idx_pairs):\n",
    "        if i < len(t) and j < len(t) and t[j] > t[i]:\n",
    "            level = STRESS_LEVELS.get(base_id, {}).get(stage)\n",
    "            spans.append({\n",
    "                \"start\": t[i],\n",
    "                \"end\": t[j],\n",
    "                \"stage\": stage,\n",
    "                \"stress_level\": level,\n",
    "            })\n",
    "    return spans\n",
    "\n",
    "\n",
    "# Extract active intervals from tags.\n",
    "def active_intervals_from_tags(tags: List[Tuple[float, float]], label: str) -> List[dict]:\n",
    "    if len(tags) < 2:\n",
    "        return []\n",
    "    t = [x[0] for x in tags]\n",
    "    spans = []\n",
    "    for a, b in zip(t[:-1], t[1:]):\n",
    "        if b > a:\n",
    "            spans.append({\n",
    "                \"start\": a,\n",
    "                \"end\": b,\n",
    "                \"stage\": label,\n",
    "                \"stress_level\": 0.0,\n",
    "            })\n",
    "    return spans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Map numeric stress score to four-level category.\n",
    "def stress_bucket(level: float | None, phase: str | None) -> str:\n",
    "    if phase in {\"aerobic\", \"anaerobic\", \"rest\"}:\n",
    "        return \"no_stress\"\n",
    "    if level is None or pd.isna(level) or level <= 0:\n",
    "        return \"no_stress\"\n",
    "    if level <= STRESS_LEVEL_BOUNDS[\"low\"]:\n",
    "        return \"low_stress\"\n",
    "    if level <= STRESS_LEVEL_BOUNDS[\"moderate\"]:\n",
    "        return \"moderate_stress\"\n",
    "    return \"high_stress\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample signal to target frequency.\n",
    "def resample_to_rate(signal: np.ndarray, src_fs: float, tgt_fs: float) -> np.ndarray:\n",
    "    if signal.ndim == 1:\n",
    "        signal = signal[:, None]\n",
    "    src_len = signal.shape[0]\n",
    "    duration = src_len / src_fs\n",
    "    tgt_len = int(duration * tgt_fs)\n",
    "    src_t = np.linspace(0, duration, src_len)\n",
    "    tgt_t = np.linspace(0, duration, tgt_len)\n",
    "    resampled = np.stack([np.interp(tgt_t, src_t, signal[:, i]) for i in range(signal.shape[1])], axis=1)\n",
    "    if resampled.shape[1] == 1:\n",
    "        return resampled[:, 0]\n",
    "    return resampled\n",
    "\n",
    "# Simple Hampel filter for outlier suppression.\n",
    "def hampel_filter(x: np.ndarray, k: int = 5, t0: float = 3.0) -> np.ndarray:\n",
    "    x = x.copy()\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        window = x[max(i - k, 0): min(i + k, n)]\n",
    "        med = np.median(window)\n",
    "        mad = np.median(np.abs(window - med)) or 1e-6\n",
    "        if abs(x[i] - med) > t0 * mad:\n",
    "            x[i] = med\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate ACC activity envelope using sliding RMS to capture movement intensity.\n",
    "def acc_activity_signal(\n",
    "    acc_raw: np.ndarray,\n",
    "    fs: float,\n",
    "    win_sec: float = ACC_ACTIVITY_WINDOW_SEC,\n",
    "    step_sec: float = ACC_ACTIVITY_STEP_SEC,\n",
    ") -> np.ndarray:\n",
    "    if acc_raw.size == 0 or fs <= 0:\n",
    "        return np.array([])\n",
    "    if acc_raw.ndim == 1:\n",
    "        acc_raw = acc_raw[:, None]\n",
    "    magnitude = np.linalg.norm(acc_raw, axis=1)\n",
    "    win = max(1, int(round(win_sec * fs)))\n",
    "    step = max(1, int(round(step_sec * fs)))\n",
    "    if len(magnitude) < win:\n",
    "        rms = float(np.sqrt(np.mean(magnitude ** 2))) if len(magnitude) else 0.0\n",
    "        return np.array([rms])\n",
    "    activity = []\n",
    "    start = 0\n",
    "    while start + win <= len(magnitude):\n",
    "        segment = magnitude[start:start + win]\n",
    "        rms = float(np.sqrt(np.mean(segment ** 2)))\n",
    "        activity.append(rms)\n",
    "        start += step\n",
    "    if start < len(magnitude):\n",
    "        segment = magnitude[-win:]\n",
    "        rms = float(np.sqrt(np.mean(segment ** 2)))\n",
    "        if not activity or abs(activity[-1] - rms) > 1e-9:\n",
    "            activity.append(rms)\n",
    "    return np.asarray(activity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_centroid(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size == 0 or fs <= 0:\n",
    "        return np.nan\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(256, len(signal)))\n",
    "    total = psd.sum()\n",
    "    if total <= 0:\n",
    "        return np.nan\n",
    "    return float(np.sum(freqs * psd) / total)\n",
    "\n",
    "\n",
    "def bandpower(signal: np.ndarray, fs: float, fmin: float, fmax: float) -> float:\n",
    "    if signal.size == 0 or fs <= 0 or fmax <= fmin:\n",
    "        return np.nan\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(256, len(signal)))\n",
    "    mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    trap = getattr(np, \"trapezoid\", np.trapz)\n",
    "    return float(trap(psd[mask], freqs[mask]))\n",
    "\n",
    "\n",
    "def linear_slope(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size < 2 or fs <= 0:\n",
    "        return 0.0\n",
    "    t = np.arange(signal.size) / fs\n",
    "    slope, _ = np.polyfit(t, signal, 1)\n",
    "    return float(slope)\n",
    "\n",
    "\n",
    "def peak_rate(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size < 3 or fs <= 0:\n",
    "        return 0.0\n",
    "    duration = signal.size / fs\n",
    "    if duration <= 0:\n",
    "        return 0.0\n",
    "    peaks = np.sum((signal[1:-1] > signal[:-2]) & (signal[1:-1] > signal[2:]))\n",
    "    return float(peaks / duration)\n",
    "\n",
    "\n",
    "def eda_features(eda: np.ndarray, fs: float) -> Dict[str, float]:\n",
    "    if eda.size == 0 or fs <= 0:\n",
    "        return {\n",
    "            \"eda_mean\": np.nan,\n",
    "            \"eda_std\": np.nan,\n",
    "            \"eda_slope\": np.nan,\n",
    "            \"eda_peak_rate\": np.nan,\n",
    "            \"eda_range\": np.nan,\n",
    "            \"eda_power_slow\": np.nan,\n",
    "            \"eda_power_mid\": np.nan,\n",
    "            \"eda_power_fast\": np.nan,\n",
    "            \"eda_centroid\": np.nan,\n",
    "        }\n",
    "    clean = hampel_filter(eda)\n",
    "    return {\n",
    "        \"eda_mean\": float(np.mean(clean)),\n",
    "        \"eda_std\": float(np.std(clean)),\n",
    "        \"eda_slope\": linear_slope(clean, fs),\n",
    "        \"eda_peak_rate\": peak_rate(clean, fs),\n",
    "        \"eda_range\": float(np.max(clean) - np.min(clean)),\n",
    "        \"eda_power_slow\": bandpower(clean, fs, 0.01, 0.05),\n",
    "        \"eda_power_mid\": bandpower(clean, fs, 0.045, 0.25),\n",
    "        \"eda_power_fast\": bandpower(clean, fs, 0.25, 1.5),\n",
    "        \"eda_centroid\": spectral_centroid(clean, fs),\n",
    "    }\n",
    "\n",
    "\n",
    "def temp_features(temp: np.ndarray, fs: float) -> Dict[str, float]:\n",
    "    if temp.size == 0 or fs <= 0:\n",
    "        return {\n",
    "            \"temp_mean\": np.nan,\n",
    "            \"temp_std\": np.nan,\n",
    "            \"temp_slope\": np.nan,\n",
    "            \"temp_min\": np.nan,\n",
    "            \"temp_max\": np.nan,\n",
    "        }\n",
    "    return {\n",
    "        \"temp_mean\": float(np.mean(temp)),\n",
    "        \"temp_std\": float(np.std(temp)),\n",
    "        \"temp_slope\": linear_slope(temp, fs),\n",
    "        \"temp_min\": float(np.min(temp)),\n",
    "        \"temp_max\": float(np.max(temp)),\n",
    "    }\n",
    "\n",
    "\n",
    "def hrv_features(ibi: np.ndarray) -> Dict[str, float]:\n",
    "    if ibi.size < 2:\n",
    "        return {\n",
    "            \"hr_mean\": np.nan,\n",
    "            \"rmssd\": np.nan,\n",
    "            \"sdnn\": np.nan,\n",
    "            \"pnn50\": np.nan,\n",
    "            \"lf_hf\": np.nan,\n",
    "            \"sd1\": np.nan,\n",
    "            \"sd2\": np.nan,\n",
    "        }\n",
    "    rr = ibi.astype(float)\n",
    "    diff = np.diff(rr)\n",
    "    rmssd = float(np.sqrt(np.mean(diff ** 2))) if diff.size else np.nan\n",
    "    sdnn = float(np.std(rr))\n",
    "    pnn50 = float(np.mean(np.abs(diff) > 0.05)) if diff.size else np.nan\n",
    "    hr_mean = float(60.0 / np.mean(rr)) if np.mean(rr) > 0 else np.nan\n",
    "\n",
    "    if diff.size:\n",
    "        sd1 = float(np.sqrt(0.5) * np.std(diff))\n",
    "    else:\n",
    "        sd1 = np.nan\n",
    "    if not np.isnan(sdnn) and not np.isnan(sd1):\n",
    "        sd2_sq = max(0.0, 2 * (sdnn ** 2) - 0.5 * (sd1 ** 2))\n",
    "        sd2 = float(np.sqrt(sd2_sq))\n",
    "    else:\n",
    "        sd2 = np.nan\n",
    "\n",
    "    lf_hf = np.nan\n",
    "    try:\n",
    "        t = np.cumsum(rr)\n",
    "        t = t - t[0]\n",
    "        if t[-1] > 0 and rr.size >= 4:\n",
    "            fs_interp = 4.0\n",
    "            grid = np.arange(0, t[-1], 1 / fs_interp)\n",
    "            if grid.size >= 8:\n",
    "                interp_rr = np.interp(grid, t[: len(grid)], rr[: len(grid)])\n",
    "                lf = bandpower(interp_rr, fs_interp, 0.04, 0.15)\n",
    "                hf = bandpower(interp_rr, fs_interp, 0.15, 0.4)\n",
    "                if hf and hf > 0:\n",
    "                    lf_hf = float(lf / hf)\n",
    "    except Exception:\n",
    "        lf_hf = np.nan\n",
    "\n",
    "    return {\n",
    "        \"hr_mean\": hr_mean,\n",
    "        \"rmssd\": rmssd,\n",
    "        \"sdnn\": sdnn,\n",
    "        \"pnn50\": pnn50,\n",
    "        \"lf_hf\": lf_hf,\n",
    "        \"sd1\": sd1,\n",
    "        \"sd2\": sd2,\n",
    "    }\n",
    "\n",
    "\n",
    "def acc_features(acc_mag: np.ndarray, fs: float, acc_activity: np.ndarray | None = None) -> Dict[str, float]:\n",
    "    if len(acc_mag) == 0:\n",
    "        base = {\n",
    "            \"acc_mean\": np.nan,\n",
    "            \"acc_std\": np.nan,\n",
    "            \"acc_energy\": np.nan,\n",
    "            \"acc_peak_freq\": np.nan,\n",
    "            \"acc_bandpower_low\": np.nan,\n",
    "            \"acc_bandpower_mid\": np.nan,\n",
    "            \"acc_bandpower_high\": np.nan,\n",
    "            \"acc_mad\": np.nan,\n",
    "        }\n",
    "    else:\n",
    "        energy = np.mean(acc_mag ** 2)\n",
    "        base = {\n",
    "            \"acc_mean\": float(np.mean(acc_mag)),\n",
    "            \"acc_std\": float(np.std(acc_mag)),\n",
    "            \"acc_energy\": float(energy),\n",
    "            \"acc_peak_freq\": spectral_centroid(acc_mag, fs),\n",
    "            \"acc_bandpower_low\": bandpower(acc_mag, fs, 0.1, 0.5),\n",
    "            \"acc_bandpower_mid\": bandpower(acc_mag, fs, 0.5, 2.0),\n",
    "            \"acc_bandpower_high\": bandpower(acc_mag, fs, 2.0, 8.0),\n",
    "            \"acc_mad\": float(np.median(np.abs(acc_mag - np.median(acc_mag)))),\n",
    "        }\n",
    "    if acc_activity is not None and len(acc_activity):\n",
    "        base.update(\n",
    "            {\n",
    "                \"acc_activity_mean\": float(np.mean(acc_activity)),\n",
    "                \"acc_activity_std\": float(np.std(acc_activity)),\n",
    "                \"acc_activity_max\": float(np.max(acc_activity)),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        base.update(\n",
    "            {\n",
    "                \"acc_activity_mean\": np.nan,\n",
    "                \"acc_activity_std\": np.nan,\n",
    "                \"acc_activity_max\": np.nan,\n",
    "            }\n",
    "        )\n",
    "    return base\n",
    "\n",
    "\n",
    "def combine_features(eda, eda_fs, acc_mag, acc_fs, temp, temp_fs, ibi, acc_activity=None) -> Dict[str, float]:\n",
    "    feats = {}\n",
    "    feats.update(eda_features(eda, eda_fs))\n",
    "    feats.update(acc_features(acc_mag, acc_fs, acc_activity))\n",
    "    feats.update(temp_features(temp, temp_fs))\n",
    "    feats.update(hrv_features(ibi))\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing and label assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Window:\n",
    "    start: float\n",
    "    end: float\n",
    "    label: str\n",
    "    subject: str\n",
    "    state: str\n",
    "\n",
    "\n",
    "def window_intervals(duration: float, win_s: int = WINDOW_SECONDS, step_s: int = WINDOW_STEP_SECONDS) -> List[Tuple[float, float]]:\n",
    "    windows = []\n",
    "    t = 0.0\n",
    "    while t + win_s <= duration:\n",
    "        windows.append((t, t + win_s))\n",
    "        t += step_s\n",
    "    return windows\n",
    "\n",
    "\n",
    "def _span_bounds(span) -> Tuple[float, float]:\n",
    "    if isinstance(span, dict):\n",
    "        return span[\"start\"], span[\"end\"]\n",
    "    return span\n",
    "\n",
    "\n",
    "def assign_label(win: Tuple[float, float], intervals: Dict[str, List[dict]]) -> Tuple[str | None, dict | None]:\n",
    "    start, end = win\n",
    "    length = end - start\n",
    "    best_label = None\n",
    "    best_cov = 0.0\n",
    "    best_span = None\n",
    "    for lbl, spans in intervals.items():\n",
    "        label_overlap = 0.0\n",
    "        label_best_span = None\n",
    "        label_best_overlap = 0.0\n",
    "        for span in spans:\n",
    "            a, b = _span_bounds(span)\n",
    "            inter = max(0.0, min(end, b) - max(start, a))\n",
    "            if inter > 0:\n",
    "                label_overlap += inter\n",
    "                if inter > label_best_overlap:\n",
    "                    label_best_overlap = inter\n",
    "                    label_best_span = span\n",
    "        coverage = label_overlap / length\n",
    "        if coverage > best_cov:\n",
    "            best_cov = coverage\n",
    "            best_label = lbl\n",
    "            best_span = label_best_span\n",
    "    if best_cov >= MIN_LABEL_COVERAGE and best_label is not None:\n",
    "        return best_label, best_span\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def make_label_intervals(state: str, subject: str, tags: List[Tuple[float, float]], duration: float) -> Dict[str, List[dict]]:\n",
    "    rest_span = {\"start\": 0.0, \"end\": duration, \"stage\": \"rest\", \"stress_level\": 0.0}\n",
    "    if state == \"STRESS\":\n",
    "        stress_spans = stress_intervals_from_tags(tags, subject)\n",
    "        if not stress_spans:\n",
    "            return {\"rest\": [rest_span]}\n",
    "        return {\n",
    "            \"stress\": stress_spans,\n",
    "            \"rest\": [rest_span],\n",
    "        }\n",
    "    else:\n",
    "        lbl = \"aerobic\" if state == \"AEROBIC\" else \"anaerobic\"\n",
    "        active = active_intervals_from_tags(tags, lbl)\n",
    "        return {\n",
    "            lbl: active,\n",
    "            \"rest\": [rest_span],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion: read signals per subject/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_state(state: str, subject: str) -> dict:\n",
    "    folder = DATASET_ROOT / state / subject\n",
    "    base_id = base_subject_id(subject)\n",
    "    sensors = {}\n",
    "    fs_map = {}\n",
    "    missing = MISSING_SENSORS.get(base_id, set())\n",
    "\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(folder)\n",
    "    fs_eda, eda_raw, start_ts = read_signal(folder / \"EDA.csv\")\n",
    "    sensors[\"EDA\"] = np.squeeze(eda_raw)\n",
    "    fs_map[\"EDA\"] = fs_eda\n",
    "\n",
    "    temp_path = folder / \"TEMP.csv\"\n",
    "    if \"TEMP\" not in missing and temp_path.exists():\n",
    "        fs_temp, temp_raw, _ = read_signal(temp_path)\n",
    "        sensors[\"TEMP\"] = np.squeeze(temp_raw)\n",
    "        fs_map[\"TEMP\"] = fs_temp\n",
    "\n",
    "    fs_acc, acc_raw, _ = read_signal(folder / \"ACC.csv\")\n",
    "    acc_mag = np.linalg.norm(acc_raw, axis=1)\n",
    "    sensors[\"ACC_MAG\"] = acc_mag\n",
    "    fs_map[\"ACC_MAG\"] = fs_acc\n",
    "    acc_activity = acc_activity_signal(acc_raw, fs_acc)\n",
    "\n",
    "    if acc_activity.size:\n",
    "        sensors[\"ACC_ACTIVITY\"] = acc_activity\n",
    "        fs_map[\"ACC_ACTIVITY\"] = 1.0 / ACC_ACTIVITY_STEP_SEC\n",
    "\n",
    "\n",
    "    if \"IBI\" not in missing:\n",
    "        sensors[\"IBI\"] = read_ibi(folder / \"IBI.csv\")\n",
    "    else:\n",
    "        sensors[\"IBI\"] = np.array([])\n",
    "    tags = read_tags(folder / \"tags.csv\", start_ts)\n",
    "\n",
    "    if base_id in DUPLICATE_CUTS:\n",
    "        cuts = DUPLICATE_CUTS[base_id]\n",
    "        if \"EDA\" in cuts and \"EDA\" in sensors:\n",
    "            sensors[\"EDA\"] = sensors[\"EDA\"][:cuts[\"EDA\"]]\n",
    "        if \"TEMP\" in cuts and \"TEMP\" in sensors:\n",
    "            sensors[\"TEMP\"] = sensors[\"TEMP\"][:cuts[\"TEMP\"]]\n",
    "        if \"ACC\" in cuts and \"ACC_MAG\" in sensors:\n",
    "            sensors[\"ACC_MAG\"] = sensors[\"ACC_MAG\"][:cuts[\"ACC\"]]\n",
    "\n",
    "    duration = len(sensors[\"EDA\"]) / fs_map[\"EDA\"]\n",
    "    return {\n",
    "        \"sensors\": sensors,\n",
    "        \"fs\": fs_map,\n",
    "        \"tags\": tags,\n",
    "        \"duration\": duration,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subject_baselines(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    baseline = df[df[\"label\"] == \"no_stress\"].groupby(\"subject\").mean(numeric_only=True)\n",
    "    return baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build windowed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_windows_for_subject(state: str, subject: str, tgt_fs: float = 4.0) -> List[dict]:\n",
    "    info = load_subject_state(state, subject)\n",
    "    sensors = info[\"sensors\"]\n",
    "    fs_map = info[\"fs\"]\n",
    "    tags = info[\"tags\"]\n",
    "    duration = info[\"duration\"]\n",
    "\n",
    "    # Resample signals\n",
    "    eda = resample_to_rate(sensors[\"EDA\"], fs_map[\"EDA\"], tgt_fs)\n",
    "    temp = resample_to_rate(sensors[\"TEMP\"], fs_map.get(\"TEMP\", tgt_fs), tgt_fs) if \"TEMP\" in sensors else np.array([])\n",
    "    acc = resample_to_rate(sensors[\"ACC_MAG\"], fs_map[\"ACC_MAG\"], tgt_fs)\n",
    "    acc_activity = resample_to_rate(sensors[\"ACC_ACTIVITY\"], fs_map.get(\"ACC_ACTIVITY\", tgt_fs), tgt_fs) if \"ACC_ACTIVITY\" in sensors else np.array([])\n",
    "\n",
    "    intervals = make_label_intervals(state, subject, tags, duration)\n",
    "    windows = window_intervals(duration, WINDOW_SECONDS, WINDOW_STEP_SECONDS)\n",
    "    rows = []\n",
    "    for w in windows:\n",
    "        lbl, span_meta = assign_label(w, intervals)\n",
    "        if lbl is None or span_meta is None:\n",
    "            continue\n",
    "        start_idx = int(w[0] * tgt_fs)\n",
    "        end_idx = int(w[1] * tgt_fs)\n",
    "        eda_win = eda[start_idx:end_idx]\n",
    "        temp_win = temp[start_idx:end_idx] if len(temp) else np.array([])\n",
    "        acc_win = acc[start_idx:end_idx]\n",
    "        activity_win = acc_activity[start_idx:end_idx] if len(acc_activity) else np.array([])\n",
    "        ibi_win = sensors[\"IBI\"][(sensors[\"IBI\"] > 0)] if len(sensors[\"IBI\"]) else np.array([])\n",
    "\n",
    "        feats = combine_features(eda_win, tgt_fs, acc_win, tgt_fs, temp_win, tgt_fs, ibi_win, activity_win)\n",
    "        stress_stage = span_meta.get(\"stage\") if isinstance(span_meta, dict) else None\n",
    "        stress_level = span_meta.get(\"stress_level\") if isinstance(span_meta, dict) else None\n",
    "        phase_start = span_meta.get(\"start\") if isinstance(span_meta, dict) else None\n",
    "        phase_end = span_meta.get(\"end\") if isinstance(span_meta, dict) else None\n",
    "        if lbl == \"stress\":\n",
    "            if stress_level is None or np.isnan(stress_level):\n",
    "                continue\n",
    "        else:\n",
    "            stress_level = 0.0 if stress_level is None or np.isnan(stress_level) else stress_level\n",
    "        phase_label = stress_stage if stress_stage else lbl\n",
    "        stress_class = stress_bucket(stress_level, phase_label)\n",
    "        phase_duration = None\n",
    "        phase_progress = None\n",
    "        phase_elapsed = None\n",
    "        if phase_start is not None and phase_end is not None and phase_end > phase_start:\n",
    "            phase_duration = float(phase_end - phase_start)\n",
    "            phase_elapsed = float(w[0] - phase_start)\n",
    "            phase_progress = max(0.0, min(1.0, phase_elapsed / phase_duration))\n",
    "        row = {\n",
    "            \"subject\": base_subject_id(subject),\n",
    "            \"state\": state.lower(),\n",
    "            \"phase\": phase_label,\n",
    "            \"stress_stage\": stress_stage if lbl == \"stress\" else None,\n",
    "            \"stress_level\": float(stress_level),\n",
    "            \"label\": stress_class,\n",
    "            \"is_stress\": 1 if phase_label in STRESS_PHASES else 0,\n",
    "            \"win_start\": w[0],\n",
    "            \"win_end\": w[1],\n",
    "            \"phase_start\": phase_start,\n",
    "            \"phase_end\": phase_end,\n",
    "            \"phase_duration\": phase_duration,\n",
    "            \"phase_elapsed\": phase_elapsed,\n",
    "            \"phase_progress\": phase_progress,\n",
    "        }\n",
    "        row.update(feats)\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def build_dataset(states: List[str] = STATES, max_subjects: int | None = None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for state in states:\n",
    "        subjects = sorted([p.name for p in (DATASET_ROOT / state).iterdir() if p.is_dir()])\n",
    "        if max_subjects:\n",
    "            subjects = subjects[:max_subjects]\n",
    "        for subj in subjects:\n",
    "            try:\n",
    "                rows.extend(build_windows_for_subject(state, subj))\n",
    "            except Exception as e:\n",
    "                print(f\"Skip {state}/{subj}: {e}\")\n",
    "                continue\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows 6742\n",
      "Phase counts phase\n",
      "rest                2981\n",
      "aerobic             1546\n",
      "anaerobic            998\n",
      "TMCT                 633\n",
      "Opposite Opinion     432\n",
      "Stroop               152\n",
      "Name: count, dtype: int64\n",
      "Label distribution label\n",
      "no_stress          5525\n",
      "moderate_stress     607\n",
      "high_stress         355\n",
      "low_stress          255\n",
      "Name: count, dtype: int64\n",
      "Stress level summary count    6742.000000\n",
      "mean        0.929398\n",
      "std         2.179948\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max        10.000000\n",
      "Name: stress_level, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = build_dataset()\n",
    "df = df.sort_values([\"subject\", \"win_start\"]).reset_index(drop=True)\n",
    "\n",
    "phase_group = df.groupby([\"subject\", \"phase\"], dropna=False)\n",
    "phase_min_start = phase_group[\"win_start\"].transform(\"min\")\n",
    "phase_max_end = phase_group[\"win_end\"].transform(\"max\")\n",
    "phase_duration_fallback = (phase_max_end - phase_min_start).replace(0, np.nan)\n",
    "df[\"phase_elapsed\"] = df[\"phase_elapsed\"].fillna(df[\"win_start\"] - phase_min_start)\n",
    "df[\"phase_duration\"] = df[\"phase_duration\"].fillna(phase_duration_fallback)\n",
    "df[\"phase_progress\"] = df[\"phase_progress\"].fillna(\n",
    "    df[\"phase_elapsed\"] / df[\"phase_duration\"].replace(0, np.nan)\n",
    ")\n",
    "df[\"phase_progress\"] = df[\"phase_progress\"].clip(0.0, 1.0).fillna(0.0)\n",
    "df[\"phase_position\"] = phase_group.cumcount()\n",
    "phase_counts = phase_group.size().rename(\"phase_size\")\n",
    "df = df.join(phase_counts, on=[\"subject\", \"phase\"])\n",
    "df[\"phase_position\"] = np.where(\n",
    "    df[\"phase_size\"] > 1,\n",
    "    df[\"phase_position\"] / (df[\"phase_size\"] - 1),\n",
    "    0.0,\n",
    ")\n",
    "df.drop(columns=[\"phase_size\"], inplace=True)\n",
    "\n",
    "context_cols = []\n",
    "for feat in TEMPORAL_FEATURES:\n",
    "    prev_col = f\"prev_{feat}\"\n",
    "    delta_col = f\"delta_{feat}\"\n",
    "    roll_mean_col = f\"roll_mean_{feat}\"\n",
    "    roll_std_col = f\"roll_std_{feat}\"\n",
    "    ema_col = f\"ema_{feat}\"\n",
    "    phase_delta_col = f\"phase_delta_{feat}\"\n",
    "    phase_z_col = f\"phase_z_{feat}\"\n",
    "    subj_group = df.groupby(\"subject\")[feat]\n",
    "    df[prev_col] = subj_group.shift(1)\n",
    "    df[delta_col] = df[feat] - df[prev_col]\n",
    "    roll_mean = (\n",
    "        subj_group.rolling(window=3, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    roll_std = (\n",
    "        subj_group.rolling(window=3, min_periods=1)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df[roll_mean_col] = roll_mean.to_numpy()\n",
    "    df[roll_std_col] = roll_std.to_numpy()\n",
    "    df[ema_col] = df.groupby(\"subject\")[feat].transform(lambda s: s.ewm(span=EMA_SPAN, adjust=False).mean())\n",
    "    phase_first = (\n",
    "        df.sort_values(\"win_start\")\n",
    "        .groupby([\"subject\", \"phase\"], dropna=False)[feat]\n",
    "        .transform(\"first\")\n",
    "    )\n",
    "    df[phase_delta_col] = df[feat] - phase_first\n",
    "    phase_running_mean = phase_group[feat].transform(lambda s: s.expanding().mean())\n",
    "    phase_running_std = phase_group[feat].transform(lambda s: s.expanding().std()).replace(0, np.nan)\n",
    "    df[phase_z_col] = (df[feat] - phase_running_mean) / phase_running_std\n",
    "    df[phase_z_col] = df[phase_z_col].replace([np.inf, -np.inf], 0)\n",
    "    context_cols.extend([prev_col, delta_col, roll_mean_col, roll_std_col, ema_col, phase_delta_col, phase_z_col])\n",
    "\n",
    "df[context_cols] = df[context_cols].fillna(0)\n",
    "\n",
    "df.to_csv(\"stress_level_dataset.csv\", index=False)\n",
    "print(\"Rows\", len(df))\n",
    "print(\"Phase counts\", df[\"phase\"].value_counts())\n",
    "print(\"Label distribution\", df[\"label\"].value_counts())\n",
    "print(\"Stress level summary\", df[\"stress_level\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "numeric_cols = [\n",
    "    c for c in df.columns if c not in FEATURE_EXCLUDE_COLS and np.issubdtype(df[c].dtype, np.number)\n",
    "]\n",
    "helper_cols = [\"label\", \"subject\", \"phase\", \"win_start\"]\n",
    "train_df = df.loc[:, numeric_cols + helper_cols].copy()\n",
    "train_df = train_df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def balance_by_phase_label(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    balanced = []\n",
    "    for phase_name, phase_df in data.groupby(\"phase\"):\n",
    "        if phase_df.empty:\n",
    "            continue\n",
    "        stress_df = phase_df[phase_df[\"label\"] != \"no_stress\"]\n",
    "        phase_parts = []\n",
    "        if not stress_df.empty:\n",
    "            target = stress_df[\"label\"].value_counts().max()\n",
    "            target = max(1, int(np.ceil(target * PHASE_WEIGHT_MAP.get(phase_name, 1.0))))\n",
    "            for label_name, label_df in stress_df.groupby(\"label\"):\n",
    "                if len(label_df) < target:\n",
    "                    phase_parts.append(label_df.sample(target, replace=True, random_state=42))\n",
    "                else:\n",
    "                    phase_parts.append(label_df)\n",
    "        else:\n",
    "            target = len(phase_df)\n",
    "        no_stress_df = phase_df[phase_df[\"label\"] == \"no_stress\"]\n",
    "        if not no_stress_df.empty:\n",
    "            cap = int(np.ceil(PHASE_BALANCE_NO_STRESS_RATIO * target)) if target else len(no_stress_df)\n",
    "            if cap > 0:\n",
    "                if len(no_stress_df) > cap:\n",
    "                    phase_parts.append(no_stress_df.sample(cap, random_state=42, replace=False))\n",
    "                else:\n",
    "                    phase_parts.append(no_stress_df)\n",
    "        if phase_parts:\n",
    "            balanced.append(pd.concat(phase_parts, ignore_index=True))\n",
    "        else:\n",
    "            balanced.append(phase_df)\n",
    "    return pd.concat(balanced, ignore_index=True)\n",
    "\n",
    "train_df = balance_by_phase_label(train_df)\n",
    "phase_series = train_df[\"phase\"].fillna(\"unknown\")\n",
    "\n",
    "# Phase-specific baselines (relative to first window in each phase)\n",
    "phase_sorted = train_df.sort_values([\"subject\", \"phase\", \"win_start\"])\n",
    "phase_baselines = (\n",
    "    phase_sorted.groupby([\"subject\", \"phase\"], dropna=False)[numeric_cols]\n",
    "    .transform(\"first\")\n",
    ")\n",
    "train_df[numeric_cols] = train_df[numeric_cols] - phase_baselines.fillna(0)\n",
    "\n",
    "# Subject baselines (computed on retained windows)\n",
    "baselines = (\n",
    "    train_df[train_df[\"label\"] == \"no_stress\"]\n",
    "    .groupby(\"subject\")[numeric_cols]\n",
    "    .mean()\n",
    "    .add_prefix(\"baseline_\")\n",
    ")\n",
    "train_df = train_df.join(baselines, on=\"subject\")\n",
    "baseline_cols = [c for c in train_df.columns if c.startswith(\"baseline_\")]\n",
    "for col in numeric_cols:\n",
    "    base_col = f\"baseline_{col}\"\n",
    "    if base_col in train_df:\n",
    "        train_df[col] = train_df[col] - train_df[base_col].fillna(0)\n",
    "train_df = train_df.drop(columns=baseline_cols, errors=\"ignore\")\n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "feature_cols = numeric_cols\n",
    "X = train_df[feature_cols]\n",
    "y = train_df[\"label\"].astype(str)\n",
    "groups = train_df[\"subject\"].to_numpy()\n",
    "phases = phase_series.to_numpy()\n",
    "\n",
    "# Drop columns with all NaNs (after normalization)\n",
    "non_nan_cols = X.columns[~X.isna().all()]\n",
    "X = X[non_nan_cols]\n",
    "feature_cols = non_nan_cols.tolist()\n",
    "\n",
    "# Scale & encode\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "class_counts = pd.Series(y).value_counts()\n",
    "class_weight_map = {\n",
    "    label: len(y) / (len(class_counts) * count)\n",
    "    for label, count in class_counts.items()\n",
    "}\n",
    "class_weight_encoded = {idx: class_weight_map[label] for idx, label in enumerate(le.classes_)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/evaluate XGBoost\n",
    "- Grouped by subject to avoid leakage\n",
    "- Binary target `is_stress`\n",
    "- Basic hyperparameters; tune as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== SMOTE k_neighbors=1 =====\n",
      "Mean metrics:\n",
      "accuracy       0.767991\n",
      "macro_f1       0.489172\n",
      "weighted_f1    0.744656\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 119.0             54.3                  78.3   \n",
      "true_low_stress                    6.0             33.3                 175.7   \n",
      "true_moderate_stress              26.0             42.3                 122.0   \n",
      "true_no_stress                     4.3              6.7                  12.7   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                42.3  \n",
      "true_low_stress                 79.0  \n",
      "true_moderate_stress           103.7  \n",
      "true_no_stress                1818.0  \n",
      "===== SMOTE k_neighbors=2 =====\n",
      "Mean metrics:\n",
      "accuracy       0.769215\n",
      "macro_f1       0.497152\n",
      "weighted_f1    0.750094\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 123.0             55.3                  78.3   \n",
      "true_low_stress                    3.7             42.7                 170.3   \n",
      "true_moderate_stress              30.7             51.7                 121.3   \n",
      "true_no_stress                     7.3             10.3                  15.7   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                37.3  \n",
      "true_low_stress                 77.3  \n",
      "true_moderate_stress            90.3  \n",
      "true_no_stress                1808.3  \n",
      "===== SMOTE k_neighbors=3 =====\n",
      "Mean metrics:\n",
      "accuracy       0.768498\n",
      "macro_f1       0.489799\n",
      "weighted_f1    0.747119\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 138.0             62.0                  58.3   \n",
      "true_low_stress                    7.3             24.0                 185.0   \n",
      "true_moderate_stress              30.7             51.7                 120.7   \n",
      "true_no_stress                     4.7              7.7                  18.3   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                35.7  \n",
      "true_low_stress                 77.7  \n",
      "true_moderate_stress            91.0  \n",
      "true_no_stress                1811.0  \n",
      "===== SMOTE k_neighbors=4 =====\n",
      "Mean metrics:\n",
      "accuracy       0.768609\n",
      "macro_f1       0.498926\n",
      "weighted_f1    0.752877\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 142.0             36.0                  77.7   \n",
      "true_low_stress                    8.3             32.3                 198.3   \n",
      "true_moderate_stress              29.7             67.0                 117.7   \n",
      "true_no_stress                     5.3             14.3                  20.0   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                38.3  \n",
      "true_low_stress                 55.0  \n",
      "true_moderate_stress            79.7  \n",
      "true_no_stress                1802.0  \n",
      "===== SMOTE k_neighbors=5 =====\n",
      "Mean metrics:\n",
      "accuracy       0.771359\n",
      "macro_f1       0.497682\n",
      "weighted_f1    0.752842\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 145.7             57.3                  67.7   \n",
      "true_low_stress                   10.3             24.3                 185.7   \n",
      "true_moderate_stress              30.0             48.7                 122.3   \n",
      "true_no_stress                     6.7              9.0                  16.7   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                23.3  \n",
      "true_low_stress                 73.7  \n",
      "true_moderate_stress            93.0  \n",
      "true_no_stress                1809.3  \n",
      "===== SMOTE k_neighbors=6 =====\n",
      "Mean metrics:\n",
      "accuracy       0.755256\n",
      "macro_f1       0.455536\n",
      "weighted_f1    0.735936\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 111.0             92.7                  67.0   \n",
      "true_low_stress                    8.3             16.3                 219.7   \n",
      "true_moderate_stress              29.0             64.7                 124.0   \n",
      "true_no_stress                     4.7             10.7                  20.3   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                23.3  \n",
      "true_low_stress                 49.7  \n",
      "true_moderate_stress            76.3  \n",
      "true_no_stress                1806.0  \n",
      "===== SMOTE k_neighbors=7 =====\n",
      "Mean metrics:\n",
      "accuracy       0.757814\n",
      "macro_f1       0.470183\n",
      "weighted_f1    0.743385\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                  98.3             64.7                 101.0   \n",
      "true_low_stress                    7.7             38.7                 198.3   \n",
      "true_moderate_stress              20.0             70.0                 128.0   \n",
      "true_no_stress                     3.3             15.3                  23.7   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                30.0  \n",
      "true_low_stress                 49.3  \n",
      "true_moderate_stress            76.0  \n",
      "true_no_stress                1799.3  \n",
      "===== SMOTE k_neighbors=8 =====\n",
      "Mean metrics:\n",
      "accuracy       0.762427\n",
      "macro_f1       0.481362\n",
      "weighted_f1    0.746183\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 118.0             43.7                 103.3   \n",
      "true_low_stress                    7.7             29.7                 196.7   \n",
      "true_moderate_stress              20.3             63.7                 129.3   \n",
      "true_no_stress                     4.0             14.3                  23.3   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                29.0  \n",
      "true_low_stress                 60.0  \n",
      "true_moderate_stress            80.7  \n",
      "true_no_stress                1800.0  \n",
      "===== SMOTE k_neighbors=9 =====\n",
      "Mean metrics:\n",
      "accuracy       0.768131\n",
      "macro_f1       0.480316\n",
      "weighted_f1    0.750884\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 140.0             36.0                  86.3   \n",
      "true_low_stress                    8.7             14.7                 221.3   \n",
      "true_moderate_stress              31.7             51.3                 138.0   \n",
      "true_no_stress                     6.3             12.0                  23.3   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                31.7  \n",
      "true_low_stress                 49.3  \n",
      "true_moderate_stress            73.0  \n",
      "true_no_stress                1800.0  \n",
      "===== SMOTE k_neighbors=10 =====\n",
      "Mean metrics:\n",
      "accuracy       0.753023\n",
      "macro_f1       0.455820\n",
      "weighted_f1    0.730371\n",
      "dtype: float64\n",
      "Average confusion matrix (rows=true, cols=pred):\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 101.0             71.3                  86.0   \n",
      "true_low_stress                    7.3             25.7                 198.3   \n",
      "true_moderate_stress              33.3             57.0                 124.7   \n",
      "true_no_stress                     5.3             13.0                  23.3   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                35.7  \n",
      "true_low_stress                 62.7  \n",
      "true_moderate_stress            79.0  \n",
      "true_no_stress                1800.0  \n",
      "Best SMOTE k_neighbors: 4\n",
      "Mean metrics for best k:\n",
      "accuracy       0.768609\n",
      "macro_f1       0.498926\n",
      "weighted_f1    0.752877\n",
      "dtype: float64\n",
      "Average confusion matrix for best k:\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 142.0             36.0                  77.7   \n",
      "true_low_stress                    8.3             32.3                 198.3   \n",
      "true_moderate_stress              29.7             67.0                 117.7   \n",
      "true_no_stress                     5.3             14.3                  20.0   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                38.3  \n",
      "true_low_stress                 55.0  \n",
      "true_moderate_stress            79.7  \n",
      "true_no_stress                1802.0  \n",
      "Label distribution:\n",
      "label\n",
      "no_stress          5525\n",
      "high_stress         882\n",
      "low_stress          882\n",
      "moderate_stress     882\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Grid search over SMOTE k_neighbors\n",
    "k_values = list(range(1, 11))\n",
    "results = []\n",
    "\n",
    "gkf = GroupKFold(n_splits=3 if len(np.unique(groups)) >= 3 else len(np.unique(groups)))\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"===== SMOTE k_neighbors={k} =====\")\n",
    "    metrics = []\n",
    "    cm_accum = np.zeros((len(le.classes_), len(le.classes_)), dtype=float)\n",
    "    smote = SMOTE(random_state=42, k_neighbors=k)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(gkf.split(X_scaled, y_encoded, groups)):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "        X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)\n",
    "        sample_weight = np.array([class_weight_encoded[val] for val in y_train_bal])\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=400,\n",
    "            learning_rate=0.05,\n",
    "            max_depth=4,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            n_jobs=4,\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=len(le.classes_),\n",
    "        )\n",
    "        model.fit(X_train_bal, y_train_bal, sample_weight=sample_weight)\n",
    "        preds_encoded = model.predict(X_test)\n",
    "        preds = le.inverse_transform(preds_encoded)\n",
    "        true_labels = le.inverse_transform(y_test)\n",
    "        acc = np.mean(preds == true_labels)\n",
    "        report_dict = classification_report(\n",
    "            true_labels,\n",
    "            preds,\n",
    "            labels=le.classes_,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        cm = confusion_matrix(true_labels, preds, labels=le.classes_)\n",
    "        cm_accum += cm\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"fold\": fold,\n",
    "                \"accuracy\": acc,\n",
    "                \"macro_f1\": report_dict[\"macro avg\"][\"f1-score\"],\n",
    "                \"weighted_f1\": report_dict[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    mean_metrics = metrics_df[[\"accuracy\", \"macro_f1\", \"weighted_f1\"]].mean()\n",
    "    avg_cm = cm_accum / max(1, len(metrics))\n",
    "    results.append(\n",
    "        {\n",
    "            \"k\": k,\n",
    "            \"metrics\": metrics_df,\n",
    "            \"mean_metrics\": mean_metrics,\n",
    "            \"avg_cm\": avg_cm,\n",
    "        }\n",
    "    )\n",
    "    print(\"Mean metrics:\")\n",
    "    print(mean_metrics)\n",
    "    print(\"Average confusion matrix (rows=true, cols=pred):\")\n",
    "    avg_cm_df = pd.DataFrame(\n",
    "        avg_cm,\n",
    "        index=[f\"true_{c}\" for c in le.classes_],\n",
    "        columns=[f\"pred_{c}\" for c in le.classes_],\n",
    "    )\n",
    "    print(avg_cm_df.round(1))\n",
    "\n",
    "best_result = max(results, key=lambda r: r[\"mean_metrics\"][\"macro_f1\"])\n",
    "best_k_value = best_result[\"k\"]\n",
    "best_mean_metrics = best_result[\"mean_metrics\"]\n",
    "best_avg_cm = best_result[\"avg_cm\"]\n",
    "print(\"Best SMOTE k_neighbors:\", best_k_value)\n",
    "print(\"Mean metrics for best k:\")\n",
    "print(best_mean_metrics)\n",
    "print(\"Average confusion matrix for best k:\")\n",
    "best_cm_df = pd.DataFrame(\n",
    "    best_avg_cm,\n",
    "    index=[f\"true_{c}\" for c in le.classes_],\n",
    "    columns=[f\"pred_{c}\" for c in le.classes_],\n",
    ")\n",
    "print(best_cm_df.round(1))\n",
    "print(\"Label distribution:\")\n",
    "print(pd.Series(y).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing 36 parameter combinations\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7670835971066143, 'macro_f1': 0.5181901086888114, 'weighted_f1': 0.7672945941077268}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7645809487776938, 'macro_f1': 0.5095959895511406, 'weighted_f1': 0.7621371666557503}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7700050710192753, 'macro_f1': 0.5260378529813996, 'weighted_f1': 0.7701626115756802}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7697722810024948, 'macro_f1': 0.5259233150729922, 'weighted_f1': 0.7680359824359018}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7618079576580895, 'macro_f1': 0.48980784588723775, 'weighted_f1': 0.7513501687480479}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7605943300205178, 'macro_f1': 0.48036516404567803, 'weighted_f1': 0.7469371252241873}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7585265347514861, 'macro_f1': 0.476717340735941, 'weighted_f1': 0.7427796580455049}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7445813641629401, 'macro_f1': 0.4396011953141406, 'weighted_f1': 0.7269413934295424}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7484224404555299, 'macro_f1': 0.44567219316133544, 'weighted_f1': 0.7252147887530942}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7529520327647999, 'macro_f1': 0.4565130112949542, 'weighted_f1': 0.7310338206017676}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.753909352047485, 'macro_f1': 0.45806947457735564, 'weighted_f1': 0.73149528231249}\n",
      "Params {'max_depth': 3, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.756011280838313, 'macro_f1': 0.46102582119519847, 'weighted_f1': 0.7351521073887981}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7706937758138338, 'macro_f1': 0.514414770097972, 'weighted_f1': 0.7612242603487508}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7848060295084925, 'macro_f1': 0.5507622163945475, 'weighted_f1': 0.7757138825094559}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7734642195044953, 'macro_f1': 0.5207864928888674, 'weighted_f1': 0.764998807092701}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7747890493234192, 'macro_f1': 0.5252181484742442, 'weighted_f1': 0.7669844953097824}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7700899326315906, 'macro_f1': 0.49952105888632853, 'weighted_f1': 0.7498247851853902}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7635367195057748, 'macro_f1': 0.4847250600111475, 'weighted_f1': 0.7458471437612447}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.76792999643434, 'macro_f1': 0.49136000464573754, 'weighted_f1': 0.7509359423094862}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7636238463206405, 'macro_f1': 0.4838065793448338, 'weighted_f1': 0.7472340107738732}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7491341473115503, 'macro_f1': 0.4469260671472785, 'weighted_f1': 0.7249786174066245}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7537461898852923, 'macro_f1': 0.45111548545217267, 'weighted_f1': 0.7249030275130939}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7592737912768651, 'macro_f1': 0.4693092855781977, 'weighted_f1': 0.7378626492012329}\n",
      "Params {'max_depth': 4, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7562621242039628, 'macro_f1': 0.4611056748516629, 'weighted_f1': 0.7308711017766495}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7779924337781504, 'macro_f1': 0.5183104168560114, 'weighted_f1': 0.7603186408165064}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7748405684925701, 'macro_f1': 0.519722107958679, 'weighted_f1': 0.757601626260743}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7791163871719138, 'macro_f1': 0.53096858419412, 'weighted_f1': 0.7648690638405359}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.03, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7758343646043452, 'macro_f1': 0.5196982733130947, 'weighted_f1': 0.7584687118949408}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7707187102995023, 'macro_f1': 0.5073212469009233, 'weighted_f1': 0.7491313302375296}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7725357175394517, 'macro_f1': 0.5041744207812476, 'weighted_f1': 0.7453237122769151}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7756405334448369, 'macro_f1': 0.5120452130782082, 'weighted_f1': 0.7560181339659081}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.05, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7733993436652561, 'macro_f1': 0.5067744639827868, 'weighted_f1': 0.7491302439362973}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.7549782137142707, 'macro_f1': 0.4571413716558874, 'weighted_f1': 0.7208033181160401}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7502855572119604, 'macro_f1': 0.443354405169987, 'weighted_f1': 0.7126134469040216}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.7, 'n_estimators': 400} -> {'accuracy': 0.768125395166281, 'macro_f1': 0.4887703310584031, 'weighted_f1': 0.741553798199035}\n",
      "Params {'max_depth': 5, 'learning_rate': 0.1, 'subsample': 0.85, 'colsample_bytree': 0.9, 'n_estimators': 400} -> {'accuracy': 0.7691869267012336, 'macro_f1': 0.4913732850219222, 'weighted_f1': 0.7410152780508438}\n",
      "Best XGB params: {'max_depth': 4, 'learning_rate': 0.03, 'subsample': 0.7, 'colsample_bytree': 0.9, 'n_estimators': 400}\n",
      "Metrics for best params:\n",
      "accuracy       0.784806\n",
      "macro_f1       0.550762\n",
      "weighted_f1    0.775714\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# XGBoost hyperparameter grid search (using best SMOTE k)\n",
    "if \"best_k_value\" not in globals():\n",
    "    raise RuntimeError(\"Run the SMOTE k grid search cell first.\")\n",
    "\n",
    "param_grid = []\n",
    "for max_depth in [3, 4, 5]:\n",
    "    for learning_rate in [0.03, 0.05, 0.1]:\n",
    "        for subsample in [0.7, 0.85]:\n",
    "            for colsample in [0.7, 0.9]:\n",
    "                param_grid.append(\n",
    "                    {\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"colsample_bytree\": colsample,\n",
    "                        \"n_estimators\": 400,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "print(f\"Testing {len(param_grid)} parameter combinations\")\n",
    "best_xgb_params = None\n",
    "best_param_metrics = None\n",
    "best_param_score = -np.inf\n",
    "param_results = []\n",
    "param_gkf = GroupKFold(n_splits=3 if len(np.unique(groups)) >= 3 else len(np.unique(groups)))\n",
    "smote_for_params = SMOTE(random_state=42, k_neighbors=best_k_value)\n",
    "\n",
    "for params in param_grid:\n",
    "    metrics = []\n",
    "    for fold, (train_idx, test_idx) in enumerate(param_gkf.split(X_scaled, y_encoded, groups)):\n",
    "        X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "        y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "        X_train_bal, y_train_bal = smote_for_params.fit_resample(X_train, y_train)\n",
    "        sample_weight = np.array([class_weight_encoded[val] for val in y_train_bal])\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=params[\"n_estimators\"],\n",
    "            learning_rate=params[\"learning_rate\"],\n",
    "            max_depth=params[\"max_depth\"],\n",
    "            subsample=params[\"subsample\"],\n",
    "            colsample_bytree=params[\"colsample_bytree\"],\n",
    "            n_jobs=4,\n",
    "            objective=\"multi:softprob\",\n",
    "            num_class=len(le.classes_),\n",
    "        )\n",
    "        model.fit(X_train_bal, y_train_bal, sample_weight=sample_weight)\n",
    "        preds_encoded = model.predict(X_test)\n",
    "        preds = le.inverse_transform(preds_encoded)\n",
    "        true_labels = le.inverse_transform(y_test)\n",
    "        acc = np.mean(preds == true_labels)\n",
    "        report_dict = classification_report(\n",
    "            true_labels,\n",
    "            preds,\n",
    "            labels=le.classes_,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        metrics.append(\n",
    "            {\n",
    "                \"fold\": fold,\n",
    "                \"accuracy\": acc,\n",
    "                \"macro_f1\": report_dict[\"macro avg\"][\"f1-score\"],\n",
    "                \"weighted_f1\": report_dict[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    mean_metrics = metrics_df[[\"accuracy\", \"macro_f1\", \"weighted_f1\"]].mean()\n",
    "    param_results.append({\"params\": params, \"metrics\": metrics_df, \"mean\": mean_metrics})\n",
    "    print(f\"Params {params} -> {mean_metrics.to_dict()}\")\n",
    "    if mean_metrics[\"macro_f1\"] > best_param_score:\n",
    "        best_param_score = mean_metrics[\"macro_f1\"]\n",
    "        best_xgb_params = params\n",
    "        best_param_metrics = mean_metrics\n",
    "\n",
    "print(\"Best XGB params:\", best_xgb_params)\n",
    "print(\"Metrics for best params:\")\n",
    "print(best_param_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cross-validated metrics:\n",
      "   fold  accuracy  macro_f1  weighted_f1\n",
      "0     0  0.846939  0.640164     0.848532\n",
      "1     1  0.739066  0.497972     0.748806\n",
      "2     2  0.768413  0.514151     0.729804\n",
      "Mean metrics:\n",
      "accuracy       0.784806\n",
      "macro_f1       0.550762\n",
      "weighted_f1    0.775714\n",
      "dtype: float64\n",
      "Confusion matrix averaged across folds:\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                 167.0             29.0                  75.3   \n",
      "true_low_stress                    8.3             54.0                 190.0   \n",
      "true_moderate_stress              28.7             62.0                 127.0   \n",
      "true_no_stress                     6.7             17.0                  27.7   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                22.7  \n",
      "true_low_stress                 41.7  \n",
      "true_moderate_stress            76.3  \n",
      "true_no_stress                1790.3  \n",
      "Per-phase diagnostics:\n",
      "              phase  support  accuracy  macro_f1  weighted_f1\n",
      "0  Opposite Opinion     1071  0.424837  0.308257     0.411010\n",
      "1            Stroop      393  0.277354  0.193292     0.257723\n",
      "2              TMCT     1182  0.406091  0.341637     0.455517\n",
      "3           aerobic     1546  0.971539  0.246391     0.985564\n",
      "4         anaerobic      998  0.917836  0.239289     0.957158\n",
      "5              rest     2981  0.990607  0.248820     0.995281\n",
      "Per-phase class metrics:\n",
      "               phase            label  precision    recall        f1  support\n",
      "0   Opposite Opinion      high_stress   0.789474  0.840336  0.814111    357.0\n",
      "1   Opposite Opinion       low_stress   0.029126  0.008403  0.013043    357.0\n",
      "2   Opposite Opinion  moderate_stress   0.387755  0.425770  0.405874    357.0\n",
      "3   Opposite Opinion        no_stress   0.000000  0.000000  0.000000      0.0\n",
      "4             Stroop      high_stress   0.000000  0.000000  0.000000    131.0\n",
      "5             Stroop       low_stress   0.554455  0.427481  0.482759    131.0\n",
      "6             Stroop  moderate_stress   0.226496  0.404580  0.290411    131.0\n",
      "7             Stroop        no_stress   0.000000  0.000000  0.000000      0.0\n",
      "8               TMCT      high_stress   0.952607  0.510152  0.664463    394.0\n",
      "9               TMCT       low_stress   0.445887  0.261421  0.329600    394.0\n",
      "10              TMCT  moderate_stress   0.319419  0.446701  0.372487    394.0\n",
      "11              TMCT        no_stress   0.000000  0.000000  0.000000      0.0\n",
      "12           aerobic      high_stress   0.000000  0.000000  0.000000      0.0\n",
      "13           aerobic       low_stress   0.000000  0.000000  0.000000      0.0\n",
      "14           aerobic  moderate_stress   0.000000  0.000000  0.000000      0.0\n",
      "15           aerobic        no_stress   1.000000  0.971539  0.985564   1546.0\n",
      "16         anaerobic      high_stress   0.000000  0.000000  0.000000      0.0\n",
      "17         anaerobic       low_stress   0.000000  0.000000  0.000000      0.0\n",
      "18         anaerobic  moderate_stress   0.000000  0.000000  0.000000      0.0\n",
      "19         anaerobic        no_stress   1.000000  0.917836  0.957158    998.0\n",
      "20              rest      high_stress   0.000000  0.000000  0.000000      0.0\n",
      "21              rest       low_stress   0.000000  0.000000  0.000000      0.0\n",
      "22              rest  moderate_stress   0.000000  0.000000  0.000000      0.0\n",
      "23              rest        no_stress   1.000000  0.990607  0.995281   2981.0\n"
     ]
    }
   ],
   "source": [
    "# Train final model using best settings\n",
    "if \"best_k_value\" not in globals():\n",
    "    raise RuntimeError(\"Run the SMOTE k grid search cell first.\")\n",
    "\n",
    "final_params = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 4,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "}\n",
    "if \"best_xgb_params\" in globals() and best_xgb_params:\n",
    "    final_params.update(best_xgb_params)\n",
    "\n",
    "final_metrics = []\n",
    "final_cm = np.zeros((len(le.classes_), len(le.classes_)), dtype=float)\n",
    "phase_records = []\n",
    "final_smote = SMOTE(random_state=42, k_neighbors=best_k_value)\n",
    "final_gkf = GroupKFold(n_splits=3 if len(np.unique(groups)) >= 3 else len(np.unique(groups)))\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(final_gkf.split(X_scaled, y_encoded, groups)):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "    X_train_bal, y_train_bal = final_smote.fit_resample(X_train, y_train)\n",
    "    sample_weight = np.array([class_weight_encoded[val] for val in y_train_bal])\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=final_params[\"n_estimators\"],\n",
    "        learning_rate=final_params[\"learning_rate\"],\n",
    "        max_depth=final_params[\"max_depth\"],\n",
    "        subsample=final_params[\"subsample\"],\n",
    "        colsample_bytree=final_params[\"colsample_bytree\"],\n",
    "        n_jobs=4,\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=len(le.classes_),\n",
    "    )\n",
    "    model.fit(X_train_bal, y_train_bal, sample_weight=sample_weight)\n",
    "    preds_encoded = model.predict(X_test)\n",
    "    preds = le.inverse_transform(preds_encoded)\n",
    "    true_labels = le.inverse_transform(y_test)\n",
    "    acc = np.mean(preds == true_labels)\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        preds,\n",
    "        labels=le.classes_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    cm = confusion_matrix(true_labels, preds, labels=le.classes_)\n",
    "    final_cm += cm\n",
    "    phase_records.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"phase\": phases[test_idx],\n",
    "                \"true\": true_labels,\n",
    "                \"pred\": preds,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    final_metrics.append(\n",
    "        {\n",
    "            \"fold\": fold,\n",
    "            \"accuracy\": acc,\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "final_metrics_df = pd.DataFrame(final_metrics)\n",
    "print(\"Final cross-validated metrics:\")\n",
    "print(final_metrics_df)\n",
    "print(\"Mean metrics:\")\n",
    "print(final_metrics_df[[\"accuracy\", \"macro_f1\", \"weighted_f1\"]].mean())\n",
    "\n",
    "print(\"Confusion matrix averaged across folds:\")\n",
    "final_cm_df = pd.DataFrame(\n",
    "    final_cm / len(final_metrics),\n",
    "    index=[f\"true_{c}\" for c in le.classes_],\n",
    "    columns=[f\"pred_{c}\" for c in le.classes_],\n",
    ")\n",
    "print(final_cm_df.round(1))\n",
    "\n",
    "if phase_records:\n",
    "    phase_df = pd.concat(phase_records, ignore_index=True)\n",
    "    phase_summaries = []\n",
    "    per_phase_class_rows = []\n",
    "    for phase_name, group_df in phase_df.groupby(\"phase\"):\n",
    "        phase_report = classification_report(\n",
    "            group_df[\"true\"],\n",
    "            group_df[\"pred\"],\n",
    "            labels=le.classes_,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        phase_summaries.append(\n",
    "            {\n",
    "                \"phase\": phase_name,\n",
    "                \"support\": len(group_df),\n",
    "                \"accuracy\": np.mean(group_df[\"true\"] == group_df[\"pred\"]),\n",
    "                \"macro_f1\": phase_report[\"macro avg\"][\"f1-score\"],\n",
    "                \"weighted_f1\": phase_report[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "        for label in le.classes_:\n",
    "            if label not in phase_report:\n",
    "                continue\n",
    "            per_phase_class_rows.append(\n",
    "                {\n",
    "                    \"phase\": phase_name,\n",
    "                    \"label\": label,\n",
    "                    \"precision\": phase_report[label].get(\"precision\", 0.0),\n",
    "                    \"recall\": phase_report[label].get(\"recall\", 0.0),\n",
    "                    \"f1\": phase_report[label].get(\"f1-score\", 0.0),\n",
    "                    \"support\": phase_report[label].get(\"support\", 0),\n",
    "                }\n",
    "            )\n",
    "    phase_summary_df = pd.DataFrame(phase_summaries).sort_values(\"phase\")\n",
    "    print(\"Per-phase diagnostics:\")\n",
    "    print(phase_summary_df)\n",
    "    if per_phase_class_rows:\n",
    "        phase_class_df = pd.DataFrame(per_phase_class_rows)\n",
    "        print(\"Per-phase class metrics:\")\n",
    "        print(phase_class_df.sort_values([\"phase\", \"label\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
