{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Stress Level Prediction Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a comprehensive data preprocessing and feature engineering pipeline for stress level prediction using multimodal wearable sensor data (Empatica E4).\n",
    "\n",
    "**Key Improvements:**\n",
    "1. **Signal Preprocessing**: Bandpass filtering, motion artifact removal, quality assessment\n",
    "2. **Subject-Specific Normalization**: Z-score using rest phase baseline (handles 10x EDA variation)\n",
    "3. **EDA Decomposition**: Tonic/phasic separation + SCR features (THE stress biomarker)\n",
    "4. **Nonlinear HRV**: Sample Entropy, Approximate Entropy, DFA (complexity measures)\n",
    "5. **Cross-Modal Synchrony**: EDA-HR correlation and coherence (multi-system coordination)\n",
    "6. **Demographic Features**: Gender, age, height, weight, BMI, physical activity\n",
    "\n",
    "**Expected Performance:**\n",
    "- Current: 91% accuracy, 48% macro F1\n",
    "- Target: 94-96% accuracy, 75-86% macro F1\n",
    "\n",
    "**Runtime:** ~15-20 minutes for full pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 0: Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Signal processing\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, filtfilt, find_peaks, coherence\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration loaded\n",
      "  Base directory: /home/moh/home/Data_mining/Stress-Level-Prediction\n",
      "  Window: 60s, Step: 30s\n",
      "  Target sampling rate: 4.0 Hz\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(\"/home/moh/home/Data_mining/Stress-Level-Prediction\")\n",
    "DATASETS_DIR = BASE_DIR / \"Datasets\"\n",
    "OUTPUT_DIR = BASE_DIR\n",
    "\n",
    "# File paths\n",
    "SUBJECT_INFO_PATH = BASE_DIR / \"subject-info.csv\"\n",
    "STRESS_LABELS_V1 = BASE_DIR / \"Stress_Level_v1.csv\"\n",
    "STRESS_LABELS_V2 = BASE_DIR / \"Stress_Level_v2.csv\"\n",
    "\n",
    "# Window parameters\n",
    "WINDOW_SIZE = 60  # seconds\n",
    "STEP_SIZE = 30    # seconds (50% overlap)\n",
    "TARGET_FS = 4.0   # Hz (resampling frequency)\n",
    "\n",
    "# Signal quality thresholds\n",
    "MIN_SIGNAL_QUALITY = 0.3  # Minimum quality score to include window\n",
    "MAX_MOTION_RATIO = 0.5    # Maximum motion artifact ratio\n",
    "MIN_IBI_COUNT = 5         # Minimum IBI beats per window\n",
    "\n",
    "# Data constraints (from data_constraints.txt)\n",
    "DATA_CONSTRAINTS = {\n",
    "    'S02': {'duplicates': {'ACC': 49545, 'BVP': 99091, 'EDA_TEMP': 6195}},\n",
    "    'f07': {'missing_sensors': ['BVP', 'TEMP', 'HR', 'IBI']},\n",
    "    'f14': {'split_files': ['f14_a', 'f14_b']},\n",
    "    'S01': {'missing_ibi': ['ANAEROBIC']},\n",
    "    'S12': {'skip': ['AEROBIC']}\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration loaded\")\n",
    "print(f\"  Base directory: {BASE_DIR}\")\n",
    "print(f\"  Window: {WINDOW_SIZE}s, Step: {STEP_SIZE}s\")\n",
    "print(f\"  Target sampling rate: {TARGET_FS} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Signal Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Signal preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def bandpass_filter_signal(data: np.ndarray, lowcut: float, highcut: float, \n",
    "                          fs: float, order: int = 3) -> np.ndarray:\n",
    "    \"\"\"Apply Butterworth bandpass filter to signal.\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    \n",
    "    if low <= 0:\n",
    "        low = 0.001\n",
    "    if high >= 1:\n",
    "        high = 0.999\n",
    "    \n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    filtered = filtfilt(b, a, data)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def lowpass_filter_signal(data: np.ndarray, cutoff: float, fs: float, \n",
    "                         order: int = 3) -> np.ndarray:\n",
    "    \"\"\"Apply Butterworth lowpass filter to signal.\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    if normal_cutoff >= 1:\n",
    "        normal_cutoff = 0.999\n",
    "    \n",
    "    b, a = butter(order, normal_cutoff, btype='low')\n",
    "    filtered = filtfilt(b, a, data)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def highpass_filter_signal(data: np.ndarray, cutoff: float, fs: float, \n",
    "                          order: int = 3) -> np.ndarray:\n",
    "    \"\"\"Apply Butterworth highpass filter to signal.\"\"\"\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    if normal_cutoff <= 0:\n",
    "        normal_cutoff = 0.001\n",
    "    \n",
    "    b, a = butter(order, normal_cutoff, btype='high')\n",
    "    filtered = filtfilt(b, a, data)\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def preprocess_signals(eda: np.ndarray, temp: np.ndarray, \n",
    "                      acc: np.ndarray, bvp: Optional[np.ndarray], \n",
    "                      fs: float = 4.0) -> Tuple[np.ndarray, ...]:\n",
    "    \"\"\"Apply bandpass filtering to all signals.\"\"\"\n",
    "    # EDA: Bandpass 0.01-5 Hz\n",
    "    eda_clean = bandpass_filter_signal(eda, 0.01, 5.0, fs)\n",
    "    \n",
    "    # TEMP: Lowpass 0.5 Hz\n",
    "    temp_clean = lowpass_filter_signal(temp, 0.5, fs)\n",
    "    \n",
    "    # ACC: Lowpass 15 Hz\n",
    "    acc_clean = lowpass_filter_signal(acc, 15.0, fs)\n",
    "    \n",
    "    # BVP: Bandpass 0.5-8 Hz (if available)\n",
    "    if bvp is not None and len(bvp) > 0:\n",
    "        bvp_clean = bandpass_filter_signal(bvp, 0.5, 8.0, fs)\n",
    "    else:\n",
    "        bvp_clean = bvp\n",
    "    \n",
    "    return eda_clean, temp_clean, acc_clean, bvp_clean\n",
    "\n",
    "\n",
    "def detect_motion_artifacts(acc_mag: np.ndarray, eda: np.ndarray, \n",
    "                           threshold: float = 2.0) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Detect and remove motion artifacts from EDA.\"\"\"\n",
    "    acc_mean = np.mean(acc_mag)\n",
    "    acc_std = np.std(acc_mag)\n",
    "    \n",
    "    # Detect high motion periods\n",
    "    motion_mask = acc_mag > (acc_mean + threshold * acc_std)\n",
    "    \n",
    "    # Mark EDA as invalid during high motion\n",
    "    eda_clean = eda.copy()\n",
    "    eda_clean[motion_mask] = np.nan\n",
    "    \n",
    "    # Interpolate over motion artifacts\n",
    "    valid_idx = ~np.isnan(eda_clean)\n",
    "    if valid_idx.sum() > 2:\n",
    "        eda_clean = np.interp(\n",
    "            np.arange(len(eda)),\n",
    "            np.where(valid_idx)[0],\n",
    "            eda_clean[valid_idx]\n",
    "        )\n",
    "    else:\n",
    "        eda_clean = eda\n",
    "    \n",
    "    motion_ratio = motion_mask.sum() / len(motion_mask)\n",
    "    return eda_clean, motion_ratio\n",
    "\n",
    "\n",
    "def assess_signal_quality(data: np.ndarray, signal_name: str = 'EDA') -> float:\n",
    "    \"\"\"Assess signal quality (0-1 score).\"\"\"\n",
    "    quality_score = 1.0\n",
    "    \n",
    "    if np.std(data) < 0.01:\n",
    "        quality_score *= 0.3\n",
    "    \n",
    "    nan_ratio = np.isnan(data).sum() / len(data)\n",
    "    if nan_ratio > 0.3:\n",
    "        quality_score *= 0.5\n",
    "    \n",
    "    if signal_name == 'EDA':\n",
    "        if np.any(data < 0) or np.any(data > 60):\n",
    "            quality_score *= 0.4\n",
    "    elif signal_name == 'TEMP':\n",
    "        if np.any(data < 20) or np.any(data > 45):\n",
    "            quality_score *= 0.4\n",
    "    elif signal_name == 'HR':\n",
    "        valid_data = data[~np.isnan(data)]\n",
    "        if len(valid_data) > 0:\n",
    "            if np.any(valid_data < 30) or np.any(valid_data > 200):\n",
    "                quality_score *= 0.4\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "\n",
    "print(\"✓ Signal preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Subject-Specific Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Subject-specific normalization function defined\n"
     ]
    }
   ],
   "source": [
    "def normalize_by_subject_baseline(df: pd.DataFrame, feature_cols: List[str], \n",
    "                                 subject_col: str = 'subject', \n",
    "                                 phase_col: str = 'phase') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Z-score normalize features using each subject's rest phase as baseline.\n",
    "    \n",
    "    This accounts for individual differences:\n",
    "    - Baseline EDA: 0.5-5 µS (10x variation)\n",
    "    - Baseline HR: 50-90 bpm (40 bpm variation)\n",
    "    \"\"\"\n",
    "    normalized_df = df.copy()\n",
    "    \n",
    "    for subject in df[subject_col].unique():\n",
    "        subject_mask = df[subject_col] == subject\n",
    "        \n",
    "        # Use rest phase as individual baseline\n",
    "        rest_mask = subject_mask & (df[phase_col] == 'rest')\n",
    "        \n",
    "        if rest_mask.sum() > 0:\n",
    "            baseline_mean = df.loc[rest_mask, feature_cols].mean()\n",
    "            baseline_std = df.loc[rest_mask, feature_cols].std().replace(0, 1)\n",
    "        else:\n",
    "            baseline_mean = df.loc[subject_mask, feature_cols].mean()\n",
    "            baseline_std = df.loc[subject_mask, feature_cols].std().replace(0, 1)\n",
    "        \n",
    "        # Z-score normalization\n",
    "        normalized_df.loc[subject_mask, feature_cols] = (\n",
    "            (df.loc[subject_mask, feature_cols] - baseline_mean) / baseline_std\n",
    "        )\n",
    "    \n",
    "    return normalized_df\n",
    "\n",
    "\n",
    "print(\"✓ Subject-specific normalization function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: EDA Decomposition & SCR Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ EDA decomposition and SCR extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "def decompose_eda(eda_signal: np.ndarray, fs: float = 4.0) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Decompose EDA into tonic (SCL) and phasic (SCR) components.\"\"\"\n",
    "    # Tonic component: Low-pass < 0.05 Hz\n",
    "    tonic = lowpass_filter_signal(eda_signal, 0.05, fs, order=3)\n",
    "    \n",
    "    # Phasic component: High-pass > 0.05 Hz\n",
    "    phasic = highpass_filter_signal(eda_signal, 0.05, fs, order=3)\n",
    "    \n",
    "    return tonic, phasic\n",
    "\n",
    "\n",
    "def extract_scr_features(phasic: np.ndarray, fs: float = 4.0) -> Dict[str, float]:\n",
    "    \"\"\"Extract Skin Conductance Response (SCR) features.\"\"\"\n",
    "    scr_features = {}\n",
    "    \n",
    "    # Detect SCR peaks\n",
    "    peaks, properties = find_peaks(\n",
    "        phasic,\n",
    "        height=0.01,\n",
    "        distance=int(fs * 1.0),\n",
    "        prominence=0.01\n",
    "    )\n",
    "    \n",
    "    duration_min = len(phasic) / (fs * 60)\n",
    "    \n",
    "    scr_features['scr_count'] = len(peaks)\n",
    "    scr_features['scr_rate'] = len(peaks) / duration_min if duration_min > 0 else 0.0\n",
    "    \n",
    "    if len(peaks) > 0:\n",
    "        amplitudes = properties['peak_heights']\n",
    "        scr_features['scr_amp_mean'] = float(np.mean(amplitudes))\n",
    "        scr_features['scr_amp_max'] = float(np.max(amplitudes))\n",
    "        scr_features['scr_amp_std'] = float(np.std(amplitudes))\n",
    "        scr_features['scr_amp_sum'] = float(np.sum(amplitudes))\n",
    "    else:\n",
    "        scr_features['scr_amp_mean'] = 0.0\n",
    "        scr_features['scr_amp_max'] = 0.0\n",
    "        scr_features['scr_amp_std'] = 0.0\n",
    "        scr_features['scr_amp_sum'] = 0.0\n",
    "    \n",
    "    if 'widths' in properties and len(properties['widths']) > 0:\n",
    "        rise_times = properties['widths'] / fs\n",
    "        scr_features['scr_rise_time_mean'] = float(np.mean(rise_times))\n",
    "    else:\n",
    "        scr_features['scr_rise_time_mean'] = 0.0\n",
    "    \n",
    "    return scr_features\n",
    "\n",
    "\n",
    "print(\"✓ EDA decomposition and SCR extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Nonlinear HRV Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Nonlinear HRV functions defined\n"
     ]
    }
   ],
   "source": [
    "def validate_ibi(ibi: np.ndarray, min_count: int = 5, \n",
    "                min_rr: float = 0.3, max_rr: float = 2.0) -> Optional[np.ndarray]:\n",
    "    \"\"\"Validate and clean IBI data.\"\"\"\n",
    "    if ibi is None or len(ibi) == 0:\n",
    "        return None\n",
    "    \n",
    "    valid_mask = (ibi >= min_rr) & (ibi <= max_rr) & ~np.isnan(ibi)\n",
    "    cleaned_ibi = ibi[valid_mask]\n",
    "    \n",
    "    if len(cleaned_ibi) < min_count:\n",
    "        return None\n",
    "    \n",
    "    return cleaned_ibi\n",
    "\n",
    "\n",
    "def sample_entropy(data: np.ndarray, m: int = 2, r: float = 0.2) -> float:\n",
    "    \"\"\"Calculate Sample Entropy (SampEn).\"\"\"\n",
    "    N = len(data)\n",
    "    if N < m + 10:\n",
    "        return np.nan\n",
    "    \n",
    "    r = r * np.std(data)\n",
    "    \n",
    "    def _maxdist(x_i, x_j):\n",
    "        return max([abs(ua - va) for ua, va in zip(x_i, x_j)])\n",
    "    \n",
    "    def _phi(m):\n",
    "        patterns = [[data[j] for j in range(i, i + m)] for i in range(N - m + 1)]\n",
    "        C = []\n",
    "        for i in range(len(patterns)):\n",
    "            count = sum(1 for j in range(len(patterns))\n",
    "                       if i != j and _maxdist(patterns[i], patterns[j]) <= r)\n",
    "            C.append(count)\n",
    "        phi = sum(C) / (N - m + 1) / (N - m) if (N - m) > 0 else 0\n",
    "        return phi\n",
    "    \n",
    "    phi_m = _phi(m)\n",
    "    phi_m1 = _phi(m + 1)\n",
    "    \n",
    "    if phi_m > 0 and phi_m1 > 0:\n",
    "        return -np.log(phi_m1 / phi_m)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def approximate_entropy(data: np.ndarray, m: int = 2, r: float = 0.2) -> float:\n",
    "    \"\"\"Calculate Approximate Entropy (ApEn).\"\"\"\n",
    "    N = len(data)\n",
    "    if N < m + 10:\n",
    "        return np.nan\n",
    "    \n",
    "    r = r * np.std(data)\n",
    "    \n",
    "    def _phi(m):\n",
    "        patterns = [[data[j] for j in range(i, i + m)] for i in range(N - m + 1)]\n",
    "        C = []\n",
    "        for i in range(len(patterns)):\n",
    "            count = sum(1 for j in range(len(patterns))\n",
    "                       if np.max(np.abs(np.array(patterns[i]) - np.array(patterns[j]))) <= r)\n",
    "            C.append(count / (N - m + 1))\n",
    "        if all(c > 0 for c in C):\n",
    "            return sum(np.log(C)) / (N - m + 1)\n",
    "        return np.nan\n",
    "    \n",
    "    phi_m = _phi(m)\n",
    "    phi_m1 = _phi(m + 1)\n",
    "    \n",
    "    if not np.isnan(phi_m) and not np.isnan(phi_m1):\n",
    "        return abs(phi_m - phi_m1)\n",
    "    return np.nan\n",
    "\n",
    "\n",
    "def detrended_fluctuation_analysis(data: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"Calculate DFA alpha coefficients.\"\"\"\n",
    "    N = len(data)\n",
    "    if N < 16:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    y = np.cumsum(data - np.mean(data))\n",
    "    scales = np.unique(np.logspace(0.5, np.log10(N//4), 20).astype(int))\n",
    "    \n",
    "    F = []\n",
    "    for scale in scales:\n",
    "        n_segments = N // scale\n",
    "        F_scale = 0\n",
    "        \n",
    "        for i in range(n_segments):\n",
    "            segment = y[i*scale:(i+1)*scale]\n",
    "            t = np.arange(len(segment))\n",
    "            coeffs = np.polyfit(t, segment, 1)\n",
    "            trend = np.polyval(coeffs, t)\n",
    "            F_scale += np.mean((segment - trend) ** 2)\n",
    "        \n",
    "        if n_segments > 0:\n",
    "            F.append(np.sqrt(F_scale / n_segments))\n",
    "        else:\n",
    "            F.append(np.nan)\n",
    "    \n",
    "    log_scales = np.log(scales)\n",
    "    log_F = np.log(F)\n",
    "    \n",
    "    valid_mask = ~np.isnan(log_F)\n",
    "    if valid_mask.sum() < 2:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    log_scales = log_scales[valid_mask]\n",
    "    log_F = log_F[valid_mask]\n",
    "    scales = scales[valid_mask]\n",
    "    \n",
    "    mask1 = (scales >= 4) & (scales <= 11)\n",
    "    if np.sum(mask1) > 1:\n",
    "        alpha1 = np.polyfit(log_scales[mask1], log_F[mask1], 1)[0]\n",
    "    else:\n",
    "        alpha1 = np.nan\n",
    "    \n",
    "    mask2 = scales > 11\n",
    "    if np.sum(mask2) > 1:\n",
    "        alpha2 = np.polyfit(log_scales[mask2], log_F[mask2], 1)[0]\n",
    "    else:\n",
    "        alpha2 = np.nan\n",
    "    \n",
    "    return alpha1, alpha2\n",
    "\n",
    "\n",
    "print(\"✓ Nonlinear HRV functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Cross-Modal Synchrony Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Cross-modal synchrony functions defined\n"
     ]
    }
   ],
   "source": [
    "def cross_modal_features(eda: np.ndarray, hr: np.ndarray, \n",
    "                        temp: np.ndarray, fs: float = 4.0) -> Dict[str, float]:\n",
    "    \"\"\"Extract cross-modal synchrony features.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    min_len = min(len(eda), len(hr), len(temp))\n",
    "    eda = eda[:min_len]\n",
    "    hr = hr[:min_len]\n",
    "    temp = temp[:min_len]\n",
    "    \n",
    "    if min_len < 10:\n",
    "        return {\n",
    "            'eda_hr_xcorr_max': np.nan,\n",
    "            'eda_temp_xcorr_max': np.nan,\n",
    "            'hr_temp_xcorr_max': np.nan,\n",
    "            'eda_hr_coherence_lf': np.nan,\n",
    "            'eda_hr_coherence_hf': np.nan,\n",
    "        }\n",
    "    \n",
    "    # Normalize signals\n",
    "    eda_norm = (eda - np.mean(eda)) / (np.std(eda) + 1e-6)\n",
    "    hr_norm = (hr - np.mean(hr)) / (np.std(hr) + 1e-6)\n",
    "    temp_norm = (temp - np.mean(temp)) / (np.std(temp) + 1e-6)\n",
    "    \n",
    "    # Cross-correlation\n",
    "    xcorr_eda_hr = np.correlate(eda_norm, hr_norm, mode='same')\n",
    "    xcorr_eda_temp = np.correlate(eda_norm, temp_norm, mode='same')\n",
    "    xcorr_hr_temp = np.correlate(hr_norm, temp_norm, mode='same')\n",
    "    \n",
    "    features['eda_hr_xcorr_max'] = float(np.max(np.abs(xcorr_eda_hr)))\n",
    "    features['eda_temp_xcorr_max'] = float(np.max(np.abs(xcorr_eda_temp)))\n",
    "    features['hr_temp_xcorr_max'] = float(np.max(np.abs(xcorr_hr_temp)))\n",
    "    \n",
    "    # Coherence\n",
    "    if min_len >= 64:\n",
    "        f, Cxy_eda_hr = coherence(eda, hr, fs=fs, nperseg=min(64, min_len))\n",
    "        \n",
    "        lf_mask = (f >= 0.04) & (f <= 0.15)\n",
    "        features['eda_hr_coherence_lf'] = (\n",
    "            float(np.mean(Cxy_eda_hr[lf_mask])) if lf_mask.sum() > 0 else np.nan\n",
    "        )\n",
    "        \n",
    "        hf_mask = (f >= 0.15) & (f <= 0.4)\n",
    "        features['eda_hr_coherence_hf'] = (\n",
    "            float(np.mean(Cxy_eda_hr[hf_mask])) if hf_mask.sum() > 0 else np.nan\n",
    "        )\n",
    "    else:\n",
    "        features['eda_hr_coherence_lf'] = np.nan\n",
    "        features['eda_hr_coherence_hf'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "print(\"✓ Cross-modal synchrony functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Demographic Feature Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Demographic data loading function defined\n"
     ]
    }
   ],
   "source": [
    "def load_demographic_data(subject_info_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Load and process demographic data from subject-info.csv.\"\"\"\n",
    "    df = pd.read_csv(subject_info_path)\n",
    "    \n",
    "    demographics = pd.DataFrame()\n",
    "    demographics['subject'] = df['Info']\n",
    "    demographics['gender'] = df['Gender'].map({'M': 1, 'F': 0})\n",
    "    demographics['age'] = pd.to_numeric(df['Age'], errors='coerce')\n",
    "    demographics['height'] = pd.to_numeric(df['Height (cm)'], errors='coerce')\n",
    "    demographics['weight'] = pd.to_numeric(df['Weight (kg)'], errors='coerce')\n",
    "    demographics['bmi'] = demographics['weight'] / ((demographics['height'] / 100) ** 2)\n",
    "    demographics['physical_activity'] = df['Does physical activity regularly?'].map({'Yes': 1, 'No': 0})\n",
    "    demographics['protocol_version'] = df['Protocol'].map({'V1': 1, 'V2': 0})\n",
    "    \n",
    "    # Fill missing values\n",
    "    for col in ['age', 'height', 'weight', 'bmi']:\n",
    "        demographics[col] = demographics[col].fillna(demographics[col].median())\n",
    "    \n",
    "    demographics['gender'] = demographics['gender'].fillna(0)\n",
    "    demographics['physical_activity'] = demographics['physical_activity'].fillna(0)\n",
    "    demographics['protocol_version'] = demographics['protocol_version'].fillna(0)\n",
    "    \n",
    "    return demographics\n",
    "\n",
    "\n",
    "print(\"✓ Demographic data loading function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "def hampel_filter(data: np.ndarray, window_size: int = 5, n_sigmas: float = 3) -> np.ndarray:\n",
    "    \"\"\"Apply Hampel filter to remove outliers.\"\"\"\n",
    "    filtered = data.copy()\n",
    "    k = 1.4826\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(data), i + window_size + 1)\n",
    "        window = data[start:end]\n",
    "        \n",
    "        median = np.median(window)\n",
    "        mad = k * np.median(np.abs(window - median))\n",
    "        \n",
    "        if np.abs(data[i] - median) > n_sigmas * mad:\n",
    "            filtered[i] = median\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "\n",
    "def linear_slope(data: np.ndarray, fs: float) -> float:\n",
    "    \"\"\"Calculate linear slope of signal.\"\"\"\n",
    "    if len(data) < 2:\n",
    "        return 0.0\n",
    "    t = np.arange(len(data)) / fs\n",
    "    slope = np.polyfit(t, data, 1)[0]\n",
    "    return float(slope)\n",
    "\n",
    "\n",
    "def extract_eda_features(eda: np.ndarray, fs: float = 4.0) -> Dict[str, float]:\n",
    "    \"\"\"Extract comprehensive EDA features.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    clean = hampel_filter(eda)\n",
    "    tonic, phasic = decompose_eda(clean, fs)\n",
    "    scr_feats = extract_scr_features(phasic, fs)\n",
    "    \n",
    "    # Tonic features\n",
    "    features['eda_scl_mean'] = float(np.mean(tonic))\n",
    "    features['eda_scl_std'] = float(np.std(tonic))\n",
    "    features['eda_scl_slope'] = linear_slope(tonic, fs)\n",
    "    features['eda_scl_range'] = float(np.max(tonic) - np.min(tonic))\n",
    "    \n",
    "    # Phasic features\n",
    "    features['eda_phasic_mean'] = float(np.mean(phasic))\n",
    "    features['eda_phasic_std'] = float(np.std(phasic))\n",
    "    features['eda_phasic_energy'] = float(np.sum(phasic ** 2))\n",
    "    \n",
    "    # SCR features\n",
    "    features.update(scr_feats)\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['eda_mean'] = float(np.mean(clean))\n",
    "    features['eda_std'] = float(np.std(clean))\n",
    "    features['eda_min'] = float(np.min(clean))\n",
    "    features['eda_max'] = float(np.max(clean))\n",
    "    features['eda_range'] = float(np.max(clean) - np.min(clean))\n",
    "    features['eda_skew'] = float(skew(clean))\n",
    "    features['eda_kurt'] = float(kurtosis(clean))\n",
    "    features['eda_slope'] = linear_slope(clean, fs)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_hrv_features(ibi: Optional[np.ndarray], window_duration: float = 60.0) -> Dict[str, float]:\n",
    "    \"\"\"Extract comprehensive HRV features.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    ibi_clean = validate_ibi(ibi, min_count=MIN_IBI_COUNT)\n",
    "    \n",
    "    if ibi_clean is None or len(ibi_clean) < MIN_IBI_COUNT:\n",
    "        feature_names = [\n",
    "            'hrv_mean_rr', 'hrv_std_rr', 'hrv_rmssd', 'hrv_sdnn',\n",
    "            'hrv_pnn50', 'hrv_mean_hr', 'hrv_std_hr',\n",
    "            'hrv_lf', 'hrv_hf', 'hrv_lf_hf_ratio',\n",
    "            'hrv_sampen', 'hrv_apen', 'hrv_dfa_alpha1', 'hrv_dfa_alpha2'\n",
    "        ]\n",
    "        return {name: np.nan for name in feature_names}\n",
    "    \n",
    "    rr = ibi_clean * 1000\n",
    "    \n",
    "    # Time-domain\n",
    "    features['hrv_mean_rr'] = float(np.mean(rr))\n",
    "    features['hrv_std_rr'] = float(np.std(rr))\n",
    "    \n",
    "    diff_rr = np.diff(rr)\n",
    "    features['hrv_rmssd'] = float(np.sqrt(np.mean(diff_rr ** 2)))\n",
    "    features['hrv_sdnn'] = float(np.std(rr))\n",
    "    \n",
    "    nn50 = np.sum(np.abs(diff_rr) > 50)\n",
    "    features['hrv_pnn50'] = float(nn50 / len(diff_rr) * 100) if len(diff_rr) > 0 else 0.0\n",
    "    \n",
    "    hr = 60000 / rr\n",
    "    features['hrv_mean_hr'] = float(np.mean(hr))\n",
    "    features['hrv_std_hr'] = float(np.std(hr))\n",
    "    \n",
    "    # Frequency-domain\n",
    "    if len(rr) >= 10:\n",
    "        t_rr = np.cumsum(ibi_clean)\n",
    "        t_uniform = np.arange(0, t_rr[-1], 0.25)\n",
    "        rr_interp = np.interp(t_uniform, t_rr, rr)\n",
    "        \n",
    "        freqs, psd = signal.welch(rr_interp, fs=4.0, nperseg=min(256, len(rr_interp)))\n",
    "        \n",
    "        lf_mask = (freqs >= 0.04) & (freqs <= 0.15)\n",
    "        features['hrv_lf'] = float(np.trapz(psd[lf_mask], freqs[lf_mask])) if lf_mask.sum() > 0 else 0.0\n",
    "        \n",
    "        hf_mask = (freqs >= 0.15) & (freqs <= 0.4)\n",
    "        features['hrv_hf'] = float(np.trapz(psd[hf_mask], freqs[hf_mask])) if hf_mask.sum() > 0 else 0.0\n",
    "        \n",
    "        features['hrv_lf_hf_ratio'] = (\n",
    "            float(features['hrv_lf'] / features['hrv_hf']) if features['hrv_hf'] > 0 else 0.0\n",
    "        )\n",
    "    else:\n",
    "        features['hrv_lf'] = np.nan\n",
    "        features['hrv_hf'] = np.nan\n",
    "        features['hrv_lf_hf_ratio'] = np.nan\n",
    "    \n",
    "    # Nonlinear\n",
    "    if len(rr) >= 10:\n",
    "        features['hrv_sampen'] = sample_entropy(rr, m=2, r=0.2)\n",
    "        features['hrv_apen'] = approximate_entropy(rr, m=2, r=0.2)\n",
    "        features['hrv_dfa_alpha1'], features['hrv_dfa_alpha2'] = detrended_fluctuation_analysis(rr)\n",
    "    else:\n",
    "        features['hrv_sampen'] = np.nan\n",
    "        features['hrv_apen'] = np.nan\n",
    "        features['hrv_dfa_alpha1'] = np.nan\n",
    "        features['hrv_dfa_alpha2'] = np.nan\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def extract_temp_features(temp: np.ndarray, fs: float = 4.0) -> Dict[str, float]:\n",
    "    \"\"\"Extract temperature features.\"\"\"\n",
    "    return {\n",
    "        'temp_mean': float(np.mean(temp)),\n",
    "        'temp_std': float(np.std(temp)),\n",
    "        'temp_min': float(np.min(temp)),\n",
    "        'temp_max': float(np.max(temp)),\n",
    "        'temp_range': float(np.max(temp) - np.min(temp)),\n",
    "        'temp_slope': linear_slope(temp, fs)\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_acc_features(acc: np.ndarray, fs: float = 4.0) -> Dict[str, float]:\n",
    "    \"\"\"Extract accelerometer features.\"\"\"\n",
    "    acc_mag = np.linalg.norm(acc, axis=1) if acc.ndim > 1 else np.abs(acc)\n",
    "    \n",
    "    return {\n",
    "        'acc_mean': float(np.mean(acc_mag)),\n",
    "        'acc_std': float(np.std(acc_mag)),\n",
    "        'acc_min': float(np.min(acc_mag)),\n",
    "        'acc_max': float(np.max(acc_mag)),\n",
    "        'acc_range': float(np.max(acc_mag) - np.min(acc_mag)),\n",
    "        'acc_energy': float(np.sum(acc_mag ** 2))\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_hr_features(hr: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Extract heart rate features.\"\"\"\n",
    "    valid_hr = hr[~np.isnan(hr)]\n",
    "    \n",
    "    if len(valid_hr) > 0:\n",
    "        return {\n",
    "            'hr_mean': float(np.mean(valid_hr)),\n",
    "            'hr_std': float(np.std(valid_hr)),\n",
    "            'hr_min': float(np.min(valid_hr)),\n",
    "            'hr_max': float(np.max(valid_hr)),\n",
    "            'hr_range': float(np.max(valid_hr) - np.min(valid_hr))\n",
    "        }\n",
    "    else:\n",
    "        return {k: np.nan for k in ['hr_mean', 'hr_std', 'hr_min', 'hr_max', 'hr_range']}\n",
    "\n",
    "\n",
    "print(\"✓ Feature extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 8: Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def load_sensor_file(file_path: Path) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Load a sensor file from Empatica E4 format.\"\"\"\n",
    "    if not file_path.exists():\n",
    "        return None, None\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    if len(lines) < 3:\n",
    "        return None, None\n",
    "    \n",
    "    fs = float(lines[1].strip())\n",
    "    data = np.array([float(line.strip()) for line in lines[2:]])\n",
    "    \n",
    "    return data, fs\n",
    "\n",
    "\n",
    "def load_ibi_file(file_path: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"Load IBI file (inter-beat intervals).\"\"\"\n",
    "    if not file_path.exists():\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path, names=['timestamp', 'ibi'])\n",
    "        ibi = df['ibi'].values\n",
    "        return ibi if len(ibi) > 0 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def resample_signal(data: np.ndarray, original_fs: float, target_fs: float) -> np.ndarray:\n",
    "    \"\"\"Resample signal to target frequency.\"\"\"\n",
    "    if original_fs == target_fs:\n",
    "        return data\n",
    "    \n",
    "    duration = len(data) / original_fs\n",
    "    n_samples = int(duration * target_fs)\n",
    "    \n",
    "    t_original = np.arange(len(data)) / original_fs\n",
    "    t_target = np.arange(n_samples) / target_fs\n",
    "    \n",
    "    if data.ndim == 1:\n",
    "        resampled = np.interp(t_target, t_original, data)\n",
    "    else:\n",
    "        resampled = np.zeros((n_samples, data.shape[1]))\n",
    "        for i in range(data.shape[1]):\n",
    "            resampled[:, i] = np.interp(t_target, t_original, data[:, i])\n",
    "    \n",
    "    return resampled\n",
    "\n",
    "\n",
    "def load_stress_labels() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load stress level labels for all subjects.\"\"\"\n",
    "    labels = {}\n",
    "    \n",
    "    if STRESS_LABELS_V1.exists():\n",
    "        df_v1 = pd.read_csv(STRESS_LABELS_V1)\n",
    "        for _, row in df_v1.iterrows():\n",
    "            subject = row.iloc[0]\n",
    "            labels[subject] = row.iloc[1:].to_dict()\n",
    "    \n",
    "    if STRESS_LABELS_V2.exists():\n",
    "        df_v2 = pd.read_csv(STRESS_LABELS_V2)\n",
    "        for _, row in df_v2.iterrows():\n",
    "            subject = row.iloc[0]\n",
    "            labels[subject] = row.iloc[1:].to_dict()\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "def map_stress_score_to_class(score: float) -> str:\n",
    "    \"\"\"Map self-reported stress score [0-10] to stress class.\"\"\"\n",
    "    if pd.isna(score):\n",
    "        return 'unknown'\n",
    "    \n",
    "    if score <= 2:\n",
    "        return 'no_stress'\n",
    "    elif score <= 5:\n",
    "        return 'low_stress'\n",
    "    elif score <= 7:\n",
    "        return 'moderate_stress'\n",
    "    else:\n",
    "        return 'high_stress'\n",
    "\n",
    "\n",
    "print(\"✓ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 9: Main Processing Pipeline\n",
    "\n",
    "This cell processes all subjects and extracts features. Due to length, this has been simplified. Run this to build the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Subject processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_subject_protocol(subject: str, protocol: str, phase: str,\n",
    "                            stress_labels: Dict) -> List[Dict]:\n",
    "    \"\"\"Process one subject-protocol-phase combination.\"\"\"\n",
    "    # Skip special cases\n",
    "    if subject == 'S12' and protocol == 'AEROBIC':\n",
    "        return []\n",
    "    \n",
    "    subject_dir = DATASETS_DIR / protocol / subject / phase\n",
    "    if not subject_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    # Load sensor files\n",
    "    eda_raw, eda_fs = load_sensor_file(subject_dir / 'EDA.csv')\n",
    "    temp_raw, temp_fs = load_sensor_file(subject_dir / 'TEMP.csv')\n",
    "    hr_raw, hr_fs = load_sensor_file(subject_dir / 'HR.csv')\n",
    "    bvp_raw, bvp_fs = load_sensor_file(subject_dir / 'BVP.csv')\n",
    "    ibi_raw = load_ibi_file(subject_dir / 'IBI.csv')\n",
    "    \n",
    "    # Load ACC\n",
    "    acc_file = subject_dir / 'ACC.csv'\n",
    "    if acc_file.exists():\n",
    "        with open(acc_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        acc_fs = float(lines[1].strip())\n",
    "        acc_data = []\n",
    "        for line in lines[2:]:\n",
    "            values = line.strip().split(',')\n",
    "            if len(values) == 3:\n",
    "                acc_data.append([float(v) for v in values])\n",
    "        acc_raw = np.array(acc_data) / 64.0  # Convert to g\n",
    "    else:\n",
    "        acc_raw, acc_fs = None, None\n",
    "    \n",
    "    if eda_raw is None or temp_raw is None:\n",
    "        return []\n",
    "    \n",
    "    # Handle f07 special case\n",
    "    if subject == 'f07':\n",
    "        bvp_raw = None\n",
    "        hr_raw = None\n",
    "        ibi_raw = None\n",
    "    \n",
    "    # Resample signals\n",
    "    eda = resample_signal(eda_raw, eda_fs, TARGET_FS)\n",
    "    temp = resample_signal(temp_raw, temp_fs, TARGET_FS)\n",
    "    \n",
    "    if hr_raw is not None:\n",
    "        hr = resample_signal(hr_raw, hr_fs, TARGET_FS)\n",
    "    else:\n",
    "        hr = np.full(len(eda), np.nan)\n",
    "    \n",
    "    if acc_raw is not None:\n",
    "        acc = resample_signal(acc_raw, acc_fs, TARGET_FS)\n",
    "    else:\n",
    "        acc = np.zeros((len(eda), 3))\n",
    "    \n",
    "    if bvp_raw is not None:\n",
    "        bvp = resample_signal(bvp_raw, bvp_fs, TARGET_FS)\n",
    "    else:\n",
    "        bvp = None\n",
    "    \n",
    "    # Apply preprocessing\n",
    "    eda_clean, temp_clean, acc_clean, bvp_clean = preprocess_signals(\n",
    "        eda, temp, acc, bvp, fs=TARGET_FS\n",
    "    )\n",
    "    \n",
    "    # Motion artifact removal\n",
    "    acc_mag = np.linalg.norm(acc_clean, axis=1)\n",
    "    eda_clean, motion_ratio = detect_motion_artifacts(acc_mag, eda_clean)\n",
    "    \n",
    "    # Extract windows\n",
    "    window_samples = int(WINDOW_SIZE * TARGET_FS)\n",
    "    step_samples = int(STEP_SIZE * TARGET_FS)\n",
    "    \n",
    "    windows = []\n",
    "    n_samples = min(len(eda_clean), len(temp_clean), len(hr), len(acc_clean))\n",
    "    \n",
    "    for start in range(0, n_samples - window_samples + 1, step_samples):\n",
    "        end = start + window_samples\n",
    "        \n",
    "        eda_win = eda_clean[start:end]\n",
    "        temp_win = temp_clean[start:end]\n",
    "        hr_win = hr[start:end]\n",
    "        acc_win = acc_clean[start:end]\n",
    "        \n",
    "        # Quality assessment\n",
    "        eda_quality = assess_signal_quality(eda_win, 'EDA')\n",
    "        temp_quality = assess_signal_quality(temp_win, 'TEMP')\n",
    "        hr_quality = assess_signal_quality(hr_win, 'HR')\n",
    "        \n",
    "        if eda_quality < MIN_SIGNAL_QUALITY or temp_quality < MIN_SIGNAL_QUALITY:\n",
    "            continue\n",
    "        \n",
    "        # Extract features\n",
    "        features = {}\n",
    "        features.update(extract_eda_features(eda_win, TARGET_FS))\n",
    "        features.update(extract_hrv_features(ibi_raw, WINDOW_SIZE))\n",
    "        features.update(extract_temp_features(temp_win, TARGET_FS))\n",
    "        features.update(extract_acc_features(acc_win, TARGET_FS))\n",
    "        features.update(extract_hr_features(hr_win))\n",
    "        features.update(cross_modal_features(eda_win, hr_win, temp_win, TARGET_FS))\n",
    "        \n",
    "        # Metadata\n",
    "        features['subject'] = subject\n",
    "        features['protocol'] = protocol\n",
    "        features['phase'] = phase\n",
    "        features['window_id'] = len(windows)\n",
    "        features['signal_quality_eda'] = eda_quality\n",
    "        features['signal_quality_temp'] = temp_quality\n",
    "        features['signal_quality_hr'] = hr_quality\n",
    "        features['motion_ratio'] = motion_ratio\n",
    "        \n",
    "        # Assign label\n",
    "        if protocol == 'STRESS' and subject in stress_labels:\n",
    "            stress_score = stress_labels[subject].get(phase, np.nan)\n",
    "            features['stress_score'] = stress_score\n",
    "            features['stress_class'] = map_stress_score_to_class(stress_score)\n",
    "        else:\n",
    "            features['stress_score'] = np.nan\n",
    "            if protocol in ['AEROBIC', 'ANAEROBIC']:\n",
    "                features['stress_class'] = 'no_stress' if phase == 'rest' else protocol.lower()\n",
    "            else:\n",
    "                features['stress_class'] = 'unknown'\n",
    "        \n",
    "        windows.append(features)\n",
    "    \n",
    "    return windows\n",
    "\n",
    "\n",
    "print(\"✓ Subject processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset building function defined\n"
     ]
    }
   ],
   "source": [
    "def build_dataset() -> pd.DataFrame:\n",
    "    \"\"\"Build complete dataset from all subjects and protocols.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BUILDING DATASET\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load stress labels\n",
    "    print(\"\\n[1/3] Loading stress labels...\")\n",
    "    stress_labels = load_stress_labels()\n",
    "    print(f\"  Loaded labels for {len(stress_labels)} subjects\")\n",
    "    \n",
    "    # Get all subjects\n",
    "    subjects = []\n",
    "    for protocol_dir in DATASETS_DIR.iterdir():\n",
    "        if protocol_dir.is_dir():\n",
    "            for subject_dir in protocol_dir.iterdir():\n",
    "                if subject_dir.is_dir():\n",
    "                    subjects.append((subject_dir.name, protocol_dir.name))\n",
    "    \n",
    "    subjects = list(set(subjects))\n",
    "    subjects.sort()\n",
    "    \n",
    "    print(f\"\\n[2/3] Processing {len(subjects)} subject-protocol combinations...\")\n",
    "    all_windows = []\n",
    "    \n",
    "    for subject, protocol in tqdm(subjects, desc=\"Processing\"):\n",
    "        subject_dir = DATASETS_DIR / protocol / subject\n",
    "        \n",
    "        for phase_dir in subject_dir.iterdir():\n",
    "            if phase_dir.is_dir():\n",
    "                phase = phase_dir.name\n",
    "                windows = process_subject_protocol(subject, protocol, phase, stress_labels)\n",
    "                all_windows.extend(windows)\n",
    "    \n",
    "    print(f\"  Extracted {len(all_windows)} windows\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    print(\"\\n[3/3] Building DataFrame...\")\n",
    "    df = pd.DataFrame(all_windows)\n",
    "    \n",
    "    print(f\"\\n  Dataset shape: {df.shape}\")\n",
    "    print(f\"  Subjects: {df['subject'].nunique()}\")\n",
    "    if 'stress_class' in df.columns:\n",
    "        print(f\"\\n  Stress class distribution:\")\n",
    "        print(df['stress_class'].value_counts())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"✓ Dataset building function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset construction...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "BUILDING DATASET\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading stress labels...\n",
      "  Loaded labels for 36 subjects\n",
      "\n",
      "[2/3] Processing 100 subject-protocol combinations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a013a77f174b2a8e01c8c11d95d751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Extracted 0 windows\n",
      "\n",
      "[3/3] Building DataFrame...\n",
      "\n",
      "  Dataset shape: (0, 0)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'subject'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Build the dataset (this will take 15-20 minutes)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting dataset construction...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDataset construction complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_windows)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Subjects: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstress_class\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Stress class distribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf-gpu/lib/python3.9/site-packages/pandas/core/indexes/range.py:417\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[0;32m--> 417\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'subject'"
     ]
    }
   ],
   "source": [
    "# Build the dataset (this will take 15-20 minutes)\n",
    "print(\"Starting dataset construction...\\n\")\n",
    "dataset = build_dataset()\n",
    "print(\"\\nDataset construction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 10: Add Demographics & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add demographics\n",
    "print(\"\\nAdding demographic features...\")\n",
    "demographics = load_demographic_data(SUBJECT_INFO_PATH)\n",
    "dataset = dataset.merge(demographics, on='subject', how='left')\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n",
    "\n",
    "# Apply normalization\n",
    "print(\"\\nApplying subject-specific normalization...\")\n",
    "exclude_cols = ['subject', 'protocol', 'phase', 'window_id', 'stress_score', 'stress_class',\n",
    "                'signal_quality_eda', 'signal_quality_temp', 'signal_quality_hr', 'motion_ratio']\n",
    "feature_cols = [col for col in dataset.columns if col not in exclude_cols]\n",
    "\n",
    "dataset = normalize_by_subject_baseline(dataset, feature_cols, 'subject', 'phase')\n",
    "print(\"✓ Normalization complete\")\n",
    "\n",
    "# Save dataset\n",
    "output_file = OUTPUT_DIR / \"improved_stress_dataset.csv\"\n",
    "dataset.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 11: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "valid_classes = ['no_stress', 'low_stress', 'moderate_stress', 'high_stress']\n",
    "dataset_filtered = dataset[dataset['stress_class'].isin(valid_classes)].copy()\n",
    "\n",
    "print(f\"Filtered dataset: {dataset_filtered.shape}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(dataset_filtered['stress_class'].value_counts())\n",
    "\n",
    "exclude_cols = ['subject', 'protocol', 'phase', 'window_id', 'stress_score', 'stress_class',\n",
    "                'signal_quality_eda', 'signal_quality_temp', 'signal_quality_hr', 'motion_ratio']\n",
    "feature_cols = [col for col in dataset_filtered.columns if col not in exclude_cols]\n",
    "\n",
    "X = dataset_filtered[feature_cols].fillna(0).values\n",
    "y = dataset_filtered['stress_class'].values\n",
    "groups = dataset_filtered['subject'].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"\\nFeatures: {X.shape}\")\n",
    "print(f\"Classes: {le.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with cross-validation\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"5-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_params = {\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 200,\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': len(le.classes_),\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': 42,\n",
    "    'n_jobs': -1\n",
    "}\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y_encoded, groups), 1):\n",
    "    print(f\"\\n--- Fold {fold} ---\")\n",
    "    \n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    \n",
    "    model = xgb.XGBClassifier(**xgb_params)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    f1_macro = f1_score(y_val, y_pred, average='macro')\n",
    "    f1_weighted = f1_score(y_val, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy:    {acc:.4f}\")\n",
    "    print(f\"Macro F1:    {f1_macro:.4f}\")\n",
    "    print(f\"Weighted F1: {f1_weighted:.4f}\")\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': acc,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'model': model\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "avg_acc = np.mean([r['accuracy'] for r in fold_results])\n",
    "avg_f1_macro = np.mean([r['f1_macro'] for r in fold_results])\n",
    "avg_f1_weighted = np.mean([r['f1_weighted'] for r in fold_results])\n",
    "\n",
    "print(f\"\\nAverage Accuracy:    {avg_acc:.4f}\")\n",
    "print(f\"Average Macro F1:    {avg_f1_macro:.4f}\")\n",
    "print(f\"Average Weighted F1: {avg_f1_weighted:.4f}\")\n",
    "\n",
    "print(\"\\n### BASELINE vs IMPROVED:\")\n",
    "print(f\"Accuracy:  90.8% → {avg_acc*100:.1f}% ({(avg_acc-0.908)*100:+.1f}pp)\")\n",
    "print(f\"Macro F1:  47.6% → {avg_f1_macro*100:.1f}% ({(avg_f1_macro-0.476)*100:+.1f}pp)\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 12: Train Final Model & Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model\n",
    "print(\"\\nTraining final model on full dataset...\")\n",
    "final_model = xgb.XGBClassifier(**xgb_params)\n",
    "final_model.fit(X, y_encoded)\n",
    "\n",
    "model_path = OUTPUT_DIR / \"xgboost_stress_model.json\"\n",
    "final_model.save_model(str(model_path))\n",
    "print(f\"✓ Model saved to: {model_path}\")\n",
    "\n",
    "# Feature importance\n",
    "importances = final_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "importance_path = OUTPUT_DIR / \"feature_importance.csv\"\n",
    "feature_importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"✓ Feature importance saved to: {importance_path}\")\n",
    "\n",
    "print(f\"\\nTop 20 features:\")\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_20 = feature_importance_df.head(20)\n",
    "plt.barh(range(len(top_20)), top_20['importance'])\n",
    "plt.yticks(range(len(top_20)), top_20['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"feature_importance.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. improved_stress_dataset.csv\")\n",
    "print(\"  2. xgboost_stress_model.json\")\n",
    "print(\"  3. feature_importance.csv\")\n",
    "print(\"  4. feature_importance.png\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
