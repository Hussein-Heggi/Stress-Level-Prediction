{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress Level Prediction Pipeline\n",
    "\n",
    "End-to-end preprocessing and modeling for stress vs non-stress, including aerobic/anaerobic as active negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "- Window length 60s, step 30s\n",
    "- Minimum label coverage 0.7\n",
    "- Protocol tag mapping per provided notebook\n",
    "- Data constraints applied during load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DEFAULT_DATASET_ROOT = Path(\"./Datasets\")\n",
    "DATASET_ROOT = Path(os.getenv(\"DATASET_ROOT\", DEFAULT_DATASET_ROOT))\n",
    "\n",
    "# Windowing\n",
    "WINDOW_SECONDS = 60\n",
    "WINDOW_STEP_SECONDS = 30\n",
    "MIN_LABEL_COVERAGE = 0.6\n",
    "\n",
    "# Sampling / weighting\n",
    "PHASE_BALANCE_NO_STRESS_RATIO = 1.0\n",
    "PHASE_WEIGHT_MAP = {\n",
    "    \"Stroop\": 1.6,\n",
    "    \"Opposite Opinion\": 1.4,\n",
    "    \"Real Opinion\": 1.3,\n",
    "    \"Subtract\": 1.2,\n",
    "    \"TMCT\": 1.4,\n",
    "}\n",
    "PHASE_LABEL_OVERSAMPLE = {\n",
    "    (\"Stroop\", \"high_stress\"): 3.5,\n",
    "    (\"Stroop\", \"moderate_stress\"): 2.2,\n",
    "    (\"TMCT\", \"high_stress\"): 2.8,\n",
    "    (\"TMCT\", \"moderate_stress\"): 1.8,\n",
    "    (\"Opposite Opinion\", \"low_stress\"): 2.5,\n",
    "    (\"Opposite Opinion\", \"moderate_stress\"): 1.8,\n",
    "    (\"Real Opinion\", \"moderate_stress\"): 1.6,\n",
    "}\n",
    "EMA_SPAN = 5\n",
    "PHASE_Z_MIN_STD = 1e-3\n",
    "STRESS_PROB_THRESHOLD = 0.4\n",
    "PHASE_SMOOTHING_WINDOW = 3\n",
    "\n",
    "# Data constraints\n",
    "DUPLICATE_CUTS = {\"S02\": {\"ACC\": 49545, \"BVP\": 99091, \"EDA\": 6195, \"TEMP\": 6195}}\n",
    "MISSING_SENSORS = {\"f07\": {\"BVP\", \"TEMP\", \"HR\", \"IBI\"}}\n",
    "\n",
    "STATES = [\"STRESS\", \"AEROBIC\", \"ANAEROBIC\"]\n",
    "\n",
    "# Stress protocol metadata\n",
    "STRESS_STAGE_ORDER_S = [\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_STAGE_ORDER_F = [\"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"]\n",
    "STRESS_TAG_PAIRS_S = [(3, 4), (5, 6), (7, 8), (9, 10), (11, 12)]\n",
    "STRESS_TAG_PAIRS_F = [(2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "STRESS_LEVEL_FILES = [\"Stress_Level_v1.csv\", \"Stress_Level_v2.csv\"]\n",
    "\n",
    "def load_stress_levels() -> Dict[str, Dict[str, float]]:\n",
    "    tables = []\n",
    "    for fname in STRESS_LEVEL_FILES:\n",
    "        path = Path(fname)\n",
    "        if not path.exists():\n",
    "            continue\n",
    "        df = pd.read_csv(path, index_col=0)\n",
    "        df.columns = [str(c).strip() for c in df.columns]\n",
    "        tables.append(df)\n",
    "    levels: Dict[str, Dict[str, float]] = {}\n",
    "    for table in tables:\n",
    "        for subject, row in table.iterrows():\n",
    "            subj = str(subject).strip()\n",
    "            levels[subj] = {col: (float(row[col]) if not pd.isna(row[col]) else np.nan) for col in table.columns}\n",
    "    return levels\n",
    "\n",
    "\n",
    "STRESS_LEVELS = load_stress_levels()\n",
    "\n",
    "\n",
    "STRESS_PHASES = {\"Stroop\", \"TMCT\", \"Real Opinion\", \"Opposite Opinion\", \"Subtract\"}\n",
    "STRESS_LEVEL_BOUNDS = {\"low\": 3.0, \"moderate\": 6.0}\n",
    "STRESS_LEVEL_PHASE_BOUNDS = {\n",
    "    \"Stroop\": {\"low\": 2.5, \"moderate\": 5.0},\n",
    "    \"Opposite Opinion\": {\"low\": 2.5, \"moderate\": 5.5},\n",
    "    \"Real Opinion\": {\"low\": 2.8, \"moderate\": 5.5},\n",
    "    \"TMCT\": {\"low\": 2.8, \"moderate\": 5.8},\n",
    "    \"Subtract\": {\"low\": 2.8, \"moderate\": 5.8},\n",
    "}\n",
    "TEMPORAL_FEATURES = [\n",
    "    \"eda_mean\",\n",
    "    \"eda_std\",\n",
    "    \"eda_range\",\n",
    "    \"eda_slope\",\n",
    "    \"acc_energy\",\n",
    "    \"temp_mean\",\n",
    "    \"temp_range\",\n",
    "    \"hr_mean\",\n",
    "    \"bvp_mean\",\n",
    "    \"bvp_std\",\n",
    "    \"bvp_hr_est\",\n",
    "]\n",
    "FEATURE_EXCLUDE_COLS = {\n",
    "    \"subject\",\n",
    "    \"state\",\n",
    "    \"phase\",\n",
    "    \"stress_stage\",\n",
    "    \"stress_level\",\n",
    "    \"label\",\n",
    "    \"is_stress\",\n",
    "    \"win_start\",\n",
    "    \"win_end\",\n",
    "    \"phase_start\",\n",
    "    \"phase_end\",\n",
    "}\n",
    "\n",
    "ACC_ACTIVITY_WINDOW_SEC = 2.0\n",
    "ACC_ACTIVITY_STEP_SEC = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for Empatica format and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base subject ID without session suffix.\n",
    "def base_subject_id(subject: str) -> str:\n",
    "    return subject.split(\"_\")[0]\n",
    "\n",
    "\n",
    "# Read signal CSV and return (fs, data, start_timestamp).\n",
    "def read_signal(path: Path) -> Tuple[float, np.ndarray, pd.Timestamp]:\n",
    "    with open(path, \"r\") as f:\n",
    "        start_line = f.readline()\n",
    "        if not start_line:\n",
    "            raise ValueError(f\"Missing start timestamp in {path}\")\n",
    "        start = pd.to_datetime(start_line.split(\",\")[0])\n",
    "        fs_line = f.readline()\n",
    "        if not fs_line:\n",
    "            raise ValueError(f\"Missing sample rate in {path}\")\n",
    "        fs = float(fs_line.split(\",\")[0])\n",
    "        data = np.genfromtxt(f, delimiter=\",\")\n",
    "    if np.isscalar(data):\n",
    "        if math.isnan(float(data)):\n",
    "            data = np.empty((0, 1))\n",
    "        else:\n",
    "            data = np.array([[float(data)]])\n",
    "    elif data.size == 0:\n",
    "        data = np.empty((0, 1))\n",
    "    else:\n",
    "        data = np.asarray(data, dtype=float)\n",
    "        if np.isnan(data).all():\n",
    "            data = np.empty((0, 1))\n",
    "        elif data.ndim == 1:\n",
    "            data = data[:, None]\n",
    "    return fs, data, start\n",
    "\n",
    "\n",
    "# Read IBI CSV and return timestamped inter-beat intervals.\n",
    "def read_ibi(path: Path) -> np.ndarray:\n",
    "    if not path.exists():\n",
    "        return np.empty((0, 2), dtype=float)\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None, skiprows=1)\n",
    "    except (pd.errors.EmptyDataError, FileNotFoundError):\n",
    "        return np.empty((0, 2), dtype=float)\n",
    "    if df.empty:\n",
    "        return np.empty((0, 2), dtype=float)\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\").dropna()\n",
    "    if df.empty:\n",
    "        return np.empty((0, 2), dtype=float)\n",
    "    if df.shape[1] >= 2:\n",
    "        times = df.iloc[:, 0].to_numpy(dtype=float)\n",
    "        ibi = df.iloc[:, 1].to_numpy(dtype=float)\n",
    "    else:\n",
    "        ibi = df.iloc[:, 0].to_numpy(dtype=float)\n",
    "        times = np.cumsum(ibi)\n",
    "    if times.size:\n",
    "        times = np.clip(times, a_min=0.0, a_max=None)\n",
    "    mask = (ibi > 0) & np.isfinite(times) & np.isfinite(ibi)\n",
    "    if not np.any(mask):\n",
    "        return np.empty((0, 2), dtype=float)\n",
    "    return np.column_stack([times[mask], ibi[mask]])\n",
    "\n",
    "\n",
    "# Read tags.csv and return list of tag timestamps (seconds since start).\n",
    "def read_tags(path: Path, start_ts: pd.Timestamp) -> List[Tuple[float, float]]:\n",
    "    try:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return []\n",
    "    tags = []\n",
    "    for ts_str in df[0].astype(str):\n",
    "        ts = pd.to_datetime(ts_str)\n",
    "        tags.append((ts - start_ts).total_seconds())\n",
    "    return [(t, t) for t in tags]\n",
    "\n",
    "\n",
    "# Extract stress intervals from tags based on subject version.\n",
    "def stress_intervals_from_tags(tags: List[Tuple[float, float]], subject: str) -> List[dict]:\n",
    "    if not tags:\n",
    "        return []\n",
    "    t = [x[0] for x in tags]\n",
    "    if subject.startswith(\"S\"):\n",
    "        idx_pairs = STRESS_TAG_PAIRS_S\n",
    "        stage_order = STRESS_STAGE_ORDER_S\n",
    "    else:\n",
    "        idx_pairs = STRESS_TAG_PAIRS_F\n",
    "        stage_order = STRESS_STAGE_ORDER_F\n",
    "    base_id = base_subject_id(subject)\n",
    "    spans = []\n",
    "    for stage, (i, j) in zip(stage_order, idx_pairs):\n",
    "        if i < len(t) and j < len(t) and t[j] > t[i]:\n",
    "            level = STRESS_LEVELS.get(base_id, {}).get(stage)\n",
    "            spans.append({\n",
    "                \"start\": t[i],\n",
    "                \"end\": t[j],\n",
    "                \"stage\": stage,\n",
    "                \"stress_level\": level,\n",
    "            })\n",
    "    return spans\n",
    "\n",
    "\n",
    "# Extract active intervals from tags.\n",
    "def active_intervals_from_tags(tags: List[Tuple[float, float]], label: str) -> List[dict]:\n",
    "    if len(tags) < 2:\n",
    "        return []\n",
    "    t = [x[0] for x in tags]\n",
    "    spans = []\n",
    "    for a, b in zip(t[:-1], t[1:]):\n",
    "        if b > a:\n",
    "            spans.append({\n",
    "                \"start\": a,\n",
    "                \"end\": b,\n",
    "                \"stage\": label,\n",
    "                \"stress_level\": 0.0,\n",
    "            })\n",
    "    return spans\n",
    "\n",
    "\n",
    "# Map numeric stress score to four-level category.\n",
    "def stress_bucket(level: float | None, phase: str | None) -> str:\n",
    "    if phase in {\"aerobic\", \"anaerobic\", \"rest\"}:\n",
    "        return \"no_stress\"\n",
    "    if level is None or pd.isna(level) or level <= 0:\n",
    "        return \"no_stress\"\n",
    "    bounds = STRESS_LEVEL_PHASE_BOUNDS.get(phase, STRESS_LEVEL_BOUNDS)\n",
    "    if level <= bounds[\"low\"]:\n",
    "        return \"low_stress\"\n",
    "    if level <= bounds[\"moderate\"]:\n",
    "        return \"moderate_stress\"\n",
    "    return \"high_stress\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample signal to target frequency.\n",
    "def resample_to_rate(signal: np.ndarray, src_fs: float, tgt_fs: float) -> np.ndarray:\n",
    "    if signal.ndim == 1:\n",
    "        signal = signal[:, None]\n",
    "    src_len = signal.shape[0]\n",
    "    duration = src_len / src_fs\n",
    "    tgt_len = int(duration * tgt_fs)\n",
    "    src_t = np.linspace(0, duration, src_len)\n",
    "    tgt_t = np.linspace(0, duration, tgt_len)\n",
    "    resampled = np.stack([np.interp(tgt_t, src_t, signal[:, i]) for i in range(signal.shape[1])], axis=1)\n",
    "    if resampled.shape[1] == 1:\n",
    "        return resampled[:, 0]\n",
    "    return resampled\n",
    "\n",
    "# Simple Hampel filter for outlier suppression.\n",
    "def hampel_filter(x: np.ndarray, k: int = 5, t0: float = 3.0) -> np.ndarray:\n",
    "    x = x.copy()\n",
    "    n = len(x)\n",
    "    for i in range(n):\n",
    "        window = x[max(i - k, 0): min(i + k, n)]\n",
    "        med = np.median(window)\n",
    "        mad = np.median(np.abs(window - med)) or 1e-6\n",
    "        if abs(x[i] - med) > t0 * mad:\n",
    "            x[i] = med\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximate ACC activity envelope using sliding RMS to capture movement intensity.\n",
    "def acc_activity_signal(\n",
    "    acc_raw: np.ndarray,\n",
    "    fs: float,\n",
    "    win_sec: float = ACC_ACTIVITY_WINDOW_SEC,\n",
    "    step_sec: float = ACC_ACTIVITY_STEP_SEC,\n",
    ") -> np.ndarray:\n",
    "    if acc_raw.size == 0 or fs <= 0:\n",
    "        return np.array([])\n",
    "    if acc_raw.ndim == 1:\n",
    "        acc_raw = acc_raw[:, None]\n",
    "    magnitude = np.linalg.norm(acc_raw, axis=1)\n",
    "    win = max(1, int(round(win_sec * fs)))\n",
    "    step = max(1, int(round(step_sec * fs)))\n",
    "    if len(magnitude) < win:\n",
    "        rms = float(np.sqrt(np.mean(magnitude ** 2))) if len(magnitude) else 0.0\n",
    "        return np.array([rms])\n",
    "    activity = []\n",
    "    start = 0\n",
    "    while start + win <= len(magnitude):\n",
    "        segment = magnitude[start:start + win]\n",
    "        rms = float(np.sqrt(np.mean(segment ** 2)))\n",
    "        activity.append(rms)\n",
    "        start += step\n",
    "    if start < len(magnitude):\n",
    "        segment = magnitude[-win:]\n",
    "        rms = float(np.sqrt(np.mean(segment ** 2)))\n",
    "        if not activity or abs(activity[-1] - rms) > 1e-9:\n",
    "            activity.append(rms)\n",
    "    return np.asarray(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5b1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def spectral_centroid(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size == 0 or fs <= 0:\n",
    "        return np.nan\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(256, len(signal)))\n",
    "    total = psd.sum()\n",
    "    if total <= 0:\n",
    "        return np.nan\n",
    "    return float(np.sum(freqs * psd) / total)\n",
    "\n",
    "\n",
    "def bandpower(signal: np.ndarray, fs: float, fmin: float, fmax: float) -> float:\n",
    "    if signal.size == 0 or fs <= 0 or fmax <= fmin:\n",
    "        return np.nan\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(256, len(signal)))\n",
    "    mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    trap = getattr(np, \"trapezoid\", np.trapz)\n",
    "    return float(trap(psd[mask], freqs[mask]))\n",
    "\n",
    "\n",
    "def dominant_frequency(signal: np.ndarray, fs: float, fmin: float, fmax: float) -> float:\n",
    "    if signal.size == 0 or fs <= 0 or fmax <= fmin:\n",
    "        return np.nan\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=min(256, len(signal)))\n",
    "    mask = (freqs >= fmin) & (freqs <= fmax)\n",
    "    if not np.any(mask):\n",
    "        return np.nan\n",
    "    idx = np.argmax(psd[mask])\n",
    "    return float(freqs[mask][idx])\n",
    "\n",
    "\n",
    "def linear_slope(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size < 2 or fs <= 0:\n",
    "        return 0.0\n",
    "    t = np.arange(signal.size) / fs\n",
    "    slope, _ = np.polyfit(t, signal, 1)\n",
    "    return float(slope)\n",
    "\n",
    "\n",
    "def peak_rate(signal: np.ndarray, fs: float) -> float:\n",
    "    if signal.size < 3 or fs <= 0:\n",
    "        return 0.0\n",
    "    duration = signal.size / fs\n",
    "    if duration <= 0:\n",
    "        return 0.0\n",
    "    peaks = np.sum((signal[1:-1] > signal[:-2]) & (signal[1:-1] > signal[2:]))\n",
    "    return float(peaks / duration)\n",
    "\n",
    "\n",
    "def eda_features(eda: np.ndarray, fs: float) -> Dict[str, float]:\n",
    "    if eda.size == 0 or fs <= 0:\n",
    "        return {\n",
    "            \"eda_mean\": np.nan,\n",
    "            \"eda_std\": np.nan,\n",
    "            \"eda_slope\": np.nan,\n",
    "            \"eda_peak_rate\": np.nan,\n",
    "            \"eda_range\": np.nan,\n",
    "            \"eda_power_slow\": np.nan,\n",
    "            \"eda_power_mid\": np.nan,\n",
    "            \"eda_power_fast\": np.nan,\n",
    "            \"eda_centroid\": np.nan,\n",
    "            \"eda_min\": np.nan,\n",
    "            \"eda_max\": np.nan,\n",
    "            \"eda_skew\": np.nan,\n",
    "            \"eda_kurt\": np.nan,\n",
    "            \"eda_diff_mean\": np.nan,\n",
    "            \"eda_diff_std\": np.nan,\n",
    "        }\n",
    "    clean = hampel_filter(eda)\n",
    "    diff = np.diff(clean) if clean.size > 1 else np.array([0.0])\n",
    "    return {\n",
    "        \"eda_mean\": float(np.mean(clean)),\n",
    "        \"eda_std\": float(np.std(clean)),\n",
    "        \"eda_slope\": linear_slope(clean, fs),\n",
    "        \"eda_peak_rate\": peak_rate(clean, fs),\n",
    "        \"eda_range\": float(np.max(clean) - np.min(clean)),\n",
    "        \"eda_power_slow\": bandpower(clean, fs, 0.01, 0.05),\n",
    "        \"eda_power_mid\": bandpower(clean, fs, 0.045, 0.25),\n",
    "        \"eda_power_fast\": bandpower(clean, fs, 0.25, 1.5),\n",
    "        \"eda_centroid\": spectral_centroid(clean, fs),\n",
    "        \"eda_min\": float(np.min(clean)),\n",
    "        \"eda_max\": float(np.max(clean)),\n",
    "        \"eda_skew\": float(skew(clean)),\n",
    "        \"eda_kurt\": float(kurtosis(clean)),\n",
    "        \"eda_diff_mean\": float(np.mean(diff)) if diff.size else 0.0,\n",
    "        \"eda_diff_std\": float(np.std(diff)) if diff.size else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def temp_features(temp: np.ndarray, fs: float) -> Dict[str, float]:\n",
    "    if temp.size == 0 or fs <= 0:\n",
    "        return {\n",
    "            \"temp_mean\": np.nan,\n",
    "            \"temp_std\": np.nan,\n",
    "            \"temp_slope\": np.nan,\n",
    "            \"temp_min\": np.nan,\n",
    "            \"temp_max\": np.nan,\n",
    "            \"temp_range\": np.nan,\n",
    "        }\n",
    "    return {\n",
    "        \"temp_mean\": float(np.mean(temp)),\n",
    "        \"temp_std\": float(np.std(temp)),\n",
    "        \"temp_slope\": linear_slope(temp, fs),\n",
    "        \"temp_min\": float(np.min(temp)),\n",
    "        \"temp_max\": float(np.max(temp)),\n",
    "        \"temp_range\": float(np.max(temp) - np.min(temp)),\n",
    "    }\n",
    "\n",
    "\n",
    "def hrv_features(ibi: np.ndarray) -> Dict[str, float]:\n",
    "    if ibi.size < 2:\n",
    "        return {\n",
    "            \"hr_mean\": np.nan,\n",
    "            \"rmssd\": np.nan,\n",
    "            \"sdnn\": np.nan,\n",
    "            \"pnn50\": np.nan,\n",
    "            \"lf_hf\": np.nan,\n",
    "            \"sd1\": np.nan,\n",
    "            \"sd2\": np.nan,\n",
    "        }\n",
    "    rr = ibi.astype(float)\n",
    "    diff = np.diff(rr)\n",
    "    rmssd = float(np.sqrt(np.mean(diff ** 2))) if diff.size else np.nan\n",
    "    sdnn = float(np.std(rr))\n",
    "    pnn50 = float(np.mean(np.abs(diff) > 0.05)) if diff.size else np.nan\n",
    "    hr_mean = float(60.0 / np.mean(rr)) if np.mean(rr) > 0 else np.nan\n",
    "\n",
    "    if diff.size:\n",
    "        sd1 = float(np.sqrt(0.5) * np.std(diff))\n",
    "    else:\n",
    "        sd1 = np.nan\n",
    "    if not np.isnan(sdnn) and not np.isnan(sd1):\n",
    "        sd2_sq = max(0.0, 2 * (sdnn ** 2) - 0.5 * (sd1 ** 2))\n",
    "        sd2 = float(np.sqrt(sd2_sq))\n",
    "    else:\n",
    "        sd2 = np.nan\n",
    "\n",
    "    lf_hf = np.nan\n",
    "    try:\n",
    "        t = np.cumsum(rr)\n",
    "        t = t - t[0]\n",
    "        if t[-1] > 0 and rr.size >= 4:\n",
    "            fs_interp = 4.0\n",
    "            grid = np.arange(0, t[-1], 1 / fs_interp)\n",
    "            if grid.size >= 8:\n",
    "                interp_rr = np.interp(grid, t[: len(grid)], rr[: len(grid)])\n",
    "                lf = bandpower(interp_rr, fs_interp, 0.04, 0.15)\n",
    "                hf = bandpower(interp_rr, fs_interp, 0.15, 0.4)\n",
    "                if hf and hf > 0:\n",
    "                    lf_hf = float(lf / hf)\n",
    "    except Exception:\n",
    "        lf_hf = np.nan\n",
    "\n",
    "    return {\n",
    "        \"hr_mean\": hr_mean,\n",
    "        \"rmssd\": rmssd,\n",
    "        \"sdnn\": sdnn,\n",
    "        \"pnn50\": pnn50,\n",
    "        \"lf_hf\": lf_hf,\n",
    "        \"sd1\": sd1,\n",
    "        \"sd2\": sd2,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acc_features(acc_mag: np.ndarray, fs: float, acc_activity: np.ndarray | None = None) -> Dict[str, float]:\n",
    "    if len(acc_mag) == 0:\n",
    "        base = {\n",
    "            \"acc_mean\": np.nan,\n",
    "            \"acc_std\": np.nan,\n",
    "            \"acc_energy\": np.nan,\n",
    "            \"acc_peak_freq\": np.nan,\n",
    "            \"acc_bandpower_low\": np.nan,\n",
    "            \"acc_bandpower_mid\": np.nan,\n",
    "            \"acc_bandpower_high\": np.nan,\n",
    "            \"acc_mad\": np.nan,\n",
    "        }\n",
    "    else:\n",
    "        energy = np.mean(acc_mag ** 2)\n",
    "        base = {\n",
    "            \"acc_mean\": float(np.mean(acc_mag)),\n",
    "            \"acc_std\": float(np.std(acc_mag)),\n",
    "            \"acc_energy\": float(energy),\n",
    "            \"acc_peak_freq\": spectral_centroid(acc_mag, fs),\n",
    "            \"acc_bandpower_low\": bandpower(acc_mag, fs, 0.1, 0.5),\n",
    "            \"acc_bandpower_mid\": bandpower(acc_mag, fs, 0.5, 2.0),\n",
    "            \"acc_bandpower_high\": bandpower(acc_mag, fs, 2.0, 8.0),\n",
    "            \"acc_mad\": float(np.median(np.abs(acc_mag - np.median(acc_mag)))),\n",
    "        }\n",
    "    if acc_activity is not None and len(acc_activity):\n",
    "        base.update(\n",
    "            {\n",
    "                \"acc_activity_mean\": float(np.mean(acc_activity)),\n",
    "                \"acc_activity_std\": float(np.std(acc_activity)),\n",
    "                \"acc_activity_max\": float(np.max(acc_activity)),\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        base.update(\n",
    "            {\n",
    "                \"acc_activity_mean\": np.nan,\n",
    "                \"acc_activity_std\": np.nan,\n",
    "                \"acc_activity_max\": np.nan,\n",
    "            }\n",
    "        )\n",
    "    return base\n",
    "\n",
    "\n",
    "def bvp_features(bvp: np.ndarray | None, fs: float | None) -> Dict[str, float]:\n",
    "    if bvp is None or len(bvp) == 0 or fs is None or fs <= 0:\n",
    "        return {\n",
    "            \"bvp_mean\": np.nan,\n",
    "            \"bvp_std\": np.nan,\n",
    "            \"bvp_slope\": np.nan,\n",
    "            \"bvp_range\": np.nan,\n",
    "            \"bvp_peak_rate\": np.nan,\n",
    "            \"bvp_hr_est\": np.nan,\n",
    "            \"bvp_power_low\": np.nan,\n",
    "            \"bvp_power_high\": np.nan,\n",
    "            \"bvp_centroid\": np.nan,\n",
    "            \"bvp_power_ratio\": np.nan,\n",
    "            \"bvp_resp_freq\": np.nan,\n",
    "        }\n",
    "    clean = hampel_filter(bvp)\n",
    "    peak_per_sec = peak_rate(clean, fs)\n",
    "    hr_est = peak_per_sec * 60.0 if peak_per_sec > 0 else np.nan\n",
    "    low_power = bandpower(clean, fs, 0.04, 0.15)\n",
    "    high_power = bandpower(clean, fs, 0.15, 0.4)\n",
    "    ratio = float(high_power / (low_power + 1e-6)) if not np.isnan(high_power) else np.nan\n",
    "    resp_freq = dominant_frequency(clean, fs, 0.1, 0.5)\n",
    "    return {\n",
    "        \"bvp_mean\": float(np.mean(clean)),\n",
    "        \"bvp_std\": float(np.std(clean)),\n",
    "        \"bvp_slope\": linear_slope(clean, fs),\n",
    "        \"bvp_range\": float(np.max(clean) - np.min(clean)),\n",
    "        \"bvp_peak_rate\": float(peak_per_sec),\n",
    "        \"bvp_hr_est\": float(hr_est),\n",
    "        \"bvp_power_low\": low_power,\n",
    "        \"bvp_power_high\": high_power,\n",
    "        \"bvp_centroid\": spectral_centroid(clean, fs),\n",
    "        \"bvp_power_ratio\": ratio,\n",
    "        \"bvp_resp_freq\": float(resp_freq),\n",
    "    }\n",
    "\n",
    "\n",
    "def combine_features(eda, eda_fs, acc_mag, acc_fs, temp, temp_fs, ibi, acc_activity=None, bvp=None, bvp_fs=None) -> Dict[str, float]:\n",
    "    feats = {}\n",
    "    feats.update(eda_features(eda, eda_fs))\n",
    "    feats.update(acc_features(acc_mag, acc_fs, acc_activity))\n",
    "    feats.update(temp_features(temp, temp_fs))\n",
    "    feats.update(hrv_features(ibi))\n",
    "    feats.update(bvp_features(bvp, bvp_fs))\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windowing and label assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Window:\n",
    "    start: float\n",
    "    end: float\n",
    "    label: str\n",
    "    subject: str\n",
    "    state: str\n",
    "\n",
    "\n",
    "def window_intervals(duration: float, win_s: int = WINDOW_SECONDS, step_s: int = WINDOW_STEP_SECONDS) -> List[Tuple[float, float]]:\n",
    "    windows = []\n",
    "    t = 0.0\n",
    "    while t + win_s <= duration:\n",
    "        windows.append((t, t + win_s))\n",
    "        t += step_s\n",
    "    return windows\n",
    "\n",
    "\n",
    "def _span_bounds(span) -> Tuple[float, float]:\n",
    "    if isinstance(span, dict):\n",
    "        return span[\"start\"], span[\"end\"]\n",
    "    return span\n",
    "\n",
    "\n",
    "def assign_label(win: Tuple[float, float], intervals: Dict[str, List[dict]]) -> Tuple[str | None, dict | None]:\n",
    "    start, end = win\n",
    "    length = end - start\n",
    "    best_label = None\n",
    "    best_cov = 0.0\n",
    "    best_span = None\n",
    "    for lbl, spans in intervals.items():\n",
    "        label_overlap = 0.0\n",
    "        label_best_span = None\n",
    "        label_best_overlap = 0.0\n",
    "        for span in spans:\n",
    "            a, b = _span_bounds(span)\n",
    "            inter = max(0.0, min(end, b) - max(start, a))\n",
    "            if inter > 0:\n",
    "                label_overlap += inter\n",
    "                if inter > label_best_overlap:\n",
    "                    label_best_overlap = inter\n",
    "                    label_best_span = span\n",
    "        coverage = label_overlap / length\n",
    "        if coverage > best_cov:\n",
    "            best_cov = coverage\n",
    "            best_label = lbl\n",
    "            best_span = label_best_span\n",
    "    if best_cov >= MIN_LABEL_COVERAGE and best_label is not None:\n",
    "        return best_label, best_span\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def make_label_intervals(state: str, subject: str, tags: List[Tuple[float, float]], duration: float) -> Dict[str, List[dict]]:\n",
    "    rest_span = {\"start\": 0.0, \"end\": duration, \"stage\": \"rest\", \"stress_level\": 0.0}\n",
    "    if state == \"STRESS\":\n",
    "        stress_spans = stress_intervals_from_tags(tags, subject)\n",
    "        if not stress_spans:\n",
    "            return {\"rest\": [rest_span]}\n",
    "        return {\n",
    "            \"stress\": stress_spans,\n",
    "            \"rest\": [rest_span],\n",
    "        }\n",
    "    else:\n",
    "        lbl = \"aerobic\" if state == \"AEROBIC\" else \"anaerobic\"\n",
    "        active = active_intervals_from_tags(tags, lbl)\n",
    "        return {\n",
    "            lbl: active,\n",
    "            \"rest\": [rest_span],\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion: read signals per subject/state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject_state(state: str, subject: str) -> dict:\n",
    "    folder = DATASET_ROOT / state / subject\n",
    "    base_id = base_subject_id(subject)\n",
    "    sensors = {}\n",
    "    fs_map = {}\n",
    "    missing = MISSING_SENSORS.get(base_id, set())\n",
    "\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(folder)\n",
    "    fs_eda, eda_raw, start_ts = read_signal(folder / \"EDA.csv\")\n",
    "    sensors[\"EDA\"] = np.squeeze(eda_raw)\n",
    "    fs_map[\"EDA\"] = fs_eda\n",
    "\n",
    "    temp_path = folder / \"TEMP.csv\"\n",
    "    if \"TEMP\" not in missing and temp_path.exists():\n",
    "        fs_temp, temp_raw, _ = read_signal(temp_path)\n",
    "        sensors[\"TEMP\"] = np.squeeze(temp_raw)\n",
    "        fs_map[\"TEMP\"] = fs_temp\n",
    "\n",
    "    fs_acc, acc_raw, _ = read_signal(folder / \"ACC.csv\")\n",
    "    acc_mag = np.linalg.norm(acc_raw, axis=1)\n",
    "    sensors[\"ACC_MAG\"] = acc_mag\n",
    "    fs_map[\"ACC_MAG\"] = fs_acc\n",
    "    acc_activity = acc_activity_signal(acc_raw, fs_acc)\n",
    "\n",
    "    if acc_activity.size:\n",
    "        sensors[\"ACC_ACTIVITY\"] = acc_activity\n",
    "        fs_map[\"ACC_ACTIVITY\"] = 1.0 / ACC_ACTIVITY_STEP_SEC\n",
    "\n",
    "    bvp_path = folder / \"BVP.csv\"\n",
    "    if \"BVP\" not in missing and bvp_path.exists():\n",
    "        fs_bvp, bvp_raw, _ = read_signal(bvp_path)\n",
    "        sensors[\"BVP\"] = np.squeeze(bvp_raw)\n",
    "        fs_map[\"BVP\"] = fs_bvp\n",
    "\n",
    "    if \"IBI\" not in missing:\n",
    "        sensors[\"IBI\"] = read_ibi(folder / \"IBI.csv\")\n",
    "    else:\n",
    "        sensors[\"IBI\"] = np.empty((0, 2))\n",
    "    tags = read_tags(folder / \"tags.csv\", start_ts)\n",
    "\n",
    "    if base_id in DUPLICATE_CUTS:\n",
    "        cuts = DUPLICATE_CUTS[base_id]\n",
    "        if \"EDA\" in cuts and \"EDA\" in sensors:\n",
    "            sensors[\"EDA\"] = sensors[\"EDA\"][:cuts[\"EDA\"]]\n",
    "        if \"TEMP\" in cuts and \"TEMP\" in sensors:\n",
    "            sensors[\"TEMP\"] = sensors[\"TEMP\"][:cuts[\"TEMP\"]]\n",
    "        if \"ACC\" in cuts and \"ACC_MAG\" in sensors:\n",
    "            sensors[\"ACC_MAG\"] = sensors[\"ACC_MAG\"][:cuts[\"ACC\"]]\n",
    "        if \"BVP\" in cuts and \"BVP\" in sensors:\n",
    "            sensors[\"BVP\"] = sensors[\"BVP\"][:cuts[\"BVP\"]]\n",
    "\n",
    "    duration = len(sensors[\"EDA\"]) / fs_map[\"EDA\"]\n",
    "    return {\n",
    "        \"sensors\": sensors,\n",
    "        \"fs\": fs_map,\n",
    "        \"tags\": tags,\n",
    "        \"duration\": duration,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build windowed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_windows_for_subject(state: str, subject: str, tgt_fs: float = 4.0) -> List[dict]:\n",
    "    info = load_subject_state(state, subject)\n",
    "    sensors = info[\"sensors\"]\n",
    "    fs_map = info[\"fs\"]\n",
    "    tags = info[\"tags\"]\n",
    "    duration = info[\"duration\"]\n",
    "\n",
    "    # Resample signals\n",
    "    eda = resample_to_rate(sensors[\"EDA\"], fs_map[\"EDA\"], tgt_fs)\n",
    "    temp = resample_to_rate(sensors[\"TEMP\"], fs_map.get(\"TEMP\", tgt_fs), tgt_fs) if \"TEMP\" in sensors else np.array([])\n",
    "    acc = resample_to_rate(sensors[\"ACC_MAG\"], fs_map[\"ACC_MAG\"], tgt_fs)\n",
    "    acc_activity = resample_to_rate(sensors[\"ACC_ACTIVITY\"], fs_map.get(\"ACC_ACTIVITY\", tgt_fs), tgt_fs) if \"ACC_ACTIVITY\" in sensors else np.array([])\n",
    "    bvp_signal = sensors.get(\"BVP\")\n",
    "    bvp_fs = fs_map.get(\"BVP\")\n",
    "\n",
    "    intervals = make_label_intervals(state, subject, tags, duration)\n",
    "    windows = window_intervals(duration, WINDOW_SECONDS, WINDOW_STEP_SECONDS)\n",
    "    rows = []\n",
    "    for w in windows:\n",
    "        lbl, span_meta = assign_label(w, intervals)\n",
    "        if lbl is None or span_meta is None:\n",
    "            continue\n",
    "        start_idx = int(w[0] * tgt_fs)\n",
    "        end_idx = int(w[1] * tgt_fs)\n",
    "        eda_win = eda[start_idx:end_idx]\n",
    "        temp_win = temp[start_idx:end_idx] if len(temp) else np.array([])\n",
    "        acc_win = acc[start_idx:end_idx]\n",
    "        activity_win = acc_activity[start_idx:end_idx] if len(acc_activity) else np.array([])\n",
    "        if sensors[\"IBI\"].size:\n",
    "            ibi_arr = sensors[\"IBI\"]\n",
    "            mask = (ibi_arr[:, 0] >= w[0]) & (ibi_arr[:, 0] <= w[1])\n",
    "            ibi_win = ibi_arr[mask, 1]\n",
    "        else:\n",
    "            ibi_win = np.array([])\n",
    "        if bvp_signal is not None and bvp_fs:\n",
    "            bvp_start = int(max(0, math.floor(w[0] * bvp_fs)))\n",
    "            bvp_end = int(min(len(bvp_signal), math.floor(w[1] * bvp_fs)))\n",
    "            bvp_win = bvp_signal[bvp_start:bvp_end]\n",
    "        else:\n",
    "            bvp_win = None\n",
    "\n",
    "        feats = combine_features(\n",
    "            eda_win,\n",
    "            tgt_fs,\n",
    "            acc_win,\n",
    "            tgt_fs,\n",
    "            temp_win,\n",
    "            tgt_fs,\n",
    "            ibi_win,\n",
    "            activity_win,\n",
    "            bvp_win,\n",
    "            bvp_fs,\n",
    "        )\n",
    "        stress_stage = span_meta.get(\"stage\") if isinstance(span_meta, dict) else None\n",
    "        stress_level = span_meta.get(\"stress_level\") if isinstance(span_meta, dict) else None\n",
    "        phase_start = span_meta.get(\"start\") if isinstance(span_meta, dict) else None\n",
    "        phase_end = span_meta.get(\"end\") if isinstance(span_meta, dict) else None\n",
    "        if lbl == \"stress\":\n",
    "            if stress_level is None or np.isnan(stress_level):\n",
    "                continue\n",
    "        else:\n",
    "            stress_level = 0.0 if stress_level is None or np.isnan(stress_level) else stress_level\n",
    "        phase_label = stress_stage if stress_stage else lbl\n",
    "        stress_class = stress_bucket(stress_level, phase_label)\n",
    "        phase_duration = None\n",
    "        phase_progress = None\n",
    "        phase_elapsed = None\n",
    "        if phase_start is not None and phase_end is not None and phase_end > phase_start:\n",
    "            phase_duration = float(phase_end - phase_start)\n",
    "            phase_elapsed = float(w[0] - phase_start)\n",
    "            phase_progress = max(0.0, min(1.0, phase_elapsed / phase_duration))\n",
    "        row = {\n",
    "            \"subject\": base_subject_id(subject),\n",
    "            \"state\": state.lower(),\n",
    "            \"phase\": phase_label,\n",
    "            \"stress_stage\": stress_stage if lbl == \"stress\" else None,\n",
    "            \"stress_level\": float(stress_level),\n",
    "            \"label\": stress_class,\n",
    "            \"is_stress\": 1 if phase_label in STRESS_PHASES else 0,\n",
    "            \"win_start\": w[0],\n",
    "            \"win_end\": w[1],\n",
    "            \"phase_start\": phase_start,\n",
    "            \"phase_end\": phase_end,\n",
    "            \"phase_duration\": phase_duration,\n",
    "            \"phase_elapsed\": phase_elapsed,\n",
    "            \"phase_progress\": phase_progress,\n",
    "        }\n",
    "        row.update(feats)\n",
    "        rows.append(row)\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64e1f51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(states: List[str] = STATES, max_subjects: int | None = None) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for state in states:\n",
    "        state_dir = DATASET_ROOT / state\n",
    "        if not state_dir.exists():\n",
    "            continue\n",
    "        subjects = sorted([p.name for p in state_dir.iterdir() if p.is_dir()])\n",
    "        if max_subjects:\n",
    "            subjects = subjects[:max_subjects]\n",
    "        for subj in subjects:\n",
    "            try:\n",
    "                rows.extend(build_windows_for_subject(state, subj))\n",
    "            except Exception as exc:\n",
    "                print(f\"Skip {state}/{subj}: {exc}\")\n",
    "                continue\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows 6742\n",
      "Phase counts phase\n",
      "rest                2981\n",
      "aerobic             1546\n",
      "anaerobic            998\n",
      "TMCT                 633\n",
      "Opposite Opinion     432\n",
      "Stroop               152\n",
      "Name: count, dtype: int64\n",
      "Label distribution label\n",
      "no_stress          5525\n",
      "moderate_stress     664\n",
      "high_stress         462\n",
      "low_stress           91\n",
      "Name: count, dtype: int64\n",
      "Stress level summary count    6742.000000\n",
      "mean        0.929398\n",
      "std         2.179948\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max        10.000000\n",
      "Name: stress_level, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = build_dataset()\n",
    "df = df.sort_values([\"subject\", \"win_start\"]).reset_index(drop=True)\n",
    "\n",
    "phase_group = df.groupby([\"subject\", \"phase\"], dropna=False)\n",
    "phase_min_start = phase_group[\"win_start\"].transform(\"min\")\n",
    "phase_max_end = phase_group[\"win_end\"].transform(\"max\")\n",
    "phase_duration_fallback = (phase_max_end - phase_min_start).replace(0, np.nan)\n",
    "df[\"phase_elapsed\"] = df[\"phase_elapsed\"].fillna(df[\"win_start\"] - phase_min_start)\n",
    "df[\"phase_duration\"] = df[\"phase_duration\"].fillna(phase_duration_fallback)\n",
    "df[\"phase_progress\"] = df[\"phase_progress\"].fillna(\n",
    "    df[\"phase_elapsed\"] / df[\"phase_duration\"].replace(0, np.nan)\n",
    ")\n",
    "df[\"phase_progress\"] = df[\"phase_progress\"].clip(0.0, 1.0).fillna(0.0)\n",
    "df[\"phase_position\"] = phase_group.cumcount()\n",
    "phase_counts = phase_group.size().rename(\"phase_size\")\n",
    "df = df.join(phase_counts, on=[\"subject\", \"phase\"])\n",
    "df[\"phase_position\"] = np.where(\n",
    "    df[\"phase_size\"] > 1,\n",
    "    df[\"phase_position\"] / (df[\"phase_size\"] - 1),\n",
    "    0.0,\n",
    ")\n",
    "df.drop(columns=[\"phase_size\"], inplace=True)\n",
    "df[\"phase_remaining\"] = (df[\"phase_duration\"] - df[\"phase_elapsed\"]).clip(lower=0).fillna(0)\n",
    "df[\"phase_early\"] = (df[\"phase_progress\"] <= 0.33).astype(int)\n",
    "df[\"phase_mid\"] = ((df[\"phase_progress\"] > 0.33) & (df[\"phase_progress\"] <= 0.66)).astype(int)\n",
    "df[\"phase_late\"] = (df[\"phase_progress\"] > 0.66).astype(int)\n",
    "\n",
    "context_cols = []\n",
    "for feat in TEMPORAL_FEATURES:\n",
    "    prev_col = f\"prev_{feat}\"\n",
    "    delta_col = f\"delta_{feat}\"\n",
    "    roll_mean_col = f\"roll_mean_{feat}\"\n",
    "    roll_std_col = f\"roll_std_{feat}\"\n",
    "    ema_col = f\"ema_{feat}\"\n",
    "    phase_delta_col = f\"phase_delta_{feat}\"\n",
    "    phase_z_col = f\"phase_z_{feat}\"\n",
    "    subj_group = df.groupby(\"subject\")[feat]\n",
    "    df[prev_col] = subj_group.shift(1)\n",
    "    df[delta_col] = df[feat] - df[prev_col]\n",
    "    roll_mean = (\n",
    "        subj_group.rolling(window=3, min_periods=1)\n",
    "        .mean()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    roll_std = (\n",
    "        subj_group.rolling(window=3, min_periods=1)\n",
    "        .std()\n",
    "        .reset_index(level=0, drop=True)\n",
    "    )\n",
    "    df[roll_mean_col] = roll_mean.to_numpy()\n",
    "    df[roll_std_col] = roll_std.to_numpy()\n",
    "    df[ema_col] = df.groupby(\"subject\")[feat].transform(lambda s: s.ewm(span=EMA_SPAN, adjust=False).mean())\n",
    "    phase_first = (\n",
    "        df.sort_values(\"win_start\")\n",
    "        .groupby([\"subject\", \"phase\"], dropna=False)[feat]\n",
    "        .transform(\"first\")\n",
    "    )\n",
    "    df[phase_delta_col] = df[feat] - phase_first\n",
    "    phase_running_mean = phase_group[feat].transform(lambda s: s.expanding().mean())\n",
    "    phase_running_std = phase_group[feat].transform(lambda s: s.expanding().std()).replace(0, np.nan)\n",
    "    df[phase_z_col] = (df[feat] - phase_running_mean) / phase_running_std\n",
    "    df[phase_z_col] = df[phase_z_col].replace([np.inf, -np.inf], 0)\n",
    "    context_cols.extend([prev_col, delta_col, roll_mean_col, roll_std_col, ema_col, phase_delta_col, phase_z_col])\n",
    "\n",
    "df[context_cols] = df[context_cols].fillna(0)\n",
    "\n",
    "df.to_csv(\"stress_level_dataset.csv\", index=False)\n",
    "print(\"Rows\", len(df))\n",
    "print(\"Phase counts\", df[\"phase\"].value_counts())\n",
    "print(\"Label distribution\", df[\"label\"].value_counts())\n",
    "print(\"Stress level summary\", df[\"stress_level\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a3d2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_by_phase_label(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    balanced = []\n",
    "    for phase_name, phase_df in data.groupby(\"phase\"):\n",
    "        if phase_df.empty:\n",
    "            continue\n",
    "        stress_df = phase_df[phase_df[\"label\"] != \"no_stress\"]\n",
    "        phase_parts = []\n",
    "        base_target = stress_df[\"label\"].value_counts().max() if not stress_df.empty else 0\n",
    "        phase_factor = PHASE_WEIGHT_MAP.get(phase_name, 1.0)\n",
    "        target = max(1, int(np.ceil(base_target * phase_factor))) if base_target else 0\n",
    "        if not stress_df.empty:\n",
    "            for label_name, label_df in stress_df.groupby(\"label\"):\n",
    "                label_factor = PHASE_LABEL_OVERSAMPLE.get((phase_name, label_name), 1.0)\n",
    "                label_target = max(1, int(np.ceil(target * label_factor))) if target else len(label_df)\n",
    "                if len(label_df) < label_target:\n",
    "                    phase_parts.append(label_df.sample(label_target, replace=True, random_state=42))\n",
    "                else:\n",
    "                    phase_parts.append(label_df)\n",
    "        no_stress_df = phase_df[phase_df[\"label\"] == \"no_stress\"]\n",
    "        if not no_stress_df.empty:\n",
    "            cap = int(np.ceil(PHASE_BALANCE_NO_STRESS_RATIO * max(target, 1))) if target else len(no_stress_df)\n",
    "            if len(no_stress_df) > cap:\n",
    "                phase_parts.append(no_stress_df.sample(cap, random_state=42, replace=False))\n",
    "            else:\n",
    "                phase_parts.append(no_stress_df)\n",
    "        if phase_parts:\n",
    "            balanced.append(pd.concat(phase_parts, ignore_index=True))\n",
    "        else:\n",
    "            balanced.append(phase_df)\n",
    "    return pd.concat(balanced, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c40a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def apply_phase_smoothing(meta: pd.DataFrame, predictions: np.ndarray, window: int = PHASE_SMOOTHING_WINDOW) -> np.ndarray:\n",
    "    if window <= 1 or len(predictions) == 0:\n",
    "        return predictions\n",
    "    smoothed = predictions.copy()\n",
    "    half = window // 2\n",
    "    meta = meta.reset_index(drop=True)\n",
    "    for (_, phase_df) in meta.groupby([\"subject\", \"phase\"], dropna=False):\n",
    "        if phase_df.empty:\n",
    "            continue\n",
    "        order = phase_df.sort_values(\"win_start\").index.to_numpy()\n",
    "        group_vals = smoothed[order].copy()\n",
    "        for i in range(len(group_vals)):\n",
    "            start = max(0, i - half)\n",
    "            end = min(len(group_vals), i + half + 1)\n",
    "            window_vals = group_vals[start:end]\n",
    "            if window_vals.size == 0:\n",
    "                continue\n",
    "            uniq, counts = np.unique(window_vals, return_counts=True)\n",
    "            group_vals[i] = uniq[np.argmax(counts)]\n",
    "        smoothed[order] = group_vals\n",
    "    return smoothed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated metrics:\n",
      "   fold  accuracy  macro_f1  weighted_f1\n",
      "0     0  0.930050  0.504649     0.922342\n",
      "1     1  0.922097  0.420842     0.922616\n",
      "2     2  0.915546  0.492934     0.909425\n",
      "3     3  0.840960  0.376735     0.815438\n",
      "4     4  0.931835  0.587332     0.921059\n",
      "Mean metrics:\n",
      "accuracy       0.908098\n",
      "macro_f1       0.476498\n",
      "weighted_f1    0.898176\n",
      "dtype: float64\n",
      "Average confusion matrix:\n",
      "                      pred_high_stress  pred_low_stress  pred_moderate_stress  \\\n",
      "true_high_stress                  19.6              2.8                  64.8   \n",
      "true_low_stress                    2.6              0.0                  15.6   \n",
      "true_moderate_stress              31.6              0.2                 100.2   \n",
      "true_no_stress                     0.0              0.0                   0.0   \n",
      "\n",
      "                      pred_no_stress  \n",
      "true_high_stress                 5.2  \n",
      "true_low_stress                  0.0  \n",
      "true_moderate_stress             0.8  \n",
      "true_no_stress                1105.0  \n",
      "Per-phase diagnostics:\n",
      "              phase  support  accuracy  macro_f1  weighted_f1\n",
      "0  Opposite Opinion      432  0.541667  0.206564     0.501953\n",
      "1            Stroop      152  0.388158  0.194213     0.394690\n",
      "2              TMCT      633  0.483412  0.235369     0.438225\n",
      "3           aerobic     1546  1.000000  0.250000     1.000000\n",
      "4         anaerobic      998  1.000000  0.250000     1.000000\n",
      "5              rest     2981  1.000000  0.250000     1.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare features for modeling\n",
    "numeric_cols = [\n",
    "    c for c in df.columns if c not in FEATURE_EXCLUDE_COLS and np.issubdtype(df[c].dtype, np.number)\n",
    "]\n",
    "helper_cols = [\"label\", \"subject\", \"phase\", \"win_start\", \"stress_level\"]\n",
    "feature_df = df.loc[:, numeric_cols + helper_cols].copy()\n",
    "feature_df = feature_df.dropna(subset=[\"label\"]).reset_index(drop=True)\n",
    "feature_df[\"phase\"] = feature_df[\"phase\"].fillna(\"unknown\")\n",
    "\n",
    "phase_sorted = feature_df.sort_values([\"subject\", \"phase\", \"win_start\"])\n",
    "phase_baselines = (\n",
    "    phase_sorted.groupby([\"subject\", \"phase\"], dropna=False)[numeric_cols]\n",
    "    .transform(\"first\")\n",
    ")\n",
    "feature_df.loc[:, numeric_cols] = feature_df[numeric_cols] - phase_baselines.fillna(0)\n",
    "feature_df.loc[:, numeric_cols] = feature_df[numeric_cols].fillna(0)\n",
    "\n",
    "non_nan_cols = feature_df[numeric_cols].columns[~feature_df[numeric_cols].isna().all()]\n",
    "numeric_cols = non_nan_cols.tolist()\n",
    "feature_df.loc[:, numeric_cols] = feature_df[numeric_cols].fillna(0)\n",
    "helper_df = feature_df.loc[:, numeric_cols + helper_cols].copy().reset_index(drop=True)\n",
    "helper_df[\"label\"] = helper_df[\"label\"].astype(str)\n",
    "helper_df[\"phase\"] = helper_df[\"phase\"].fillna(\"unknown\")\n",
    "helper_df[\"stress_level\"] = helper_df[\"stress_level\"].fillna(0.0)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(helper_df[\"label\"])\n",
    "\n",
    "param_grid = [\n",
    "    {\"max_depth\": 3, \"learning_rate\": 0.05, \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"n_estimators\": 400},\n",
    "    {\"max_depth\": 4, \"learning_rate\": 0.05, \"subsample\": 0.8, \"colsample_bytree\": 0.8, \"n_estimators\": 600},\n",
    "    {\"max_depth\": 4, \"learning_rate\": 0.1, \"subsample\": 0.75, \"colsample_bytree\": 0.75, \"n_estimators\": 400},\n",
    "]\n",
    "default_params = param_grid[0].copy()\n",
    "regression_params = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 3,\n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "}\n",
    "\n",
    "\n",
    "def compute_sample_weights(phases: np.ndarray, labels: np.ndarray) -> np.ndarray:\n",
    "    phase_weights = pd.Series(phases).map(lambda p: PHASE_WEIGHT_MAP.get(p, 1.0)).fillna(1.0).to_numpy()\n",
    "    label_counts = pd.Series(labels).value_counts()\n",
    "    class_weights = {\n",
    "        label: len(labels) / (len(label_counts) * count)\n",
    "        for label, count in label_counts.items()\n",
    "    }\n",
    "    class_weight_vec = np.array([class_weights[label] for label in labels])\n",
    "    return phase_weights * class_weight_vec\n",
    "\n",
    "\n",
    "def prepare_training_subset(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    balanced = balance_by_phase_label(data)\n",
    "    balanced[\"phase\"] = balanced[\"phase\"].fillna(\"unknown\")\n",
    "    return balanced.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def tune_hyperparameters(X: np.ndarray, y: np.ndarray, groups: np.ndarray, phases: np.ndarray) -> dict:\n",
    "    unique_groups = np.unique(groups)\n",
    "    if unique_groups.size < 2:\n",
    "        return default_params.copy()\n",
    "    inner_splits = min(3, unique_groups.size)\n",
    "    inner_gkf = GroupKFold(n_splits=inner_splits)\n",
    "    best_params = default_params.copy()\n",
    "    best_score = -np.inf\n",
    "    for params in param_grid:\n",
    "        fold_scores = []\n",
    "        for inner_train_idx, inner_val_idx in inner_gkf.split(X, y, groups):\n",
    "            scaler = StandardScaler().fit(X[inner_train_idx])\n",
    "            X_inner_train = scaler.transform(X[inner_train_idx])\n",
    "            X_inner_val = scaler.transform(X[inner_val_idx])\n",
    "            weights_inner = compute_sample_weights(phases[inner_train_idx], y[inner_train_idx])\n",
    "            model = XGBClassifier(\n",
    "                objective=\"binary:logistic\",\n",
    "                n_jobs=4,\n",
    "                n_estimators=params[\"n_estimators\"],\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                max_depth=params[\"max_depth\"],\n",
    "                subsample=params[\"subsample\"],\n",
    "                colsample_bytree=params[\"colsample_bytree\"],\n",
    "            )\n",
    "            model.fit(\n",
    "                X_inner_train,\n",
    "                y[inner_train_idx],\n",
    "                sample_weight=weights_inner,\n",
    "            )\n",
    "            preds_inner = model.predict(X_inner_val)\n",
    "            report = classification_report(\n",
    "                y[inner_val_idx],\n",
    "                preds_inner,\n",
    "                labels=[0, 1],\n",
    "                output_dict=True,\n",
    "                zero_division=0,\n",
    "            )\n",
    "            fold_scores.append(report[\"macro avg\"][\"f1-score\"])\n",
    "        mean_score = np.mean(fold_scores) if fold_scores else -np.inf\n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params.copy()\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def predict_stress_label_from_level(level: float, phase: str) -> str:\n",
    "    level = max(0.0, float(level))\n",
    "    return stress_bucket(level, phase)\n",
    "\n",
    "outer_groups = helper_df[\"subject\"].to_numpy()\n",
    "outer_labels = helper_df[\"label\"].to_numpy()\n",
    "unique_subjects = np.unique(outer_groups)\n",
    "if unique_subjects.size < 2:\n",
    "    raise RuntimeError(\"Need at least two subjects for cross-validation\")\n",
    "outer_gkf = GroupKFold(n_splits=min(5, unique_subjects.size))\n",
    "results = []\n",
    "confusion = np.zeros((len(le.classes_), len(le.classes_)), dtype=float)\n",
    "phase_records = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(outer_gkf.split(helper_df[numeric_cols], outer_labels, outer_groups)):\n",
    "    train_df_fold = helper_df.iloc[train_idx].copy().reset_index(drop=True)\n",
    "    test_df_fold = helper_df.iloc[test_idx].copy().reset_index(drop=True)\n",
    "\n",
    "    train_prepared = prepare_training_subset(train_df_fold)\n",
    "    X_train = train_prepared[numeric_cols].to_numpy(dtype=float)\n",
    "    phase_train = train_prepared[\"phase\"].to_numpy()\n",
    "    binary_labels_train = (train_prepared[\"label\"] != \"no_stress\").astype(int).to_numpy()\n",
    "\n",
    "    best_params = tune_hyperparameters(X_train, binary_labels_train, train_prepared[\"subject\"].to_numpy(), phase_train)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(test_df_fold[numeric_cols].to_numpy(dtype=float))\n",
    "\n",
    "    weights_stage1 = compute_sample_weights(phase_train, binary_labels_train)\n",
    "    model_stage1 = XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        n_jobs=4,\n",
    "        n_estimators=best_params[\"n_estimators\"],\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        max_depth=best_params[\"max_depth\"],\n",
    "        subsample=best_params[\"subsample\"],\n",
    "        colsample_bytree=best_params[\"colsample_bytree\"],\n",
    "    )\n",
    "    model_stage1.fit(X_train_scaled, binary_labels_train, sample_weight=weights_stage1)\n",
    "\n",
    "    stress_mask_train = binary_labels_train.astype(bool)\n",
    "    if not np.any(stress_mask_train):\n",
    "        continue\n",
    "    stage2_features = X_train_scaled[stress_mask_train]\n",
    "    stage2_targets = train_prepared.loc[stress_mask_train, \"stress_level\"].to_numpy(dtype=float)\n",
    "    valid_stage2 = np.isfinite(stage2_targets)\n",
    "    stage2_features = stage2_features[valid_stage2]\n",
    "    stage2_targets = stage2_targets[valid_stage2]\n",
    "    phase_stage2 = phase_train[stress_mask_train][valid_stage2]\n",
    "    if stage2_features.size == 0:\n",
    "        continue\n",
    "    weights_stage2 = np.array([PHASE_WEIGHT_MAP.get(phase, 1.0) for phase in phase_stage2])\n",
    "    model_stage2 = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_estimators=regression_params[\"n_estimators\"],\n",
    "        learning_rate=regression_params[\"learning_rate\"],\n",
    "        max_depth=regression_params[\"max_depth\"],\n",
    "        subsample=regression_params[\"subsample\"],\n",
    "        colsample_bytree=regression_params[\"colsample_bytree\"],\n",
    "        n_jobs=4,\n",
    "    )\n",
    "    model_stage2.fit(stage2_features, stage2_targets, sample_weight=weights_stage2)\n",
    "\n",
    "    stage1_proba = model_stage1.predict_proba(X_test_scaled)[:, 1]\n",
    "    stress_pred_mask = stage1_proba >= STRESS_PROB_THRESHOLD\n",
    "    preds = np.array([\"no_stress\"] * len(test_idx), dtype=object)\n",
    "    if stress_pred_mask.any():\n",
    "        stress_features = X_test_scaled[stress_pred_mask]\n",
    "        stress_levels_pred = model_stage2.predict(stress_features)\n",
    "        phase_subset = test_df_fold.loc[stress_pred_mask, \"phase\"].to_numpy()\n",
    "        mapped_labels = [\n",
    "            predict_stress_label_from_level(level, phase_name)\n",
    "            for level, phase_name in zip(stress_levels_pred, phase_subset)\n",
    "        ]\n",
    "        preds[stress_pred_mask] = mapped_labels\n",
    "\n",
    "    preds = apply_phase_smoothing(\n",
    "        test_df_fold[[\"subject\", \"phase\", \"win_start\"]],\n",
    "        preds.astype(object),\n",
    "        window=PHASE_SMOOTHING_WINDOW,\n",
    "    )\n",
    "    true_labels = test_df_fold[\"label\"].to_numpy()\n",
    "    accuracy = np.mean(preds == true_labels)\n",
    "    report = classification_report(\n",
    "        true_labels,\n",
    "        preds,\n",
    "        labels=le.classes_,\n",
    "        output_dict=True,\n",
    "        zero_division=0,\n",
    "    )\n",
    "    confusion += confusion_matrix(true_labels, preds, labels=le.classes_)\n",
    "    results.append(\n",
    "        {\n",
    "            \"fold\": fold,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "        }\n",
    "    )\n",
    "    phase_records.append(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"phase\": test_df_fold[\"phase\"].to_numpy(),\n",
    "                \"true\": true_labels,\n",
    "                \"pred\": preds,\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Cross-validated metrics:\")\n",
    "print(results_df)\n",
    "print(\"Mean metrics:\")\n",
    "print(results_df[[\"accuracy\", \"macro_f1\", \"weighted_f1\"]].mean())\n",
    "\n",
    "conf_df = pd.DataFrame(\n",
    "    confusion / max(1, len(results)),\n",
    "    index=[f\"true_{c}\" for c in le.classes_],\n",
    "    columns=[f\"pred_{c}\" for c in le.classes_],\n",
    ")\n",
    "print(\"Average confusion matrix:\")\n",
    "print(conf_df.round(1))\n",
    "\n",
    "if phase_records:\n",
    "    phase_df = pd.concat(phase_records, ignore_index=True)\n",
    "    phase_summary = []\n",
    "    for phase_name, subset in phase_df.groupby(\"phase\"):\n",
    "        phase_report = classification_report(\n",
    "            subset[\"true\"],\n",
    "            subset[\"pred\"],\n",
    "            labels=le.classes_,\n",
    "            output_dict=True,\n",
    "            zero_division=0,\n",
    "        )\n",
    "        phase_summary.append(\n",
    "            {\n",
    "                \"phase\": phase_name,\n",
    "                \"support\": len(subset),\n",
    "                \"accuracy\": np.mean(subset[\"true\"] == subset[\"pred\"]),\n",
    "                \"macro_f1\": phase_report[\"macro avg\"][\"f1-score\"],\n",
    "                \"weighted_f1\": phase_report[\"weighted avg\"][\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "    phase_summary_df = pd.DataFrame(phase_summary).sort_values(\"phase\")\n",
    "    print(\"Per-phase diagnostics:\")\n",
    "    print(phase_summary_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
